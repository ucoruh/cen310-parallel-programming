{"config":{"lang":["en","tr"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blank","text":""},{"location":"license/","title":"License","text":""},{"location":"license/#license","title":"License","text":""},{"location":"license/#code-license","title":"Code License","text":"<p>MIT License</p> <p>Copyright \u00a9 2019-2024 U\u011fur CORUH</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"license/#content-licence","title":"Content Licence","text":"<p>Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC BY-NC-ND 4.0)</p> <p></p> <p>You are free to:</p> <ul> <li>Share: Copy and redistribute the material in any medium or format.</li> </ul> <p>Under the following terms:</p> <ul> <li>Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial: You may not use the material for commercial purposes.</li> <li>NoDerivatives: If you remix, transform, or build upon the material, you may not distribute the modified material.</li> </ul> <p>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</p> <p>For more details, visit: Creative Commons CC BY-NC-ND 4.0</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tags","title":"Tags","text":"<p>This file contains a global index of all tags used on the pages.</p>"},{"location":"tags/#tag:cen310","title":"cen310","text":"<ul> <li>            Syllabus          </li> </ul>"},{"location":"tags/#tag:cen310-syllabus","title":"cen310-syllabus","text":"<ul> <li>            Syllabus          </li> </ul>"},{"location":"tags/#tag:cen310-week-1","title":"cen310-week-1","text":"<ul> <li>            CEN310 Parallel Programming Week-1          </li> </ul>"},{"location":"tags/#tag:cen310-week-2","title":"cen310-week-2","text":"<ul> <li>            CEN310 Parallel Programming Week-2          </li> </ul>"},{"location":"tags/#tag:course-introduction","title":"course-introduction","text":"<ul> <li>            CEN310 Parallel Programming Week-1          </li> </ul>"},{"location":"tags/#tag:development-environment","title":"development-environment","text":"<ul> <li>            CEN310 Parallel Programming Week-1          </li> </ul>"},{"location":"tags/#tag:parallel-architectures","title":"parallel-architectures","text":"<ul> <li>            CEN310 Parallel Programming Week-2          </li> </ul>"},{"location":"tags/#tag:parallel-computing","title":"parallel-computing","text":"<ul> <li>            CEN310 Parallel Programming Week-2          </li> </ul>"},{"location":"tags/#tag:parallel-programming","title":"parallel-programming","text":"<ul> <li>            CEN310 Parallel Programming Week-1          </li> <li>            CEN310 Parallel Programming Week-2          </li> <li>            Syllabus          </li> </ul>"},{"location":"tags/#tag:performance-analysis","title":"performance-analysis","text":"<ul> <li>            CEN310 Parallel Programming Week-2          </li> </ul>"},{"location":"tags/#tag:spring-2025","title":"spring-2025","text":"<ul> <li>            CEN310 Parallel Programming Week-1          </li> <li>            Syllabus          </li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":""},{"location":"changelog/#material-for-algorithm-lovers","title":"Material for Algorithm Lovers","text":""},{"location":"changelog/#1.0.0","title":"1.0.0 _ October 20, 2020","text":"<ul> <li>Initial release</li> </ul>"},{"location":"resume/","title":"Resume","text":""},{"location":"resume/#resume","title":"Resume","text":"<ul> <li>Download Resume</li> </ul>"},{"location":"syllabus/syllabus/","title":"CEN310 Parallel Programming","text":"","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#recep-tayyip-erdogan-university","title":"Recep Tayyip Erdo\u011fan University","text":"","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#faculty-of-engineering-and-architecture-computer-engineering","title":"Faculty of Engineering and Architecture, Computer Engineering","text":"","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#cen310-parallel-programming-course-syllabus","title":"CEN310 - Parallel Programming Course Syllabus","text":"","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":"<p>Download </p> <ul> <li>PDF</li> <li>DOC</li> <li>SLIDE</li> <li>PPTX</li> </ul> Instructor: Asst. Prof. Dr. U\u011fur CORUH Contact Information: ugur.coruh@erdogan.edu.tr Office No: F-301 Google Classroom Code Not Used Microsoft Teams Code ilpgjzn Lecture Hours and Days Friday, 09:00 - 12:00 D-402 Lecture Classroom D-402 or Online via Google Meet / Microsoft Teams Instructor: Asst. Prof. Dr. U\u011fur CORUH Office Hours Meetings will be scheduled via Google Meet or Microsoft Teams using your university account and email. Email requests for meetings are required. To receive a faster response, ensure your email subject begins with [CEN310], and write clear, concise, formal emails. Lecture and Communication Language English Theory Course Hour Per Week 3 Hours Credit 4 Prerequisite None Corequisite None Requirement Compulsory","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#a-course-description","title":"A. Course Description","text":"<p>This course introduces fundamental concepts and practices of parallel programming, focusing on designing and implementing efficient parallel algorithms using modern programming frameworks and architectures. Students will learn to analyze sequential algorithms and transform them into parallel solutions, understanding key concepts such as parallelization strategies, load balancing, synchronization, and performance optimization.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#b-course-learning-outcomes-part-1","title":"B. Course Learning Outcomes (Part 1)","text":"<p>After completing this course satisfactorily, a student will be able to:</p> <ol> <li> <p>Design and implement parallel algorithms by applying appropriate parallelization strategies and patterns using modern frameworks like OpenMP and MPI</p> </li> <li> <p>Analyze and optimize parallel program performance through proper evaluation of efficiency, scalability, and bottleneck identification</p> </li> <li> <p>Develop parallel solutions using various programming models (shared memory, distributed memory) while effectively managing synchronization and data structures</p> </li> </ol>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#b-course-learning-outcomes-part-2","title":"B. Course Learning Outcomes (Part 2)","text":"<ol> <li> <p>Apply parallel computing concepts to solve real-world computational problems using appropriate architectures and tools</p> </li> <li> <p>Evaluate and select appropriate parallel computing approaches based on problem requirements, considering factors such as scalability, efficiency, and hardware constraints</p> </li> </ol>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#c-course-topics","title":"C. Course Topics","text":"<ol> <li>Introduction to parallel computing concepts and architecture</li> <li>Parallel algorithm design and performance analysis principles</li> <li>Shared memory programming using OpenMP framework</li> <li>Distributed memory programming with Message Passing Interface (MPI)</li> <li>Performance optimization and profiling tools in parallel systems</li> <li>GPU computing and heterogeneous parallel architecture</li> <li>Advanced parallel programming patterns and synchronization techniques</li> <li>Real-world parallel computing applications and case studies</li> </ol>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#d-textbooks-and-required-hardware-part-1","title":"D. Textbooks and Required Hardware (Part 1)","text":"<p>This course does not require a specific coursebook. You can use the following books and online resources for reference:</p> <ul> <li>Peter S. Pacheco, An Introduction to Parallel Programming, Morgan Kaufmann</li> <li>Michael J. Quinn, Parallel Programming in C with MPI and OpenMP, McGraw-Hill</li> <li>Barbara Chapman, Using OpenMP: Portable Shared Memory Parallel Programming, MIT Press</li> <li>Additional resources will be provided during the course</li> </ul>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#d-textbooks-and-required-hardware-part-2","title":"D. Textbooks and Required Hardware (Part 2)","text":"<p>During this course, you should have:</p> <ol> <li>A laptop/desktop with Windows 10 or 11 with the following minimum specifications:</li> <li>Multi-core processor</li> <li>16GB RAM (recommended)</li> <li>100GB of free disk space</li> <li>Windows 10 version 2004 and higher (Build 19041 and higher) or Windows 11</li> </ol>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#d-textbooks-and-required-hardware-part-3","title":"D. Textbooks and Required Hardware (Part 3)","text":"<ol> <li>Required software (all free):</li> <li>Visual Studio Community 2022</li> <li>Windows Subsystem for Linux (WSL2)</li> <li>Ubuntu distribution on WSL</li> <li>Git for Windows</li> </ol>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#d-textbooks-and-required-hardware-part-4","title":"D. Textbooks and Required Hardware (Part 4)","text":"<ol> <li>Development environment setup:</li> <li> <p>Visual Studio Community 2022 with:</p> <ul> <li>\"Desktop development with C++\" workload</li> <li>\"Linux development with C++\" workload</li> <li>WSL development tools</li> </ul> </li> <li> <p>WSL requirements:</p> <ul> <li>Ubuntu on WSL</li> <li>GCC/G++ compiler (installed via apt)</li> <li>OpenMP support</li> <li>MPI implementation (will be installed during class)</li> </ul> </li> </ol>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#d-textbooks-and-required-hardware-part-5","title":"D. Textbooks and Required Hardware (Part 5)","text":"<p>Installation instructions and support for setting up the development environment will be provided during the first week of the course. All programming assignments, classroom exercises, and examinations will be conducted using this setup.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#e-grading-part-1","title":"E. Grading (Part 1)","text":"<p>You will complete one project and two written quizzes throughout the semester. You are expected to submit your Midterm Parallel Implementation Report at the midterm, demonstrating parallel algorithms and performance analysis aligned with your project plan. In the 15<sup>th</sup> week, you will present and submit your Final Project Implementation Report.</p> <p>You will take a written quiz in the 7<sup>th</sup> week and another in the 13<sup>th</sup> week.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#e-grading-part-2","title":"E. Grading (Part 2)","text":"Assessment Code Weight Scope Midterm Project Report MPR1 60% Midterm Quiz-1 QUIZ1 40% Midterm Final Project Report MPR2 70% Final Quiz-2 QUIZ2 30% Final \\[ GradeMidterm = 0.6MPR1 + 0.4QUIZ1 \\] \\[ GradeFinal = 0.7MPR2 + 0.3QUIZ2 \\] \\[ PassingGrade = (40 * GradeMidterm + 60 * GradeFinal)/100 \\]","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#e-grading-part-3","title":"E. Grading (Part 3)","text":"<p>Your final passing grade can be improved through the following achievements: (Bonus Points TBD)</p> \\[ PassingGrade = PassingGrade + Bonus(\\text{T\u00fcbitak2209A Acceptance}, \\text{Teknofest Finalist}, \\text{Hackathon and Similar Finalists}) \\]","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#f-instructional-strategies-and-methods","title":"F. Instructional Strategies and Methods","text":"<p>The basic teaching method of this course will be planned to be face-to-face in the classroom, and support resources, homework, and announcements will be shared over Microsoft teams and Github. Students are expected to be in the university. This responsibility is very important to complete this course with success. If pandemic situation changes and distance education is required during this course, this course will be done using synchronous and asynchronous distance education methods. In this scenario, students are expected to be on the online platform, zoom, Microsoft teams or google meets, or meet at the time specified in the course schedule. Attendance will be taken.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#g-late-homework","title":"G. Late Homework","text":"<p>Throughout the semester, assignments and reports must be submitted as specified by the announced deadline. Overdue assignments will not be accepted. Unexpected situations must be reported to the instructor for late homework by students.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#h-course-platform-and-communication","title":"H. Course Platform and Communication","text":"<p>Microsoft Teams Classroom and Github will be used as a course learning management system. All electronic resources and announcements about the course will be shared on this platform. It is very important to check the course page daily, access the necessary resources and announcements, and communicate with the instructor to complete the course with success.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#i-academic-integrity-plagiarism-and-cheating","title":"I. Academic Integrity, Plagiarism, and Cheating","text":"","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#a-overview","title":"A. Overview","text":"<p>Academic integrity is one of the most important principles at RTE\u00dc University. Anyone who violates academic honesty will face serious consequences.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#b-collaboration-and-boundaries","title":"B. Collaboration and Boundaries","text":"<p>Collaborating with classmates or others to \"study together\" is a normal aspect of learning. Students may seek help from others (whether paid or unpaid) to better understand a challenging topic or course. However, it is essential to recognize when such collaboration crosses the line into academic dishonesty\u2014determining when it becomes plagiarism or cheating.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#c-exam-and-assignment-guidelines","title":"C. Exam and Assignment Guidelines","text":"","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#1-exam-conduct","title":"1. Exam Conduct","text":"<ul> <li>Using another student's paper or any unauthorized source during an exam is considered cheating and will be punished.</li> </ul>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#2-guidelines-for-assignments","title":"2. Guidelines for Assignments","text":"<p>Many students initially lack a clear understanding of acceptable practices in completing assignments, especially concerning copying. The following guidelines for Faculty of Engineering and Architecture students underscore our commitment to academic honesty. If a situation arises that is not covered below, please consult with the course instructor or assistant.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#a-what-is-acceptable-when-preparing-an-assignment","title":"a. What Is Acceptable When Preparing an Assignment?","text":"<p>I. Peer Collaboration and Discussion - Communicate with classmates to better understand the assignment. - Ask for guidance to improve the assignment's English content. - Share small portions of your assignment in class for discussion. - Discuss solutions using diagrams or summarized statements rather than exchanging exact text or code.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#a-what-is-acceptable-when-preparing-an-assignment_1","title":"a. What Is Acceptable When Preparing an Assignment?","text":"<p>II. External Resources and Assistance - Include ideas, quotes, paragraphs, or small code snippets from online sources or other references, provided that:   - They do not constitute the entire solution.   - All sources are properly cited. - Use external sources for technical instructions, references, or troubleshooting (but not for direct answers). - Work with (or even compensate) a tutor for help, as long as the tutor does not complete the assignment for you.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#b-what-is-not-acceptable","title":"b. What Is Not Acceptable?","text":"<ul> <li>Requesting or viewing a classmate's solution to a problem before you have submitted your own work.</li> <li>Failing to cite the source of any text or code taken from outside the course.</li> <li>Giving or showing your solution to a classmate who is struggling to solve the problem.</li> </ul>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#j-expectations","title":"J. Expectations","text":"<p>You are expected to attend classes on time and complete weekly course requirements (readings and assignments) throughout the semester. The primary communication channel between the instructor and students will be email. Please send your questions to the instructor's university-provided email address. Be sure to include the course name in the subject line and your name in the body of the email. The instructor will also contact you via email when necessary, so it is crucial to check your email regularly for communication.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#k-course-content-and-schedule-updates","title":"K. Course Content and Schedule Updates","text":"<p>The course content and schedule may be updated as needed. Any changes will be communicated to students by the instructor.</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#course-schedule-overview","title":"Course Schedule Overview","text":"<p>Regular Course Time: Every Friday (09:00-12:00) Project Review Sessions: Full day (09:00-17:00)</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#c-weekly-lesson-plan-part-14","title":"C. Weekly Lesson Plan (Part \u00bc)","text":"Week Date Subjects Other Tasks Week 1 14.02.2025 Course Introduction and Overview\u2022 Course plan and requirements\u2022 Introduction to parallel computing\u2022 Setting up development environment (VS Code, WSL) Environment Setup (3 hours) Week 2 21.02.2025 Parallel Computing Fundamentals\u2022 Types of parallelism\u2022 Architecture overview\u2022 Performance metrics\u2022 Analysis of parallel systems First Code Exercise (3 hours) Week 3 28.02.2025 Introduction to OpenMP\u2022 Shared memory programming\u2022 Basic directives\u2022 Thread management\u2022 Data parallelism concepts OpenMP Practice (3 hours) Week 4 07.03.2025 Advanced OpenMP\u2022 Parallel loops\u2022 Synchronization\u2022 Data sharing\u2022 Performance optimization strategies OpenMP Practice (3 hours)","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#c-weekly-lesson-plan-part-24","title":"C. Weekly Lesson Plan (Part 2/4)","text":"Week Date Subjects Other Tasks Week 5 14.03.2025 Performance Analysis &amp; MPI Introduction\u2022 Profiling tools\u2022 Debugging techniques\u2022 Distributed memory concepts\u2022 Basic MPI concepts Performance Lab (3 hours) Week 6 21.03.2025 Advanced MPI &amp; Parallel Patterns\u2022 Point-to-point communication\u2022 Collective operations\u2022 Common parallel patterns\u2022 Design strategies MPI Setup &amp; Implementation (3 hours) Week 7 28.03.2025 Quiz-1\u2022 Written examination Quiz-1 (3 hours) Week 8 04.04.2025 Midterm Project Review\u2022 Project presentations\u2022 Performance analysis discussions Project Presentations (Full Day) 09:00-17:00","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#c-weekly-lesson-plan-part-34","title":"C. Weekly Lesson Plan (Part \u00be)","text":"Week Date Subjects Other Tasks Week 9 5-13.04.2025 Midterm Examination Period Midterm Project Report DueAs scheduled Week 10 18.04.2025 Parallel Algorithm Design &amp; GPU Basics\u2022 Decomposition strategies\u2022 Load balancing\u2022 GPU architecture Fundamentals\u2022 CUDA introduction Algorithm Design Lab (3 hours) Week 11 25.04.2025 Advanced GPU Programming\u2022 CUDA programming model\u2022 Memory hierarchy\u2022 Optimization techniques\u2022 Performance considerations CUDA Implementation (3 hours) Week 12 02.05.2025 Real-world Applications I\u2022 Scientific computing\u2022 Data processing applications\u2022 Performance optimization\u2022 Case studies Application Development (3 hours)","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#c-weekly-lesson-plan-part-44","title":"C. Weekly Lesson Plan (Part 4/4)","text":"Week Date Subjects Other Tasks Week 13 09.05.2025 Real-world Applications II\u2022 Advanced parallel patterns\u2022 N-body simulations\u2022 Matrix computations\u2022 Big data processing Case Study Implementation (3 hours) Week 14 16.05.2025 Quiz-2\u2022 Written examination Quiz-2 (3 hours) Week 15 23.05.2025 Final Project Review\u2022 Project presentations\u2022 Performance analysis discussions Project Presentations (Full Day) 09:00-17:00 Week 16 24.05-04.06.2025 Final Examination Period Final Project Report DueAs scheduled","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#important-time-notes","title":"Important Time Notes:","text":"<ul> <li>Regular classes: 3-hour sessions on Fridays (09:00-12:00)</li> <li>Quiz sessions: Regular 3-hour class period</li> <li>Project Review sessions (Week 8 &amp; 15): Full day (09:00-17:00)</li> <li>Midterm and Final periods: As scheduled by the university</li> </ul>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"syllabus/syllabus/#key-dates","title":"Key Dates:","text":"<ul> <li>Quiz-1: March 28, 2025 (3 hours)</li> <li>Midterm Project Review: April 4, 2025 (Full Day)</li> <li>Quiz-2: May 16, 2025 (3 hours)</li> <li>Final Project Review: May 23, 2025 (Full Day)</li> </ul> <p>\\(End-Of-Syllabus\\)</p>","tags":["cen310-syllabus","parallel-programming","spring-2025","cen310"]},{"location":"week-1/cen429-week-1/","title":"CEN310 Parallel Programming Week-1","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#week-1","title":"Week-1","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#course-introduction-and-development-environment-setup","title":"Course Introduction and Development Environment Setup","text":"<p>Download </p> <ul> <li>PDF</li> <li>DOC</li> <li>SLIDE</li> <li>PPTX</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#outline-13","title":"Outline (\u2153)","text":"<ol> <li>Course Overview</li> <li>Course Description</li> <li>Learning Outcomes</li> <li>Assessment Methods</li> <li> <p>Course Topics</p> </li> <li> <p>Development Environment Setup</p> </li> <li>Required Hardware</li> <li>Required Software</li> <li>Installation Steps</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#outline-23","title":"Outline (\u2154)","text":"<ol> <li>Introduction to Parallel Programming</li> <li>What is Parallel Programming?</li> <li>Why Parallel Programming?</li> <li> <p>Basic Concepts</p> </li> <li> <p>First Parallel Program</p> </li> <li>Hello World Example</li> <li>Compilation Steps</li> <li>Running and Testing</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#outline-33","title":"Outline (3/3)","text":"<ol> <li>Understanding Hardware</li> <li>CPU Architecture</li> <li> <p>Memory Patterns</p> </li> <li> <p>Performance and Practice</p> </li> <li>Parallel Patterns</li> <li>Performance Measurement</li> <li>Homework</li> <li>Resources</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#1-course-overview","title":"1. Course Overview","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#course-description","title":"Course Description","text":"<p>This course introduces fundamental concepts and practices of parallel programming, focusing on: - Designing and implementing efficient parallel algorithms - Using modern programming frameworks - Understanding parallel architectures - Analyzing and optimizing parallel programs</p>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#learning-outcomes-12","title":"Learning Outcomes (\u00bd)","text":"<p>After completing this course, you will be able to:</p> <ol> <li>Design and implement parallel algorithms using OpenMP and MPI</li> <li>Analyze and optimize parallel program performance</li> <li>Develop solutions using various programming models</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#learning-outcomes-22","title":"Learning Outcomes (2/2)","text":"<ol> <li>Apply parallel computing concepts to real-world problems</li> <li>Evaluate and select appropriate parallel computing approaches based on:</li> <li>Problem requirements</li> <li>Hardware constraints</li> <li>Performance goals</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#assessment-methods","title":"Assessment Methods","text":"Assessment Weight Due Date Midterm Project Report 60% Week 8 Quiz-1 40% Week 7 Final Project Report 70% Week 14 Quiz-2 30% Week 13","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#course-topics-12","title":"Course Topics (\u00bd)","text":"<ol> <li>Parallel computing concepts</li> <li>Basic principles</li> <li>Architecture overview</li> <li> <p>Programming models</p> </li> <li> <p>Algorithm design and analysis</p> </li> <li>Design patterns</li> <li>Performance metrics</li> <li>Optimization strategies</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#course-topics-22","title":"Course Topics (2/2)","text":"<ol> <li>Programming frameworks</li> <li>OpenMP</li> <li>MPI</li> <li> <p>GPU Computing</p> </li> <li> <p>Advanced topics</p> </li> <li>Performance optimization</li> <li>Real-world applications</li> <li>Best practices</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#why-parallel-programming-12","title":"Why Parallel Programming? (\u00bd)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#historical-evolution","title":"Historical Evolution","text":"<ul> <li>Moore's Law limitations</li> <li>Multi-core revolution</li> <li>Cloud computing era</li> <li>Big data requirements</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#industry-applications","title":"Industry Applications","text":"<ul> <li>Scientific simulations</li> <li>Financial modeling</li> <li>AI/Machine Learning</li> <li>Video processing</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#why-parallel-programming-22","title":"Why Parallel Programming? (2/2)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#performance-benefits","title":"Performance Benefits","text":"<ul> <li>Reduced execution time</li> <li>Better resource utilization</li> <li>Improved responsiveness</li> <li>Higher throughput</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#challenges","title":"Challenges","text":"<ul> <li>Synchronization overhead</li> <li>Load balancing</li> <li>Debugging complexity</li> <li>Race conditions</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#parallel-computing-models-12","title":"Parallel Computing Models (\u00bd)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#shared-memory","title":"Shared Memory","text":"<pre><code>CPU     CPU     CPU     CPU\n  \u2502       \u2502       \u2502       \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n    Shared Memory\n</code></pre> <ul> <li>All processors access same memory</li> <li>Easy to program</li> <li>Limited scalability</li> <li>Example: OpenMP</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#parallel-computing-models-22","title":"Parallel Computing Models (2/2)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#distributed-memory","title":"Distributed Memory","text":"<pre><code>CPU\u2500\u2500Memory   CPU\u2500\u2500Memory\n    \u2502             \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500Network\u2500\u2518\n    \u2502             \u2502\nCPU\u2500\u2500Memory   CPU\u2500\u2500Memory\n</code></pre> <ul> <li>Each processor has private memory</li> <li>Better scalability</li> <li>More complex programming</li> <li>Example: MPI</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#memory-architecture-deep-dive-13","title":"Memory Architecture Deep Dive (\u2153)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#cache-hierarchy","title":"Cache Hierarchy","text":"<pre><code>// Example showing cache effects\nvoid demonstrateCacheEffects() {\n    const int SIZE = 1024 * 1024;\n    int* arr = new int[SIZE];\n\n    // Sequential access (cache-friendly)\n    Timer t1;\n    for(int i = 0; i &lt; SIZE; i++) {\n        arr[i] = i;\n    }\n    double sequential_time = t1.elapsed();\n\n    // Random access (cache-unfriendly)\n    Timer t2;\n    for(int i = 0; i &lt; SIZE; i++) {\n        arr[(i * 16) % SIZE] = i;\n    }\n    double random_time = t2.elapsed();\n\n    printf(\"Sequential/Random time ratio: %f\\n\", \n           random_time/sequential_time);\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#memory-architecture-deep-dive-23","title":"Memory Architecture Deep Dive (\u2154)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#false-sharing-example","title":"False Sharing Example","text":"<pre><code>#include &lt;omp.h&gt;\n\n// Bad example with false sharing\nvoid falseSharing() {\n    int data[4];\n    #pragma omp parallel for\n    for(int i = 0; i &lt; 4; i++) {\n        for(int j = 0; j &lt; 1000000; j++) {\n            data[i]++; // Adjacent elements share cache line\n        }\n    }\n}\n\n// Better version avoiding false sharing\nvoid avoidFalseSharing() {\n    struct PaddedInt {\n        int value;\n        char padding[60]; // Separate cache lines\n    };\n    PaddedInt data[4];\n\n    #pragma omp parallel for\n    for(int i = 0; i &lt; 4; i++) {\n        for(int j = 0; j &lt; 1000000; j++) {\n            data[i].value++;\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#memory-architecture-deep-dive-33","title":"Memory Architecture Deep Dive (3/3)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#numa-awareness","title":"NUMA Awareness","text":"<pre><code>// NUMA-aware allocation\nvoid numaAwareAllocation() {\n    #pragma omp parallel\n    {\n        // Each thread allocates its own memory\n        std::vector&lt;double&gt; local_data(1000000);\n\n        // Process local data\n        #pragma omp for\n        for(int i = 0; i &lt; local_data.size(); i++) {\n            local_data[i] = heavyComputation(i);\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#openmp-fundamentals-14","title":"OpenMP Fundamentals (\u00bc)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#basic-parallel-regions","title":"Basic Parallel Regions","text":"<pre><code>#include &lt;omp.h&gt;\n\nvoid basicParallelRegion() {\n    #pragma omp parallel\n    {\n        // This code runs in parallel\n        int thread_id = omp_get_thread_num();\n\n        #pragma omp critical\n        std::cout &lt;&lt; \"Thread \" &lt;&lt; thread_id &lt;&lt; \" starting\\n\";\n\n        // Do some work\n        heavyComputation();\n\n        #pragma omp critical\n        std::cout &lt;&lt; \"Thread \" &lt;&lt; thread_id &lt;&lt; \" finished\\n\";\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#openmp-fundamentals-24","title":"OpenMP Fundamentals (2/4)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#work-sharing-constructs","title":"Work Sharing Constructs","text":"<pre><code>void workSharing() {\n    const int SIZE = 1000000;\n    std::vector&lt;double&gt; data(SIZE);\n\n    // Parallel for loop\n    #pragma omp parallel for schedule(dynamic, 1000)\n    for(int i = 0; i &lt; SIZE; i++) {\n        data[i] = heavyComputation(i);\n    }\n\n    // Parallel sections\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        { task1(); }\n\n        #pragma omp section\n        { task2(); }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#openmp-fundamentals-34","title":"OpenMP Fundamentals (\u00be)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#data-sharing","title":"Data Sharing","text":"<pre><code>void dataSharing() {\n    int shared_var = 0;\n    int private_var = 0;\n\n    #pragma omp parallel private(private_var) \\\n                         shared(shared_var)\n    {\n        private_var = omp_get_thread_num(); // Each thread has its copy\n\n        #pragma omp critical\n        shared_var += private_var; // Updates shared variable\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#openmp-fundamentals-44","title":"OpenMP Fundamentals (4/4)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#synchronization","title":"Synchronization","text":"<pre><code>void synchronization() {\n    #pragma omp parallel\n    {\n        // Barrier synchronization\n        #pragma omp barrier\n\n        // Critical section\n        #pragma omp critical\n        {\n            // Exclusive access\n        }\n\n        // Atomic operation\n        #pragma omp atomic\n        counter++;\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#practical-workshop-13","title":"Practical Workshop (\u2153)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#matrix-multiplication","title":"Matrix Multiplication","text":"<pre><code>void matrixMultiply(const std::vector&lt;std::vector&lt;double&gt;&gt;&amp; A,\n                   const std::vector&lt;std::vector&lt;double&gt;&gt;&amp; B,\n                   std::vector&lt;std::vector&lt;double&gt;&gt;&amp; C) {\n    int N = A.size();\n\n    #pragma omp parallel for collapse(2)\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            double sum = 0.0;\n            for(int k = 0; k &lt; N; k++) {\n                sum += A[i][k] * B[k][j];\n            }\n            C[i][j] = sum;\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#practical-workshop-23","title":"Practical Workshop (\u2154)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#performance-comparison","title":"Performance Comparison","text":"<pre><code>void comparePerformance() {\n    const int N = 1000;\n    auto A = generateRandomMatrix(N);\n    auto B = generateRandomMatrix(N);\n    auto C1 = createEmptyMatrix(N);\n    auto C2 = createEmptyMatrix(N);\n\n    // Sequential version\n    Timer t1;\n    matrixMultiplySequential(A, B, C1);\n    double sequential_time = t1.elapsed();\n\n    // Parallel version\n    Timer t2;\n    matrixMultiply(A, B, C2);\n    double parallel_time = t2.elapsed();\n\n    printf(\"Speedup: %f\\n\", sequential_time/parallel_time);\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#practical-workshop-33","title":"Practical Workshop (3/3)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#exercise-tasks","title":"Exercise Tasks","text":"<ol> <li>Implement matrix multiplication</li> <li>Measure performance with different matrix sizes</li> <li>Try different scheduling strategies</li> <li>Plot performance results</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#2-development-environment","title":"2. Development Environment","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#required-hardware","title":"Required Hardware","text":"<ul> <li>Multi-core processor</li> <li>16GB RAM (recommended)</li> <li>100GB free disk space</li> <li>Windows 10/11 (version 2004+)</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#required-software","title":"Required Software","text":"<ol> <li>Visual Studio Community 2022</li> <li>Windows Subsystem for Linux (WSL2)</li> <li>Ubuntu distribution</li> <li>Git for Windows</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#installation-steps-13","title":"Installation Steps (\u2153)","text":"<p>Visual Studio 2022: - Download from visualstudio.microsoft.com - Select workloads:   - \"Desktop development with C++\"   - \"Linux development with C++\"</p>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#installation-steps-23","title":"Installation Steps (\u2154)","text":"<p>WSL2 Setup: </p><pre><code># Enable WSL\nwsl --install\n\n# Update WSL\nwsl --update\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#installation-steps-33","title":"Installation Steps (3/3)","text":"<p>Ubuntu Setup: - Install from Microsoft Store - First time setup:   </p><pre><code># Update package list\nsudo apt update\n\n# Install development tools\nsudo apt install build-essential\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#3-introduction-to-parallel-programming","title":"3. Introduction to Parallel Programming","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#what-is-parallel-programming-12","title":"What is Parallel Programming? (\u00bd)","text":"<p>Parallel programming is the technique of writing programs that: - Execute multiple tasks simultaneously - Utilize multiple computational resources - Improve performance through parallelization</p>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#what-is-parallel-programming-22","title":"What is Parallel Programming? (2/2)","text":"<p>Key Concepts: - Task decomposition - Data distribution - Load balancing - Synchronization</p>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#4-first-parallel-program","title":"4. First Parallel Program","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#hello-world-example","title":"Hello World Example","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;omp.h&gt;\n\nint main() {\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int total_threads = omp_get_num_threads();\n\n        printf(\"Hello from thread %d of %d!\\n\", \n               thread_id, total_threads);\n    }\n    return 0;\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#compilation-steps","title":"Compilation Steps","text":"<p>Visual Studio: </p><pre><code># Create new project\nmkdir parallel_hello\ncd parallel_hello\n\n# Compile with OpenMP\ncl /openmp hello.cpp\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#running-and-testing","title":"Running and Testing","text":"<p>Windows: </p><pre><code>hello.exe\n</code></pre> <p>Linux/WSL: </p><pre><code>./hello\n</code></pre> <p>Expected Output: </p><pre><code>Hello from thread 0 of 4!\nHello from thread 2 of 4!\nHello from thread 3 of 4!\nHello from thread 1 of 4!\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#5-understanding-hardware","title":"5. Understanding Hardware","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#cpu-architecture","title":"CPU Architecture","text":"<pre><code>CPU\n\u251c\u2500\u2500 Core 0\n\u2502   \u251c\u2500\u2500 L1 Cache\n\u2502   \u2514\u2500\u2500 L2 Cache\n\u251c\u2500\u2500 Core 1\n\u2502   \u251c\u2500\u2500 L1 Cache\n\u2502   \u2514\u2500\u2500 L2 Cache\n\u2514\u2500\u2500 Shared L3 Cache\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#memory-access-patterns","title":"Memory Access Patterns","text":"<pre><code>void measureMemoryAccess() {\n    const int SIZE = 1000000;\n    std::vector&lt;int&gt; data(SIZE);\n\n    // Sequential access\n    auto start = std::chrono::high_resolution_clock::now();\n    for(int i = 0; i &lt; SIZE; i++) {\n        data[i] = i;\n    }\n    auto end = std::chrono::high_resolution_clock::now();\n\n    // Random access\n    start = std::chrono::high_resolution_clock::now();\n    for(int i = 0; i &lt; SIZE; i++) {\n        data[(i * 16) % SIZE] = i;\n    }\n    end = std::chrono::high_resolution_clock::now();\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#6-parallel-patterns","title":"6. Parallel Patterns","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#data-parallelism-example","title":"Data Parallelism Example","text":"<pre><code>#include &lt;omp.h&gt;\n#include &lt;vector&gt;\n\nvoid vectorAdd(const std::vector&lt;int&gt;&amp; a, \n               const std::vector&lt;int&gt;&amp; b, \n               std::vector&lt;int&gt;&amp; result) {\n    #pragma omp parallel for\n    for(int i = 0; i &lt; a.size(); i++) {\n        result[i] = a[i] + b[i];\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#task-parallelism-example","title":"Task Parallelism Example","text":"<pre><code>#pragma omp parallel sections\n{\n    #pragma omp section\n    {\n        // Task 1: Matrix multiplication\n    }\n\n    #pragma omp section\n    {\n        // Task 2: File processing\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#7-performance-measurement","title":"7. Performance Measurement","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#using-the-timer-class","title":"Using the Timer Class","text":"<pre><code>class Timer {\n    std::chrono::high_resolution_clock::time_point start;\npublic:\n    Timer() : start(std::chrono::high_resolution_clock::now()) {}\n\n    double elapsed() {\n        auto end = std::chrono::high_resolution_clock::now();\n        return std::chrono::duration&lt;double&gt;(end - start).count();\n    }\n};\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#measuring-parallel-performance","title":"Measuring Parallel Performance","text":"<pre><code>void measureParallelPerformance() {\n    const int SIZE = 100000000;\n    std::vector&lt;double&gt; data(SIZE);\n\n    Timer t;\n    #pragma omp parallel for\n    for(int i = 0; i &lt; SIZE; i++) {\n        data[i] = std::sin(i) * std::cos(i);\n    }\n    std::cout &lt;&lt; \"Time: \" &lt;&lt; t.elapsed() &lt;&lt; \"s\\n\";\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#8-homework","title":"8. Homework","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#assignment-1-environment-setup","title":"Assignment 1: Environment Setup","text":"<ol> <li>Screenshots of installations</li> <li>Version information</li> <li>Example program results</li> <li>Issue resolution documentation</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#assignment-2-performance-analysis","title":"Assignment 2: Performance Analysis","text":"<ol> <li>Process &amp; thread ID printing</li> <li>Execution time measurements</li> <li>Performance graphs</li> <li>Analysis report</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#9-resources","title":"9. Resources","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#documentation","title":"Documentation","text":"<ul> <li>OpenMP API Specification</li> <li>Visual Studio Parallel Programming</li> <li>WSL Documentation</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#books-and-tutorials","title":"Books and Tutorials","text":"<ul> <li>\"Introduction to Parallel Programming\"</li> <li>\"Using OpenMP\"</li> <li>Online courses</li> </ul>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#next-week-preview","title":"Next Week Preview","text":"<p>We will cover: - Advanced parallel patterns - Performance analysis - OpenMP features - Practical exercises</p>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#10-advanced-openmp-features","title":"10. Advanced OpenMP Features","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#nested-parallelism-12","title":"Nested Parallelism (\u00bd)","text":"<pre><code>#include &lt;omp.h&gt;\n\nvoid nestedParallelExample() {\n    omp_set_nested(1); // Enable nested parallelism\n\n    #pragma omp parallel num_threads(2)\n    {\n        int outer_id = omp_get_thread_num();\n\n        #pragma omp parallel num_threads(2)\n        {\n            int inner_id = omp_get_thread_num();\n            printf(\"Outer thread %d, Inner thread %d\\n\", \n                   outer_id, inner_id);\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#nested-parallelism-22","title":"Nested Parallelism (2/2)","text":"<p>Expected Output: </p><pre><code>Outer thread 0, Inner thread 0\nOuter thread 0, Inner thread 1\nOuter thread 1, Inner thread 0\nOuter thread 1, Inner thread 1\n</code></pre> <p>Benefits: - Hierarchical parallelism - Better resource utilization - Complex parallel patterns</p>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#task-based-parallelism-13","title":"Task-Based Parallelism (\u2153)","text":"<pre><code>void taskBasedExample() {\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            #pragma omp task\n            heavyTask1();\n\n            #pragma omp task\n            heavyTask2();\n\n            #pragma omp taskwait\n            printf(\"All tasks completed\\n\");\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#task-based-parallelism-23","title":"Task-Based Parallelism (\u2154)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#fibonacci-example","title":"Fibonacci Example","text":"<pre><code>int parallel_fib(int n) {\n    if (n &lt; 30) return fib_sequential(n);\n\n    int x, y;\n    #pragma omp task shared(x)\n    x = parallel_fib(n - 1);\n\n    #pragma omp task shared(y)\n    y = parallel_fib(n - 2);\n\n    #pragma omp taskwait\n    return x + y;\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#task-based-parallelism-33","title":"Task-Based Parallelism (3/3)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#task-priority","title":"Task Priority","text":"<pre><code>void priorityTasks() {\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            #pragma omp task priority(0)\n            lowPriorityTask();\n\n            #pragma omp task priority(100)\n            highPriorityTask();\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#11-performance-optimization-techniques","title":"11. Performance Optimization Techniques","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#loop-optimization-13","title":"Loop Optimization (\u2153)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#loop-scheduling","title":"Loop Scheduling","text":"<pre><code>void demonstrateScheduling() {\n    const int SIZE = 1000000;\n\n    // Static scheduling\n    #pragma omp parallel for schedule(static)\n    for(int i = 0; i &lt; SIZE; i++)\n        work_static(i);\n\n    // Dynamic scheduling\n    #pragma omp parallel for schedule(dynamic, 1000)\n    for(int i = 0; i &lt; SIZE; i++)\n        work_dynamic(i);\n\n    // Guided scheduling\n    #pragma omp parallel for schedule(guided)\n    for(int i = 0; i &lt; SIZE; i++)\n        work_guided(i);\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#loop-optimization-23","title":"Loop Optimization (\u2154)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#loop-collapse","title":"Loop Collapse","text":"<pre><code>void matrixOperations() {\n    const int N = 1000;\n    double matrix[N][N];\n\n    // Without collapse\n    #pragma omp parallel for\n    for(int i = 0; i &lt; N; i++)\n        for(int j = 0; j &lt; N; j++)\n            matrix[i][j] = compute(i, j);\n\n    // With collapse\n    #pragma omp parallel for collapse(2)\n    for(int i = 0; i &lt; N; i++)\n        for(int j = 0; j &lt; N; j++)\n            matrix[i][j] = compute(i, j);\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#loop-optimization-33","title":"Loop Optimization (3/3)","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#simd-directives","title":"SIMD Directives","text":"<pre><code>void simdExample() {\n    const int N = 1000000;\n    float a[N], b[N], c[N];\n\n    #pragma omp parallel for simd\n    for(int i = 0; i &lt; N; i++) {\n        c[i] = a[i] * b[i];\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#12-common-parallel-programming-patterns","title":"12. Common Parallel Programming Patterns","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#pipeline-pattern-12","title":"Pipeline Pattern (\u00bd)","text":"<pre><code>struct Data {\n    // ... data members\n};\n\nvoid pipelinePattern() {\n    std::queue&lt;Data&gt; queue1, queue2;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section // Stage 1\n        {\n            while(hasInput()) {\n                Data d = readInput();\n                queue1.push(d);\n            }\n        }\n\n        #pragma omp section // Stage 2\n        {\n            while(true) {\n                Data d = queue1.pop();\n                process(d);\n                queue2.push(d);\n            }\n        }\n\n        #pragma omp section // Stage 3\n        {\n            while(true) {\n                Data d = queue2.pop();\n                writeOutput(d);\n            }\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#pipeline-pattern-22","title":"Pipeline Pattern (2/2)","text":"<p>Benefits: - Improved throughput - Better resource utilization - Natural for streaming data</p> <p>Challenges: - Load balancing - Queue management - Termination conditions</p>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#13-debugging-parallel-programs","title":"13. Debugging Parallel Programs","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#common-issues-12","title":"Common Issues (\u00bd)","text":"<ol> <li>Race Conditions <pre><code>// Bad code\nint counter = 0;\n#pragma omp parallel for\nfor(int i = 0; i &lt; N; i++)\n    counter++; // Race condition!\n\n// Fixed code\nint counter = 0;\n#pragma omp parallel for reduction(+:counter)\nfor(int i = 0; i &lt; N; i++)\n    counter++;\n</code></pre></li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#common-issues-22","title":"Common Issues (2/2)","text":"<ol> <li>Deadlocks <pre><code>// Potential deadlock\n#pragma omp parallel sections\n{\n    #pragma omp section\n    {\n        #pragma omp critical(A)\n        {\n            #pragma omp critical(B)\n            { /* ... */ }\n        }\n    }\n\n    #pragma omp section\n    {\n        #pragma omp critical(B)\n        {\n            #pragma omp critical(A)\n            { /* ... */ }\n        }\n    }\n}\n</code></pre></li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#14-real-world-applications","title":"14. Real-World Applications","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#image-processing-example","title":"Image Processing Example","text":"<pre><code>void parallelImageProcessing(unsigned char* image, \n                           int width, int height) {\n    #pragma omp parallel for collapse(2)\n    for(int y = 0; y &lt; height; y++) {\n        for(int x = 0; x &lt; width; x++) {\n            int idx = (y * width + x) * 3;\n\n            // Apply Gaussian blur\n            float sum_r = 0, sum_g = 0, sum_b = 0;\n            float weight_sum = 0;\n\n            for(int ky = -2; ky &lt;= 2; ky++) {\n                for(int kx = -2; kx &lt;= 2; kx++) {\n                    int ny = y + ky;\n                    int nx = x + kx;\n\n                    if(ny &gt;= 0 &amp;&amp; ny &lt; height &amp;&amp; \n                       nx &gt;= 0 &amp;&amp; nx &lt; width) {\n                        float weight = gaussian(kx, ky);\n                        int nidx = (ny * width + nx) * 3;\n\n                        sum_r += image[nidx + 0] * weight;\n                        sum_g += image[nidx + 1] * weight;\n                        sum_b += image[nidx + 2] * weight;\n                        weight_sum += weight;\n                    }\n                }\n            }\n\n            // Store result\n            image[idx + 0] = sum_r / weight_sum;\n            image[idx + 1] = sum_g / weight_sum;\n            image[idx + 2] = sum_b / weight_sum;\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#monte-carlo-simulation","title":"Monte Carlo Simulation","text":"<pre><code>double parallelMonteCarlo(int iterations) {\n    long inside_circle = 0;\n\n    #pragma omp parallel reduction(+:inside_circle)\n    {\n        unsigned int seed = omp_get_thread_num();\n\n        #pragma omp for\n        for(int i = 0; i &lt; iterations; i++) {\n            double x = (double)rand_r(&amp;seed) / RAND_MAX;\n            double y = (double)rand_r(&amp;seed) / RAND_MAX;\n\n            if(x*x + y*y &lt;= 1.0)\n                inside_circle++;\n        }\n    }\n\n    return 4.0 * inside_circle / iterations;\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#15-advanced-workshop","title":"15. Advanced Workshop","text":"","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#project-parallel-sort-implementation","title":"Project: Parallel Sort Implementation","text":"<ol> <li>Sequential Quicksort</li> <li>Parallel Quicksort</li> <li>Performance Comparison</li> <li>Visualization Tools</li> </ol>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#workshop-tasks-13","title":"Workshop Tasks (\u2153)","text":"<pre><code>// Sequential Quicksort\nvoid quicksort(int* arr, int left, int right) {\n    if(left &lt; right) {\n        int pivot = partition(arr, left, right);\n        quicksort(arr, left, pivot - 1);\n        quicksort(arr, pivot + 1, right);\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#workshop-tasks-23","title":"Workshop Tasks (\u2154)","text":"<pre><code>// Parallel Quicksort\nvoid parallel_quicksort(int* arr, int left, int right) {\n    if(left &lt; right) {\n        if(right - left &lt; THRESHOLD) {\n            quicksort(arr, left, right);\n            return;\n        }\n\n        int pivot = partition(arr, left, right);\n\n        #pragma omp task\n        parallel_quicksort(arr, left, pivot - 1);\n\n        #pragma omp task\n        parallel_quicksort(arr, pivot + 1, right);\n\n        #pragma omp taskwait\n    }\n}\n</code></pre>","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-1/cen429-week-1/#workshop-tasks-33","title":"Workshop Tasks (3/3)","text":"<p>Performance Analysis Tools: </p><pre><code>void analyzePerformance() {\n    const int SIZES[] = {1000, 10000, 100000, 1000000};\n    const int THREADS[] = {1, 2, 4, 8, 16};\n\n    for(int size : SIZES) {\n        for(int threads : THREADS) {\n            omp_set_num_threads(threads);\n\n            // Run and measure\n            auto arr = generateRandomArray(size);\n            Timer t;\n\n            #pragma omp parallel\n            {\n                #pragma omp single\n                parallel_quicksort(arr.data(), 0, size-1);\n            }\n\n            double time = t.elapsed();\n            printf(\"Size: %d, Threads: %d, Time: %f\\n\",\n                   size, threads, time);\n        }\n    }\n}\n</code></pre> \\[ End-Of-Week-1 \\]","tags":["cen310-week-1","parallel-programming","course-introduction","development-environment","spring-2025"]},{"location":"week-10/cen310-week-10/","title":"CEN310 Parallel Programming Week-10","text":""},{"location":"week-10/cen310-week-10/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-10/cen310-week-10/#week-10-parallel-algorithm-design-gpu-basics","title":"Week-10 (Parallel Algorithm Design &amp; GPU Basics)","text":""},{"location":"week-10/cen310-week-10/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-10/cen310-week-10/#overview","title":"Overview","text":""},{"location":"week-10/cen310-week-10/#topics","title":"Topics","text":"<ol> <li>Parallel Algorithm Design Strategies</li> <li>Decomposition Techniques</li> <li>GPU Architecture Fundamentals</li> <li>Introduction to CUDA Programming</li> </ol>"},{"location":"week-10/cen310-week-10/#objectives","title":"Objectives","text":"<ul> <li>Understand parallel algorithm design principles</li> <li>Learn data decomposition methods</li> <li>Explore GPU architecture</li> <li>Get started with CUDA programming</li> </ul>"},{"location":"week-10/cen310-week-10/#1-parallel-algorithm-design-strategies","title":"1. Parallel Algorithm Design Strategies","text":""},{"location":"week-10/cen310-week-10/#design-patterns","title":"Design Patterns","text":"<ul> <li>Task parallelism</li> <li>Data parallelism</li> <li>Pipeline parallelism</li> <li>Divide and conquer</li> </ul>"},{"location":"week-10/cen310-week-10/#example-matrix-multiplication","title":"Example: Matrix Multiplication","text":"<pre><code>// Sequential version\nvoid matrix_multiply(float* A, float* B, float* C, int N) {\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            float sum = 0.0f;\n            for(int k = 0; k &lt; N; k++) {\n                sum += A[i*N + k] * B[k*N + j];\n            }\n            C[i*N + j] = sum;\n        }\n    }\n}\n\n// Parallel version\n#pragma omp parallel for collapse(2)\nvoid parallel_matrix_multiply(float* A, float* B, float* C, int N) {\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            float sum = 0.0f;\n            for(int k = 0; k &lt; N; k++) {\n                sum += A[i*N + k] * B[k*N + j];\n            }\n            C[i*N + j] = sum;\n        }\n    }\n}\n</code></pre>"},{"location":"week-10/cen310-week-10/#2-decomposition-techniques","title":"2. Decomposition Techniques","text":""},{"location":"week-10/cen310-week-10/#data-decomposition","title":"Data Decomposition","text":"<ul> <li>Block decomposition</li> <li>Cyclic decomposition</li> <li>Block-cyclic decomposition</li> </ul>"},{"location":"week-10/cen310-week-10/#example-array-processing","title":"Example: Array Processing","text":"<pre><code>// Block decomposition\nvoid block_decomposition(float* data, int size, int num_blocks) {\n    int block_size = size / num_blocks;\n    #pragma omp parallel for\n    for(int b = 0; b &lt; num_blocks; b++) {\n        int start = b * block_size;\n        int end = (b == num_blocks-1) ? size : (b+1) * block_size;\n        for(int i = start; i &lt; end; i++) {\n            // Process data[i]\n        }\n    }\n}\n</code></pre>"},{"location":"week-10/cen310-week-10/#3-gpu-architecture-fundamentals","title":"3. GPU Architecture Fundamentals","text":""},{"location":"week-10/cen310-week-10/#hardware-components","title":"Hardware Components","text":"<ul> <li>Streaming Multiprocessors (SMs)</li> <li>CUDA Cores</li> <li>Memory Hierarchy</li> <li>Warp Scheduling</li> </ul>"},{"location":"week-10/cen310-week-10/#memory-types","title":"Memory Types","text":"<pre><code>CPU (Host)           GPU (Device)\n   \u2193                      \u2193\nHost Memory          Global Memory\n                         \u2193\n                    Shared Memory\n                         \u2193\n                     L1 Cache\n                         \u2193\n                    Registers\n</code></pre>"},{"location":"week-10/cen310-week-10/#4-introduction-to-cuda-programming","title":"4. Introduction to CUDA Programming","text":""},{"location":"week-10/cen310-week-10/#basic-concepts","title":"Basic Concepts","text":"<ul> <li>Kernels</li> <li>Threads</li> <li>Blocks</li> <li>Grids</li> </ul>"},{"location":"week-10/cen310-week-10/#hello-world-example","title":"Hello World Example","text":"<pre><code>#include &lt;cuda_runtime.h&gt;\n#include &lt;stdio.h&gt;\n\n__global__ void hello_kernel() {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    printf(\"Hello from thread %d\\n\", idx);\n}\n\nint main() {\n    // Launch kernel with 1 block of 256 threads\n    hello_kernel&lt;&lt;&lt;1, 256&gt;&gt;&gt;();\n    cudaDeviceSynchronize();\n    return 0;\n}\n</code></pre>"},{"location":"week-10/cen310-week-10/#cuda-memory-management","title":"CUDA Memory Management","text":""},{"location":"week-10/cen310-week-10/#memory-operations","title":"Memory Operations","text":"<pre><code>// Allocate device memory\nfloat *d_data;\ncudaMalloc(&amp;d_data, size * sizeof(float));\n\n// Copy data to device\ncudaMemcpy(d_data, h_data, size * sizeof(float), \n           cudaMemcpyHostToDevice);\n\n// Copy results back\ncudaMemcpy(h_result, d_result, size * sizeof(float), \n           cudaMemcpyDeviceToHost);\n\n// Free device memory\ncudaFree(d_data);\n</code></pre>"},{"location":"week-10/cen310-week-10/#vector-addition-example","title":"Vector Addition Example","text":"<pre><code>__global__ void vector_add(float* a, float* b, float* c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int N = 1000000;\n    size_t size = N * sizeof(float);\n\n    // Allocate host memory\n    float *h_a = (float*)malloc(size);\n    float *h_b = (float*)malloc(size);\n    float *h_c = (float*)malloc(size);\n\n    // Initialize arrays\n    for(int i = 0; i &lt; N; i++) {\n        h_a[i] = rand() / (float)RAND_MAX;\n        h_b[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Allocate device memory\n    float *d_a, *d_b, *d_c;\n    cudaMalloc(&amp;d_a, size);\n    cudaMalloc(&amp;d_b, size);\n    cudaMalloc(&amp;d_c, size);\n\n    // Copy to device\n    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n\n    // Launch kernel\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n    vector_add&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n    // Copy result back\n    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n\n    // Cleanup\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n    free(h_a);\n    free(h_b);\n    free(h_c);\n\n    return 0;\n}\n</code></pre>"},{"location":"week-10/cen310-week-10/#lab-exercise","title":"Lab Exercise","text":""},{"location":"week-10/cen310-week-10/#tasks","title":"Tasks","text":"<ol> <li>Implement matrix multiplication using CUDA</li> <li>Compare performance with CPU version</li> <li>Experiment with different block sizes</li> <li>Analyze memory access patterns</li> </ol>"},{"location":"week-10/cen310-week-10/#performance-analysis","title":"Performance Analysis","text":"<ul> <li>Use nvprof for profiling</li> <li>Measure execution time</li> <li>Calculate speedup</li> <li>Monitor memory transfers</li> </ul>"},{"location":"week-10/cen310-week-10/#resources","title":"Resources","text":""},{"location":"week-10/cen310-week-10/#documentation","title":"Documentation","text":"<ul> <li>CUDA Programming Guide</li> <li>CUDA Best Practices Guide</li> <li>NVIDIA Developer Blog</li> </ul>"},{"location":"week-10/cen310-week-10/#tools","title":"Tools","text":"<ul> <li>NVIDIA NSight</li> <li>CUDA Toolkit</li> <li>Visual Profiler</li> </ul>"},{"location":"week-10/cen310-week-10/#questions-discussion","title":"Questions &amp; Discussion","text":""},{"location":"week-11/cen310-week-11/","title":"CEN310 Parallel Programming Week-11","text":""},{"location":"week-11/cen310-week-11/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-11/cen310-week-11/#week-11-advanced-gpu-programming","title":"Week-11 (Advanced GPU Programming)","text":""},{"location":"week-11/cen310-week-11/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-11/cen310-week-11/#overview","title":"Overview","text":""},{"location":"week-11/cen310-week-11/#topics","title":"Topics","text":"<ol> <li>CUDA Memory Model</li> <li>Shared Memory Optimization</li> <li>Thread Synchronization</li> <li>Performance Optimization Techniques</li> </ol>"},{"location":"week-11/cen310-week-11/#objectives","title":"Objectives","text":"<ul> <li>Understand CUDA memory hierarchy</li> <li>Learn shared memory usage</li> <li>Master thread synchronization</li> <li>Implement optimization strategies</li> </ul>"},{"location":"week-11/cen310-week-11/#1-cuda-memory-model","title":"1. CUDA Memory Model","text":""},{"location":"week-11/cen310-week-11/#memory-types","title":"Memory Types","text":"<ul> <li>Global Memory</li> <li>Shared Memory</li> <li>Constant Memory</li> <li>Texture Memory</li> <li>Registers</li> </ul>"},{"location":"week-11/cen310-week-11/#memory-access-patterns","title":"Memory Access Patterns","text":"<pre><code>// Coalesced memory access example\n__global__ void coalesced_access(float* data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        // Coalesced access pattern\n        float value = data[idx];\n        // Process value\n        data[idx] = value * 2.0f;\n    }\n}\n</code></pre>"},{"location":"week-11/cen310-week-11/#2-shared-memory-optimization","title":"2. Shared Memory Optimization","text":""},{"location":"week-11/cen310-week-11/#using-shared-memory","title":"Using Shared Memory","text":"<pre><code>__global__ void matrix_multiply(float* A, float* B, float* C, int N) {\n    __shared__ float sharedA[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float sharedB[BLOCK_SIZE][BLOCK_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for(int tile = 0; tile &lt; N/BLOCK_SIZE; tile++) {\n        // Load data into shared memory\n        sharedA[threadIdx.y][threadIdx.x] = \n            A[row * N + tile * BLOCK_SIZE + threadIdx.x];\n        sharedB[threadIdx.y][threadIdx.x] = \n            B[(tile * BLOCK_SIZE + threadIdx.y) * N + col];\n\n        __syncthreads();\n\n        // Compute using shared memory\n        for(int k = 0; k &lt; BLOCK_SIZE; k++) {\n            sum += sharedA[threadIdx.y][k] * sharedB[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    C[row * N + col] = sum;\n}\n</code></pre>"},{"location":"week-11/cen310-week-11/#3-thread-synchronization","title":"3. Thread Synchronization","text":""},{"location":"week-11/cen310-week-11/#synchronization-methods","title":"Synchronization Methods","text":"<ul> <li>Block-level synchronization</li> <li>Grid-level synchronization</li> <li>Atomic operations</li> </ul>"},{"location":"week-11/cen310-week-11/#example-atomic-operations","title":"Example: Atomic Operations","text":"<pre><code>__global__ void histogram(int* data, int* hist, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        atomicAdd(&amp;hist[data[idx]], 1);\n    }\n}\n</code></pre>"},{"location":"week-11/cen310-week-11/#4-performance-optimization","title":"4. Performance Optimization","text":""},{"location":"week-11/cen310-week-11/#optimization-techniques","title":"Optimization Techniques","text":"<ol> <li>Memory Coalescing</li> <li>Bank Conflict Avoidance</li> <li>Occupancy Optimization</li> <li>Loop Unrolling</li> </ol>"},{"location":"week-11/cen310-week-11/#example-bank-conflict-resolution","title":"Example: Bank Conflict Resolution","text":"<pre><code>// Bad: Bank conflicts\n__shared__ float shared_data[BLOCK_SIZE][BLOCK_SIZE];\n\n// Good: Padded to avoid bank conflicts\n__shared__ float shared_data[BLOCK_SIZE][BLOCK_SIZE + 1];\n</code></pre>"},{"location":"week-11/cen310-week-11/#advanced-memory-management","title":"Advanced Memory Management","text":""},{"location":"week-11/cen310-week-11/#unified-memory","title":"Unified Memory","text":"<pre><code>// Allocate unified memory\nfloat* unified_data;\ncudaMallocManaged(&amp;unified_data, size);\n\n// Access from host or device\n// No explicit transfers needed\nkernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(unified_data);\n\n// Free unified memory\ncudaFree(unified_data);\n</code></pre>"},{"location":"week-11/cen310-week-11/#stream-processing","title":"Stream Processing","text":""},{"location":"week-11/cen310-week-11/#concurrent-execution","title":"Concurrent Execution","text":"<pre><code>cudaStream_t stream1, stream2;\ncudaStreamCreate(&amp;stream1);\ncudaStreamCreate(&amp;stream2);\n\n// Asynchronous operations in different streams\nkernel1&lt;&lt;&lt;grid, block, 0, stream1&gt;&gt;&gt;(data1);\nkernel2&lt;&lt;&lt;grid, block, 0, stream2&gt;&gt;&gt;(data2);\n\ncudaStreamSynchronize(stream1);\ncudaStreamSynchronize(stream2);\n\ncudaStreamDestroy(stream1);\ncudaStreamDestroy(stream2);\n</code></pre>"},{"location":"week-11/cen310-week-11/#dynamic-parallelism","title":"Dynamic Parallelism","text":""},{"location":"week-11/cen310-week-11/#nested-kernel-launch","title":"Nested Kernel Launch","text":"<pre><code>__global__ void child_kernel(float* data) {\n    // Child kernel code\n}\n\n__global__ void parent_kernel(float* data) {\n    if(threadIdx.x == 0) {\n        child_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(data);\n        cudaDeviceSynchronize();\n    }\n}\n</code></pre>"},{"location":"week-11/cen310-week-11/#lab-exercise","title":"Lab Exercise","text":""},{"location":"week-11/cen310-week-11/#tasks","title":"Tasks","text":"<ol> <li>Implement matrix multiplication with shared memory</li> <li>Compare performance with global memory version</li> <li>Analyze memory access patterns</li> <li>Optimize for different GPU architectures</li> </ol>"},{"location":"week-11/cen310-week-11/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Execution time</li> <li>Memory throughput</li> <li>Occupancy</li> <li>Cache hit rate</li> </ul>"},{"location":"week-11/cen310-week-11/#resources","title":"Resources","text":""},{"location":"week-11/cen310-week-11/#documentation","title":"Documentation","text":"<ul> <li>CUDA C++ Programming Guide</li> <li>CUDA Best Practices Guide</li> <li>GPU Computing Webinars</li> </ul>"},{"location":"week-11/cen310-week-11/#tools","title":"Tools","text":"<ul> <li>Nsight Compute</li> <li>CUDA Profiler</li> <li>Visual Studio GPU Debugger</li> </ul>"},{"location":"week-11/cen310-week-11/#questions-discussion","title":"Questions &amp; Discussion","text":""},{"location":"week-12/cen310-week-12/","title":"CEN310 Parallel Programming Week-12","text":""},{"location":"week-12/cen310-week-12/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-12/cen310-week-12/#week-12-real-world-applications-i","title":"Week-12 (Real-world Applications I)","text":""},{"location":"week-12/cen310-week-12/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-12/cen310-week-12/#overview","title":"Overview","text":""},{"location":"week-12/cen310-week-12/#topics","title":"Topics","text":"<ol> <li>Scientific Computing Applications</li> <li>Data Processing Applications</li> <li>Performance Optimization</li> <li>Case Studies</li> </ol>"},{"location":"week-12/cen310-week-12/#objectives","title":"Objectives","text":"<ul> <li>Apply parallel programming to real problems</li> <li>Optimize scientific computations</li> <li>Process large datasets efficiently</li> <li>Analyze real-world performance</li> </ul>"},{"location":"week-12/cen310-week-12/#1-scientific-computing-applications","title":"1. Scientific Computing Applications","text":""},{"location":"week-12/cen310-week-12/#n-body-simulation","title":"N-Body Simulation","text":"<pre><code>__global__ void calculate_forces(float4* pos, float4* vel, float4* forces, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        float4 my_pos = pos[idx];\n        float4 force = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n\n        for(int j = 0; j &lt; n; j++) {\n            if(j != idx) {\n                float4 other_pos = pos[j];\n                float3 r = make_float3(\n                    other_pos.x - my_pos.x,\n                    other_pos.y - my_pos.y,\n                    other_pos.z - my_pos.z\n                );\n                float dist = sqrtf(r.x*r.x + r.y*r.y + r.z*r.z);\n                float f = (G * my_pos.w * other_pos.w) / (dist * dist);\n                force.x += f * r.x/dist;\n                force.y += f * r.y/dist;\n                force.z += f * r.z/dist;\n            }\n        }\n        forces[idx] = force;\n    }\n}\n</code></pre>"},{"location":"week-12/cen310-week-12/#2-data-processing-applications","title":"2. Data Processing Applications","text":""},{"location":"week-12/cen310-week-12/#image-processing","title":"Image Processing","text":"<pre><code>__global__ void gaussian_blur(\n    unsigned char* input,\n    unsigned char* output,\n    int width,\n    int height,\n    float* kernel,\n    int kernel_size\n) {\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if(x &lt; width &amp;&amp; y &lt; height) {\n        float sum = 0.0f;\n        int k_radius = kernel_size / 2;\n\n        for(int ky = -k_radius; ky &lt;= k_radius; ky++) {\n            for(int kx = -k_radius; kx &lt;= k_radius; kx++) {\n                int px = min(max(x + kx, 0), width - 1);\n                int py = min(max(y + ky, 0), height - 1);\n                float kernel_val = kernel[(ky+k_radius)*kernel_size + (kx+k_radius)];\n                sum += input[py*width + px] * kernel_val;\n            }\n        }\n\n        output[y*width + x] = (unsigned char)sum;\n    }\n}\n</code></pre>"},{"location":"week-12/cen310-week-12/#3-performance-optimization","title":"3. Performance Optimization","text":""},{"location":"week-12/cen310-week-12/#memory-access-optimization","title":"Memory Access Optimization","text":"<pre><code>// Optimize matrix transpose\n__global__ void matrix_transpose(float* input, float* output, int width, int height) {\n    __shared__ float tile[BLOCK_SIZE][BLOCK_SIZE+1]; // Avoid bank conflicts\n\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if(x &lt; width &amp;&amp; y &lt; height) {\n        // Load into shared memory\n        tile[threadIdx.y][threadIdx.x] = input[y*width + x];\n        __syncthreads();\n\n        // Calculate transposed indices\n        int new_x = blockIdx.y * blockDim.y + threadIdx.x;\n        int new_y = blockIdx.x * blockDim.x + threadIdx.y;\n\n        if(new_x &lt; height &amp;&amp; new_y &lt; width) {\n            output[new_y*height + new_x] = tile[threadIdx.x][threadIdx.y];\n        }\n    }\n}\n</code></pre>"},{"location":"week-12/cen310-week-12/#4-case-studies","title":"4. Case Studies","text":""},{"location":"week-12/cen310-week-12/#monte-carlo-simulation","title":"Monte Carlo Simulation","text":"<pre><code>__global__ void monte_carlo_pi(float* points_x, float* points_y, int* inside_circle, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx &lt; n) {\n        float x = points_x[idx];\n        float y = points_y[idx];\n        float dist = x*x + y*y;\n\n        if(dist &lt;= 1.0f) {\n            atomicAdd(inside_circle, 1);\n        }\n    }\n}\n\nint main() {\n    int n = 1000000;\n    float *h_x, *h_y, *d_x, *d_y;\n    int *h_inside, *d_inside;\n\n    // Allocate and initialize memory\n    // ... (memory allocation code)\n\n    // Generate random points\n    for(int i = 0; i &lt; n; i++) {\n        h_x[i] = (float)rand()/RAND_MAX;\n        h_y[i] = (float)rand()/RAND_MAX;\n    }\n\n    // Copy data to device and run kernel\n    // ... (CUDA memory operations and kernel launch)\n\n    // Calculate pi\n    float pi = 4.0f * (*h_inside) / (float)n;\n    printf(\"Estimated Pi: %f\\n\", pi);\n\n    // Cleanup\n    // ... (memory deallocation code)\n\n    return 0;\n}\n</code></pre>"},{"location":"week-12/cen310-week-12/#lab-exercise","title":"Lab Exercise","text":""},{"location":"week-12/cen310-week-12/#tasks","title":"Tasks","text":"<ol> <li>Implement N-body simulation</li> <li>Optimize image processing kernel</li> <li>Develop Monte Carlo simulation</li> <li>Compare performance with CPU versions</li> </ol>"},{"location":"week-12/cen310-week-12/#performance-analysis","title":"Performance Analysis","text":"<ul> <li>Execution time</li> <li>Memory bandwidth</li> <li>GPU utilization</li> <li>Scaling behavior</li> </ul>"},{"location":"week-12/cen310-week-12/#resources","title":"Resources","text":""},{"location":"week-12/cen310-week-12/#documentation","title":"Documentation","text":"<ul> <li>CUDA Sample Applications</li> <li>Scientific Computing Libraries</li> <li>Performance Analysis Tools</li> </ul>"},{"location":"week-12/cen310-week-12/#tools","title":"Tools","text":"<ul> <li>NVIDIA Visual Profiler</li> <li>Parallel Computing Toolbox</li> <li>Performance Libraries</li> </ul>"},{"location":"week-12/cen310-week-12/#questions-discussion","title":"Questions &amp; Discussion","text":""},{"location":"week-13/cen310-week-13/","title":"CEN310 Parallel Programming Week-13","text":""},{"location":"week-13/cen310-week-13/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-13/cen310-week-13/#week-13-real-world-applications-ii","title":"Week-13 (Real-world Applications II)","text":""},{"location":"week-13/cen310-week-13/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-13/cen310-week-13/#overview","title":"Overview","text":""},{"location":"week-13/cen310-week-13/#topics","title":"Topics","text":"<ol> <li>Advanced Parallel Patterns</li> <li>N-body Simulations</li> <li>Matrix Computations</li> <li>Big Data Processing</li> </ol>"},{"location":"week-13/cen310-week-13/#objectives","title":"Objectives","text":"<ul> <li>Implement complex parallel patterns</li> <li>Optimize scientific simulations</li> <li>Perform large-scale matrix operations</li> <li>Process big data efficiently</li> </ul>"},{"location":"week-13/cen310-week-13/#1-advanced-parallel-patterns","title":"1. Advanced Parallel Patterns","text":""},{"location":"week-13/cen310-week-13/#pipeline-pattern","title":"Pipeline Pattern","text":"<pre><code>template&lt;typename T&gt;\nclass ParallelPipeline {\nprivate:\n    std::vector&lt;std::thread&gt; stages;\n    std::vector&lt;std::queue&lt;T&gt;&gt; queues;\n    std::vector&lt;std::mutex&gt; mutexes;\n    std::vector&lt;std::condition_variable&gt; cvs;\n    bool running;\n\npublic:\n    ParallelPipeline(int num_stages) {\n        queues.resize(num_stages - 1);\n        mutexes.resize(num_stages - 1);\n        cvs.resize(num_stages - 1);\n        running = true;\n    }\n\n    void add_stage(std::function&lt;void(T&amp;)&gt; stage_func, int stage_id) {\n        stages.emplace_back([this, stage_func, stage_id]() {\n            while(running) {\n                T data;\n                if(stage_id == 0) {\n                    // First stage: produce data\n                    data = produce_data();\n                } else {\n                    // Get data from previous stage\n                    std::unique_lock&lt;std::mutex&gt; lock(mutexes[stage_id-1]);\n                    cvs[stage_id-1].wait(lock, \n                        [this, stage_id]() { \n                            return !queues[stage_id-1].empty() || !running; \n                        });\n                    if(!running) break;\n                    data = queues[stage_id-1].front();\n                    queues[stage_id-1].pop();\n                    lock.unlock();\n                    cvs[stage_id-1].notify_one();\n                }\n\n                // Process data\n                stage_func(data);\n\n                if(stage_id &lt; stages.size() - 1) {\n                    // Pass to next stage\n                    std::unique_lock&lt;std::mutex&gt; lock(mutexes[stage_id]);\n                    queues[stage_id].push(data);\n                    lock.unlock();\n                    cvs[stage_id].notify_one();\n                }\n            }\n        });\n    }\n\n    void start() {\n        for(auto&amp; stage : stages) {\n            stage.join();\n        }\n    }\n\n    void stop() {\n        running = false;\n        for(auto&amp; cv : cvs) {\n            cv.notify_all();\n        }\n    }\n};\n</code></pre>"},{"location":"week-13/cen310-week-13/#2-n-body-simulations","title":"2. N-body Simulations","text":""},{"location":"week-13/cen310-week-13/#barnes-hut-algorithm","title":"Barnes-Hut Algorithm","text":"<pre><code>struct Octree {\n    struct Node {\n        vec3 center;\n        float size;\n        float mass;\n        vec3 com;\n        std::vector&lt;Node*&gt; children;\n    };\n\n    Node* root;\n    float theta;\n\n    __device__ void compute_force(vec3&amp; pos, vec3&amp; force, Node* node) {\n        vec3 diff = node-&gt;com - pos;\n        float dist = length(diff);\n\n        if(node-&gt;size / dist &lt; theta || node-&gt;children.empty()) {\n            // Use approximation\n            float f = G * node-&gt;mass / (dist * dist * dist);\n            force += diff * f;\n        } else {\n            // Recurse into children\n            for(auto child : node-&gt;children) {\n                if(child != nullptr) {\n                    compute_force(pos, force, child);\n                }\n            }\n        }\n    }\n\n    __global__ void update_bodies(vec3* pos, vec3* vel, vec3* acc, \n                                float dt, int n) {\n        int idx = blockIdx.x * blockDim.x + threadIdx.x;\n        if(idx &lt; n) {\n            vec3 force(0.0f);\n            compute_force(pos[idx], force, root);\n            acc[idx] = force;\n            vel[idx] += acc[idx] * dt;\n            pos[idx] += vel[idx] * dt;\n        }\n    }\n};\n</code></pre>"},{"location":"week-13/cen310-week-13/#3-matrix-computations","title":"3. Matrix Computations","text":""},{"location":"week-13/cen310-week-13/#parallel-matrix-factorization","title":"Parallel Matrix Factorization","text":"<pre><code>__global__ void lu_factorization(float* A, int n, int k) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if(row &gt; k &amp;&amp; row &lt; n &amp;&amp; col &gt; k &amp;&amp; col &lt; n) {\n        A[row * n + col] -= A[row * n + k] * A[k * n + col] / A[k * n + k];\n    }\n}\n\nvoid parallel_lu(float* A, int n) {\n    dim3 block(16, 16);\n    dim3 grid((n + block.x - 1) / block.x, \n              (n + block.y - 1) / block.y);\n\n    for(int k = 0; k &lt; n-1; k++) {\n        lu_factorization&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, n, k);\n        cudaDeviceSynchronize();\n    }\n}\n</code></pre>"},{"location":"week-13/cen310-week-13/#4-big-data-processing","title":"4. Big Data Processing","text":""},{"location":"week-13/cen310-week-13/#parallel-data-analysis","title":"Parallel Data Analysis","text":"<pre><code>template&lt;typename T&gt;\nclass ParallelDataProcessor {\nprivate:\n    std::vector&lt;T&gt; data;\n    int num_threads;\n\npublic:\n    ParallelDataProcessor(const std::vector&lt;T&gt;&amp; input, int threads) \n        : data(input), num_threads(threads) {}\n\n    template&lt;typename Func&gt;\n    std::vector&lt;T&gt; map(Func f) {\n        std::vector&lt;T&gt; result(data.size());\n        #pragma omp parallel for num_threads(num_threads)\n        for(size_t i = 0; i &lt; data.size(); i++) {\n            result[i] = f(data[i]);\n        }\n        return result;\n    }\n\n    template&lt;typename Func&gt;\n    T reduce(Func f, T initial) {\n        T result = initial;\n        #pragma omp parallel num_threads(num_threads)\n        {\n            T local_sum = initial;\n            #pragma omp for nowait\n            for(size_t i = 0; i &lt; data.size(); i++) {\n                local_sum = f(local_sum, data[i]);\n            }\n            #pragma omp critical\n            {\n                result = f(result, local_sum);\n            }\n        }\n        return result;\n    }\n};\n</code></pre>"},{"location":"week-13/cen310-week-13/#lab-exercise","title":"Lab Exercise","text":""},{"location":"week-13/cen310-week-13/#tasks","title":"Tasks","text":"<ol> <li>Implement Barnes-Hut simulation</li> <li>Develop parallel LU factorization</li> <li>Create big data processing pipeline</li> <li>Analyze performance characteristics</li> </ol>"},{"location":"week-13/cen310-week-13/#performance-analysis","title":"Performance Analysis","text":"<ul> <li>Algorithm complexity</li> <li>Memory access patterns</li> <li>Load balancing</li> <li>Scalability testing</li> </ul>"},{"location":"week-13/cen310-week-13/#resources","title":"Resources","text":""},{"location":"week-13/cen310-week-13/#documentation","title":"Documentation","text":"<ul> <li>Advanced CUDA Programming Guide</li> <li>Parallel Algorithms Reference</li> <li>Scientific Computing Libraries</li> </ul>"},{"location":"week-13/cen310-week-13/#tools","title":"Tools","text":"<ul> <li>Performance Profilers</li> <li>Debugging Tools</li> <li>Analysis Frameworks</li> </ul>"},{"location":"week-13/cen310-week-13/#questions-discussion","title":"Questions &amp; Discussion","text":""},{"location":"week-14/cen310-week-14/","title":"CEN310 Parallel Programming Week-14","text":""},{"location":"week-14/cen310-week-14/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-14/cen310-week-14/#week-14-quiz-2","title":"Week-14 (Quiz-2)","text":""},{"location":"week-14/cen310-week-14/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-14/cen310-week-14/#quiz-2-information","title":"Quiz-2 Information","text":""},{"location":"week-14/cen310-week-14/#date-and-time","title":"Date and Time","text":"<ul> <li>Date: May 16, 2025</li> <li>Time: 09:00-12:00 (3 hours)</li> <li>Location: Regular classroom</li> </ul>"},{"location":"week-14/cen310-week-14/#format","title":"Format","text":"<ul> <li>Written examination</li> <li>Mix of theoretical and practical questions</li> <li>Both closed and open-ended questions</li> </ul>"},{"location":"week-14/cen310-week-14/#topics-covered","title":"Topics Covered","text":""},{"location":"week-14/cen310-week-14/#1-gpu-programming","title":"1. GPU Programming","text":"<ul> <li>CUDA Architecture</li> <li>Memory Hierarchy</li> <li>Thread Organization</li> <li>Performance Optimization</li> </ul>"},{"location":"week-14/cen310-week-14/#2-advanced-parallel-patterns","title":"2. Advanced Parallel Patterns","text":"<ul> <li>Pipeline Processing</li> <li>Task Parallelism</li> <li>Data Parallelism</li> <li>Hybrid Approaches</li> </ul>"},{"location":"week-14/cen310-week-14/#3-real-world-applications","title":"3. Real-world Applications","text":"<ul> <li>Scientific Computing</li> <li>Data Processing</li> <li>Matrix Operations</li> <li>N-body Simulations</li> </ul>"},{"location":"week-14/cen310-week-14/#sample-questions","title":"Sample Questions","text":""},{"location":"week-14/cen310-week-14/#theoretical-questions","title":"Theoretical Questions","text":"<ol> <li>Explain CUDA memory hierarchy and its impact on performance.</li> <li>Compare different parallel patterns and their use cases.</li> <li>Describe optimization strategies for GPU programs.</li> </ol>"},{"location":"week-14/cen310-week-14/#practical-problems","title":"Practical Problems","text":"<pre><code>// Question 1: What is the output of this CUDA program?\n__global__ void kernel(int* data) {\n    int idx = threadIdx.x;\n    __shared__ int shared_data[256];\n\n    shared_data[idx] = data[idx];\n    __syncthreads();\n\n    if(idx &lt; 128) {\n        shared_data[idx] += shared_data[idx + 128];\n    }\n    __syncthreads();\n\n    if(idx == 0) {\n        data[0] = shared_data[0];\n    }\n}\n\nint main() {\n    int* data;\n    // ... initialization code ...\n    kernel&lt;&lt;&lt;1, 256&gt;&gt;&gt;(data);\n    // ... cleanup code ...\n}\n</code></pre>"},{"location":"week-14/cen310-week-14/#preparation-guidelines","title":"Preparation Guidelines","text":""},{"location":"week-14/cen310-week-14/#1-review-materials","title":"1. Review Materials","text":"<ul> <li>Lecture slides and notes</li> <li>Lab exercises</li> <li>Sample codes</li> <li>Practice problems</li> </ul>"},{"location":"week-14/cen310-week-14/#2-focus-areas","title":"2. Focus Areas","text":"<ul> <li>CUDA Programming</li> <li>Memory Management</li> <li>Performance Optimization</li> <li>Real-world Applications</li> </ul>"},{"location":"week-14/cen310-week-14/#3-practice-exercises","title":"3. Practice Exercises","text":"<ul> <li>Write and analyze CUDA programs</li> <li>Implement parallel patterns</li> <li>Optimize existing code</li> <li>Measure performance</li> </ul>"},{"location":"week-14/cen310-week-14/#quiz-rules","title":"Quiz Rules","text":"<ol> <li>Materials Allowed</li> <li>No books or notes</li> <li>No electronic devices</li> <li> <p>Clean paper for scratch work</p> </li> <li> <p>Time Management</p> </li> <li>Read all questions carefully</li> <li>Plan your time for each section</li> <li> <p>Leave time for review</p> </li> <li> <p>Answering Questions</p> </li> <li>Show all your work</li> <li>Explain your reasoning</li> <li>Write clearly and organize your answers</li> </ol>"},{"location":"week-14/cen310-week-14/#grading-criteria","title":"Grading Criteria","text":""},{"location":"week-14/cen310-week-14/#distribution","title":"Distribution","text":"<ul> <li>Theoretical Questions: 40%</li> <li>Practical Problems: 60%</li> </ul>"},{"location":"week-14/cen310-week-14/#evaluation","title":"Evaluation","text":"<ul> <li>Understanding of concepts</li> <li>Problem-solving approach</li> <li>Code analysis and writing</li> <li>Performance considerations</li> <li>Clear explanations</li> </ul>"},{"location":"week-14/cen310-week-14/#additional-resources","title":"Additional Resources","text":""},{"location":"week-14/cen310-week-14/#review-materials","title":"Review Materials","text":"<ul> <li>CUDA Programming Guide</li> <li>Performance Optimization Guide</li> <li>Sample Applications</li> <li>Online Documentation:</li> <li>CUDA Documentation</li> <li>OpenMP Reference</li> <li>MPI Documentation</li> </ul>"},{"location":"week-14/cen310-week-14/#sample-code-repository","title":"Sample Code Repository","text":"<ul> <li>Course GitHub repository</li> <li>Example implementations</li> <li>Performance benchmarks</li> </ul>"},{"location":"week-14/cen310-week-14/#contact-information","title":"Contact Information","text":"<p>For any questions about the quiz:</p> <ul> <li>Email: ugur.coruh@erdogan.edu.tr</li> <li>Office Hours: By appointment</li> <li>Location: Engineering Faculty</li> </ul>"},{"location":"week-14/cen310-week-14/#good-luck","title":"Good Luck!","text":""},{"location":"week-15/cen310-week-15/","title":"CEN310 Parallel Programming Week-15","text":""},{"location":"week-15/cen310-week-15/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-15/cen310-week-15/#week-15-final-project-review","title":"Week-15 (Final Project Review)","text":""},{"location":"week-15/cen310-week-15/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-15/cen310-week-15/#project-review-day-schedule","title":"Project Review Day Schedule","text":""},{"location":"week-15/cen310-week-15/#morning-session-0900-1200","title":"Morning Session (09:00-12:00)","text":"<ul> <li>Project presentations (Group 1-4)</li> <li>Performance analysis discussions</li> <li>Q&amp;A sessions</li> </ul>"},{"location":"week-15/cen310-week-15/#lunch-break-1200-1300","title":"Lunch Break (12:00-13:00)","text":""},{"location":"week-15/cen310-week-15/#afternoon-session-1300-1700","title":"Afternoon Session (13:00-17:00)","text":"<ul> <li>Project presentations (Group 5-8)</li> <li>Technical demonstrations</li> <li>Final feedback</li> </ul>"},{"location":"week-15/cen310-week-15/#final-project-requirements","title":"Final Project Requirements","text":""},{"location":"week-15/cen310-week-15/#1-project-documentation","title":"1. Project Documentation","text":"<ul> <li>Comprehensive project report</li> <li>Source code documentation</li> <li>Performance analysis results</li> <li>Implementation details</li> <li>Future work proposals</li> </ul>"},{"location":"week-15/cen310-week-15/#2-technical-implementation","title":"2. Technical Implementation","text":"<ul> <li>Working parallel application</li> <li>Multiple parallel programming models</li> <li>Advanced optimization techniques</li> <li>Error handling and robustness</li> <li>Code quality and organization</li> </ul>"},{"location":"week-15/cen310-week-15/#presentation-guidelines","title":"Presentation Guidelines","text":""},{"location":"week-15/cen310-week-15/#format","title":"Format","text":"<ul> <li>30 minutes per group</li> <li>20 minutes presentation</li> <li>10 minutes Q&amp;A</li> </ul>"},{"location":"week-15/cen310-week-15/#content","title":"Content","text":"<ol> <li>Project Overview</li> <li>Problem statement</li> <li>Solution approach</li> <li> <p>Technical challenges</p> </li> <li> <p>Implementation Details</p> </li> <li>Architecture design</li> <li>Parallel strategies</li> <li> <p>Optimization techniques</p> </li> <li> <p>Results and Analysis</p> </li> <li>Performance measurements</li> <li>Scalability tests</li> <li> <p>Comparative analysis</p> </li> <li> <p>Live Demo</p> </li> <li>System setup</li> <li>Feature demonstration</li> <li>Performance showcase</li> </ol>"},{"location":"week-15/cen310-week-15/#performance-analysis-requirements","title":"Performance Analysis Requirements","text":""},{"location":"week-15/cen310-week-15/#metrics-to-cover","title":"Metrics to Cover","text":"<ul> <li>Execution time</li> <li>Speedup</li> <li>Efficiency</li> <li>Resource utilization</li> <li>Scalability</li> </ul>"},{"location":"week-15/cen310-week-15/#analysis-tools","title":"Analysis Tools","text":"<pre><code># Performance measurement examples\n$ nvprof ./cuda_program\n$ mpirun -np 4 ./mpi_program\n$ perf stat ./openmp_program\n</code></pre>"},{"location":"week-15/cen310-week-15/#project-structure-example","title":"Project Structure Example","text":"<pre><code>project/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.cpp\n\u2502   \u251c\u2500\u2500 cuda/\n\u2502   \u2502   \u251c\u2500\u2500 kernel.cu\n\u2502   \u2502   \u2514\u2500\u2500 gpu_utils.cuh\n\u2502   \u251c\u2500\u2500 mpi/\n\u2502   \u2502   \u251c\u2500\u2500 communicator.cpp\n\u2502   \u2502   \u2514\u2500\u2500 data_transfer.h\n\u2502   \u2514\u2500\u2500 openmp/\n\u2502       \u251c\u2500\u2500 parallel_loops.cpp\n\u2502       \u2514\u2500\u2500 thread_utils.h\n\u251c\u2500\u2500 include/\n\u2502   \u251c\u2500\u2500 common.h\n\u2502   \u2514\u2500\u2500 config.h\n\u251c\u2500\u2500 test/\n\u2502   \u251c\u2500\u2500 unit_tests.cpp\n\u2502   \u2514\u2500\u2500 performance_tests.cpp\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 report.pdf\n\u2502   \u2514\u2500\u2500 presentation.pptx\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 input/\n\u2502   \u2514\u2500\u2500 output/\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 build.sh\n\u2502   \u2514\u2500\u2500 run_tests.sh\n\u251c\u2500\u2500 CMakeLists.txt\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"week-15/cen310-week-15/#evaluation-criteria","title":"Evaluation Criteria","text":""},{"location":"week-15/cen310-week-15/#technical-aspects-50","title":"Technical Aspects (50%)","text":"<ul> <li>Implementation quality (15%)</li> <li>Performance optimization (15%)</li> <li>Code organization (10%)</li> <li>Error handling (10%)</li> </ul>"},{"location":"week-15/cen310-week-15/#documentation-25","title":"Documentation (25%)","text":"<ul> <li>Project report (10%)</li> <li>Code documentation (10%)</li> <li>Presentation quality (5%)</li> </ul>"},{"location":"week-15/cen310-week-15/#results-analysis-25","title":"Results &amp; Analysis (25%)","text":"<ul> <li>Performance results (10%)</li> <li>Comparative analysis (10%)</li> <li>Future improvements (5%)</li> </ul>"},{"location":"week-15/cen310-week-15/#common-project-topics","title":"Common Project Topics","text":"<ol> <li>Scientific Computing</li> <li>N-body simulations</li> <li>Fluid dynamics</li> <li>Monte Carlo methods</li> <li> <p>Matrix computations</p> </li> <li> <p>Data Processing</p> </li> <li>Image/video processing</li> <li>Signal processing</li> <li>Data mining</li> <li> <p>Pattern recognition</p> </li> <li> <p>Machine Learning</p> </li> <li>Neural network training</li> <li>Parallel model inference</li> <li>Data preprocessing</li> <li> <p>Feature extraction</p> </li> <li> <p>Graph Processing</p> </li> <li>Path finding</li> <li>Graph analytics</li> <li>Network analysis</li> <li>Tree algorithms</li> </ol>"},{"location":"week-15/cen310-week-15/#resources-references","title":"Resources &amp; References","text":""},{"location":"week-15/cen310-week-15/#documentation","title":"Documentation","text":"<ul> <li>CUDA Programming Guide</li> <li>OpenMP API Specification</li> <li>MPI Standard Documentation</li> <li>Performance Optimization Guides</li> </ul>"},{"location":"week-15/cen310-week-15/#tools","title":"Tools","text":"<ul> <li>Visual Studio</li> <li>NVIDIA NSight</li> <li>Intel VTune</li> <li>Performance Profilers</li> </ul>"},{"location":"week-15/cen310-week-15/#project-report-template","title":"Project Report Template","text":""},{"location":"week-15/cen310-week-15/#1-introduction","title":"1. Introduction","text":"<ul> <li>Background</li> <li>Objectives</li> <li>Scope</li> </ul>"},{"location":"week-15/cen310-week-15/#2-design","title":"2. Design","text":"<ul> <li>System architecture</li> <li>Component design</li> <li>Parallel strategies</li> </ul>"},{"location":"week-15/cen310-week-15/#3-implementation","title":"3. Implementation","text":"<ul> <li>Development environment</li> <li>Technical details</li> <li>Optimization techniques</li> </ul>"},{"location":"week-15/cen310-week-15/#4-results","title":"4. Results","text":"<ul> <li>Performance measurements</li> <li>Analysis</li> <li>Comparisons</li> </ul>"},{"location":"week-15/cen310-week-15/#5-conclusion","title":"5. Conclusion","text":"<ul> <li>Achievements</li> <li>Challenges</li> <li>Future work</li> </ul>"},{"location":"week-15/cen310-week-15/#contact-information","title":"Contact Information","text":"<p>For project-related queries:</p> <ul> <li>Email: ugur.coruh@erdogan.edu.tr</li> <li>Office Hours: By appointment</li> <li>Location: Engineering Faculty</li> </ul>"},{"location":"week-15/cen310-week-15/#questions-discussion","title":"Questions &amp; Discussion","text":""},{"location":"week-16-final/cen310-week-16/","title":"CEN310 Parallel Programming Week-16","text":""},{"location":"week-16-final/cen310-week-16/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-16-final/cen310-week-16/#week-16-final-examination-period","title":"Week-16 (Final Examination Period)","text":""},{"location":"week-16-final/cen310-week-16/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-16-final/cen310-week-16/#final-examination-period-information","title":"Final Examination Period Information","text":""},{"location":"week-16-final/cen310-week-16/#dates","title":"Dates","text":"<ul> <li>Period: May 24 - June 4, 2025</li> <li>Project Report Due: As scheduled by the university</li> </ul>"},{"location":"week-16-final/cen310-week-16/#location","title":"Location","text":"<ul> <li>As assigned by the university</li> <li>Check the official examination schedule</li> </ul>"},{"location":"week-16-final/cen310-week-16/#final-project-report-requirements","title":"Final Project Report Requirements","text":""},{"location":"week-16-final/cen310-week-16/#1-project-documentation","title":"1. Project Documentation","text":"<ul> <li>Complete project report</li> <li>Source code with documentation</li> <li>Performance analysis results</li> <li>Implementation details</li> <li>Future work proposals</li> </ul>"},{"location":"week-16-final/cen310-week-16/#2-technical-requirements","title":"2. Technical Requirements","text":"<ul> <li>Code quality and organization</li> <li>Performance optimization results</li> <li>Comparison with sequential version</li> <li>Scalability analysis</li> <li>Error handling implementation</li> </ul>"},{"location":"week-16-final/cen310-week-16/#report-structure","title":"Report Structure","text":""},{"location":"week-16-final/cen310-week-16/#1-executive-summary","title":"1. Executive Summary","text":"<ul> <li>Project overview</li> <li>Key achievements</li> <li>Performance highlights</li> </ul>"},{"location":"week-16-final/cen310-week-16/#2-technical-implementation","title":"2. Technical Implementation","text":"<ul> <li>Architecture details</li> <li>Algorithm descriptions</li> <li>Parallelization strategy</li> <li>Code structure</li> </ul>"},{"location":"week-16-final/cen310-week-16/#3-performance-analysis","title":"3. Performance Analysis","text":"<ul> <li>Benchmark results</li> <li>Scalability tests</li> <li>Resource utilization</li> <li>Optimization efforts</li> </ul>"},{"location":"week-16-final/cen310-week-16/#4-conclusions","title":"4. Conclusions","text":"<ul> <li>Lessons learned</li> <li>Challenges overcome</li> <li>Future improvements</li> </ul>"},{"location":"week-16-final/cen310-week-16/#submission-guidelines","title":"Submission Guidelines","text":""},{"location":"week-16-final/cen310-week-16/#format-requirements","title":"Format Requirements","text":"<ul> <li>PDF format</li> <li>Professional formatting</li> <li>Clear code listings</li> <li>Proper citations</li> <li>Performance graphs</li> </ul>"},{"location":"week-16-final/cen310-week-16/#submission-process","title":"Submission Process","text":"<ul> <li>Digital submission</li> <li>Source code repository</li> <li>Documentation package</li> <li>Presentation slides</li> </ul>"},{"location":"week-16-final/cen310-week-16/#evaluation-criteria","title":"Evaluation Criteria","text":""},{"location":"week-16-final/cen310-week-16/#technical-depth-40","title":"Technical Depth (40%)","text":"<ul> <li>Implementation quality</li> <li>Performance optimization</li> <li>Code organization</li> <li>Documentation quality</li> </ul>"},{"location":"week-16-final/cen310-week-16/#analysis-results-40","title":"Analysis &amp; Results (40%)","text":"<ul> <li>Performance measurements</li> <li>Scalability analysis</li> <li>Comparative evaluation</li> <li>Problem-solving approach</li> </ul>"},{"location":"week-16-final/cen310-week-16/#documentation-20","title":"Documentation (20%)","text":"<ul> <li>Report quality</li> <li>Code documentation</li> <li>Presentation materials</li> <li>Future recommendations</li> </ul>"},{"location":"week-16-final/cen310-week-16/#important-notes","title":"Important Notes","text":""},{"location":"week-16-final/cen310-week-16/#deadlines","title":"Deadlines","text":"<ul> <li>Report submission deadline is strict</li> <li>Late submissions may not be accepted</li> <li>Extensions require prior approval</li> </ul>"},{"location":"week-16-final/cen310-week-16/#academic-integrity","title":"Academic Integrity","text":"<ul> <li>Original work required</li> <li>Proper citations needed</li> <li>Code plagiarism checked</li> <li>Collaboration must be acknowledged</li> </ul>"},{"location":"week-16-final/cen310-week-16/#course-completion-requirements","title":"Course Completion Requirements","text":""},{"location":"week-16-final/cen310-week-16/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Project report submission</li> <li>Code repository with documentation</li> <li>Performance analysis results</li> <li>Implementation demonstration</li> </ul>"},{"location":"week-16-final/cen310-week-16/#grading-components","title":"Grading Components","text":"<ul> <li>Quiz-1: 15%</li> <li>Midterm: 35%</li> <li>Quiz-2: 15%</li> <li>Final Project: 35%</li> </ul>"},{"location":"week-16-final/cen310-week-16/#contact-information","title":"Contact Information","text":"<p>For examination related queries:</p> <ul> <li>Email: ugur.coruh@erdogan.edu.tr</li> <li>Office Hours: By appointment</li> <li>Location: Engineering Faculty</li> </ul>"},{"location":"week-16-final/cen310-week-16/#good-luck-with-your-final-examination","title":"Good Luck with Your Final Examination!","text":""},{"location":"week-2/cen429-week-2/","title":"CEN310 Parallel Programming Week-2","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#week-2","title":"Week-2","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#parallel-computing-fundamentals","title":"Parallel Computing Fundamentals","text":"<p>Download </p> <ul> <li>PDF</li> <li>DOC</li> <li>SLIDE</li> <li>PPTX</li> </ul>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#outline-14","title":"Outline (\u00bc)","text":"<ol> <li>Parallel Computing Architectures</li> <li>Flynn's Taxonomy</li> <li>Memory Architectures</li> <li>Interconnection Networks</li> <li>Modern Processor Architectures</li> <li>Cache Coherence</li> <li> <p>Memory Consistency Models</p> </li> <li> <p>Performance Metrics</p> </li> <li>Speedup Types</li> <li>Efficiency Analysis</li> <li>Amdahl's Law</li> <li>Gustafson's Law</li> <li>Scalability Measures</li> <li>Cost Models</li> </ol>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#outline-24","title":"Outline (2/4)","text":"<ol> <li>Parallel Algorithm Design</li> <li>Decomposition Strategies<ul> <li>Data Decomposition</li> <li>Task Decomposition</li> <li>Pipeline Decomposition</li> </ul> </li> <li>Load Balancing<ul> <li>Static vs Dynamic</li> <li>Work Stealing</li> </ul> </li> <li>Communication Patterns<ul> <li>Point-to-Point</li> <li>Collective Operations</li> </ul> </li> <li>Synchronization Methods<ul> <li>Barriers</li> <li>Locks</li> <li>Atomic Operations</li> </ul> </li> </ol>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#outline-34","title":"Outline (\u00be)","text":"<ol> <li>Programming Models</li> <li>Shared Memory<ul> <li>OpenMP Basics</li> <li>Pthreads Overview</li> </ul> </li> <li>Message Passing<ul> <li>MPI Concepts</li> <li>Communication Types</li> </ul> </li> <li>Data Parallel<ul> <li>SIMD Instructions</li> <li>Vector Operations</li> </ul> </li> <li>Hybrid Models<ul> <li>OpenMP + MPI</li> <li>CPU + GPU</li> </ul> </li> </ol>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#outline-44","title":"Outline (4/4)","text":"<ol> <li>Performance Analysis &amp; Optimization</li> <li>Profiling Tools</li> <li>Bottleneck Analysis</li> <li>Memory Access Patterns</li> <li>Cache Optimization</li> <li>Communication Overhead</li> <li> <p>Load Imbalance Detection</p> </li> <li> <p>Real-World Applications</p> </li> <li>Scientific Computing</li> <li>Data Processing</li> <li>Image Processing</li> <li>Financial Modeling</li> </ol>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#1-parallel-computing-architectures","title":"1. Parallel Computing Architectures","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#flynns-taxonomy-14","title":"Flynn's Taxonomy (\u00bc)","text":"<pre><code>         Instruction\n             \u2193\n    +----------------+\n    |   Processing   |\n    |     Unit      |\n    +----------------+\n             \u2193\n           Data\n</code></pre> <ul> <li>Traditional sequential computing (SISD)</li> <li>One instruction stream, one data stream</li> </ul>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#flynns-taxonomy-24","title":"Flynn's Taxonomy (2/4)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#simd-architecture","title":"SIMD Architecture","text":"<pre><code>         Instruction\n             \u2193\n    +----------------+\n    |  Processing    |\n    |    Units      |\n    +----------------+\n      \u2193    \u2193    \u2193    \u2193\n    Data Data Data Data\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#flynns-taxonomy-34","title":"Flynn's Taxonomy (\u00be)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#simd-example-code-part-1","title":"SIMD Example Code (Part 1)","text":"<pre><code>#include &lt;immintrin.h&gt;\n\nvoid vectorAdd(float* a, float* b, float* c, int n) {\n    // Process 8 elements at once using AVX\n    for (int i = 0; i &lt; n; i += 8) {\n        __m256 va = _mm256_load_ps(&amp;a[i]);\n        __m256 vb = _mm256_load_ps(&amp;b[i]);\n        __m256 vc = _mm256_add_ps(va, vb);\n        _mm256_store_ps(&amp;c[i], vc);\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#flynns-taxonomy-44","title":"Flynn's Taxonomy (4/4)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#mimd-architecture-and-example","title":"MIMD Architecture and Example","text":"<pre><code>Inst1   Inst2   Inst3   Inst4\n  \u2193       \u2193       \u2193       \u2193\n+-----+ +-----+ +-----+ +-----+\n| PU1 | | PU2 | | PU3 | | PU4 |\n+-----+ +-----+ +-----+ +-----+\n  \u2193       \u2193       \u2193       \u2193\nData1   Data2   Data3   Data4\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#memory-architectures-15","title":"Memory Architectures (\u2155)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#shared-memory-overview","title":"Shared Memory Overview","text":"<pre><code>+-----+ +-----+ +-----+ +-----+\n| CPU | | CPU | | CPU | | CPU |\n+-----+ +-----+ +-----+ +-----+\n        \u2193       \u2193       \u2193\n    +----------------------+\n    |    Shared Memory    |\n    +----------------------+\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#memory-architectures-25","title":"Memory Architectures (\u2156)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#shared-memory-example","title":"Shared Memory Example","text":"<pre><code>// Basic OpenMP shared memory example\nint shared_array[1000];\n\n#pragma omp parallel for\nfor(int i = 0; i &lt; 1000; i++) {\n    shared_array[i] = heavy_computation(i);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#memory-architectures-35","title":"Memory Architectures (\u2157)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#distributed-memory-overview","title":"Distributed Memory Overview","text":"<pre><code>+--------+   +--------+\n| CPU+Mem|   | CPU+Mem|\n+--------+   +--------+\n    \u2195            \u2195\n    Network Connection\n    \u2195            \u2195\n+--------+   +--------+\n| CPU+Mem|   | CPU+Mem|\n+--------+   +--------+\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#memory-architectures-45","title":"Memory Architectures (\u2158)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#distributed-memory-example-part-1","title":"Distributed Memory Example (Part 1)","text":"<pre><code>// MPI distributed memory example\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\nMPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n// Local computation\nint local_result = compute_local_part(rank, size);\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#memory-architectures-55","title":"Memory Architectures (5/5)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#distributed-memory-example-part-2","title":"Distributed Memory Example (Part 2)","text":"<pre><code>// Gather results from all processes\nint* all_results = NULL;\nif(rank == 0) {\n    all_results = new int[size];\n}\n\nMPI_Gather(&amp;local_result, 1, MPI_INT, \n           all_results, 1, MPI_INT, \n           0, MPI_COMM_WORLD);\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#2-performance-metrics","title":"2. Performance Metrics","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#speedup-analysis-14","title":"Speedup Analysis (\u00bc)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#theoretical-speedup","title":"Theoretical Speedup","text":"\\[ S(n) = \\frac{T_1}{T_n} \\] <p>where: - \\(\\(T_1\\)\\) is sequential execution time - \\(\\(T_n\\)\\) is parallel execution time with n processors</p>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#speedup-analysis-24","title":"Speedup Analysis (2/4)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#measurement-code","title":"Measurement Code","text":"<pre><code>Timer t;\n\n// Sequential version\nt.start();\nsequential_algorithm();\ndouble t1 = t.stop();\n\n// Parallel version\nt.start();\nparallel_algorithm();\ndouble tn = t.stop();\n\ndouble speedup = t1/tn;\nprintf(\"Speedup: %.2f\\n\", speedup);\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#speedup-analysis-34","title":"Speedup Analysis (\u00be)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#amdahls-law","title":"Amdahl's Law","text":"\\[ S(n) = \\frac{1}{(1-p) + \\frac{p}{n}} \\] <p>where: - \\(\\(p\\)\\) is the parallel portion - \\(\\(n\\)\\) is number of processors</p>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#speedup-analysis-44","title":"Speedup Analysis (4/4)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#amdahls-law-implementation","title":"Amdahl's Law Implementation","text":"<pre><code>double amdahl_speedup(double p, int n) {\n    return 1.0 / ((1-p) + p/n);\n}\n\n// Calculate theoretical speedups\nfor(int n = 1; n &lt;= 16; n *= 2) {\n    printf(\"Processors: %d, Max Speedup: %.2f\\n\",\n           n, amdahl_speedup(0.95, n));\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#3-parallel-algorithm-design","title":"3. Parallel Algorithm Design","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#decomposition-strategies-14","title":"Decomposition Strategies (\u00bc)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#data-decomposition","title":"Data Decomposition","text":"<pre><code>// Matrix multiplication with data decomposition\nvoid parallel_matrix_multiply(const Matrix&amp; A, \n                            const Matrix&amp; B,\n                            Matrix&amp; C) {\n    #pragma omp parallel for collapse(2)\n    for(int i = 0; i &lt; A.rows; i++) {\n        for(int j = 0; j &lt; B.cols; j++) {\n            double sum = 0;\n            for(int k = 0; k &lt; A.cols; k++) {\n                sum += A(i,k) * B(k,j);\n            }\n            C(i,j) = sum;\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#decomposition-strategies-24","title":"Decomposition Strategies (2/4)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#task-decomposition","title":"Task Decomposition","text":"<pre><code>// Pipeline processing example\nclass Pipeline {\n    std::queue&lt;Task&gt; stage1_queue, stage2_queue;\n\npublic:\n    void run() {\n        #pragma omp parallel sections\n        {\n            #pragma omp section\n            stage1_worker();\n\n            #pragma omp section\n            stage2_worker();\n\n            #pragma omp section\n            stage3_worker();\n        }\n    }\n\nprivate:\n    void stage1_worker() {\n        while(has_input()) {\n            Task t = read_input();\n            stage1_queue.push(t);\n        }\n    }\n\n    // Similar implementations for other stages...\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#decomposition-strategies-34","title":"Decomposition Strategies (\u00be)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#load-balancing","title":"Load Balancing","text":"<pre><code>// Dynamic load balancing example\nvoid dynamic_load_balance(std::vector&lt;Task&gt;&amp; tasks) {\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(dynamic)\n        for(size_t i = 0; i &lt; tasks.size(); i++) {\n            process_task(tasks[i]);\n        }\n    }\n}\n\n// Work stealing implementation\nclass WorkStealingQueue {\n    std::deque&lt;Task&gt; tasks;\n    std::mutex mtx;\n\npublic:\n    void push(Task t) {\n        std::lock_guard&lt;std::mutex&gt; lock(mtx);\n        tasks.push_back(std::move(t));\n    }\n\n    bool steal(Task&amp; t) {\n        std::lock_guard&lt;std::mutex&gt; lock(mtx);\n        if(tasks.empty()) return false;\n        t = std::move(tasks.front());\n        tasks.pop_front();\n        return true;\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#decomposition-strategies-44","title":"Decomposition Strategies (4/4)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#communication-patterns","title":"Communication Patterns","text":"<pre><code>// Collective communication example (MPI)\nvoid parallel_sum(std::vector&lt;int&gt;&amp; local_data) {\n    int local_sum = std::accumulate(local_data.begin(), \n                                  local_data.end(), 0);\n    int global_sum;\n\n    MPI_Allreduce(&amp;local_sum, &amp;global_sum, 1, MPI_INT,\n                  MPI_SUM, MPI_COMM_WORLD);\n\n    printf(\"Local sum: %d, Global sum: %d\\n\",\n           local_sum, global_sum);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#4-programming-models","title":"4. Programming Models","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#shared-memory-programming-13","title":"Shared Memory Programming (\u2153)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#openmp-basics","title":"OpenMP Basics","text":"<pre><code>// Basic parallel regions\n#pragma omp parallel\n{\n    int tid = omp_get_thread_num();\n    printf(\"Hello from thread %d\\n\", tid);\n}\n\n// Work sharing\n#pragma omp parallel for\nfor(int i = 0; i &lt; N; i++) {\n    heavy_computation(i);\n}\n\n// Synchronization\n#pragma omp parallel\n{\n    #pragma omp critical\n    {\n        // Critical section\n    }\n\n    #pragma omp barrier\n    // All threads synchronize here\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#shared-memory-programming-23","title":"Shared Memory Programming (\u2154)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#advanced-openmp-features","title":"Advanced OpenMP Features","text":"<pre><code>// Task parallelism\n#pragma omp parallel\n{\n    #pragma omp single\n    {\n        #pragma omp task\n        long_running_task1();\n\n        #pragma omp task\n        long_running_task2();\n    }\n}\n\n// Nested parallelism\nvoid nested_parallel() {\n    #pragma omp parallel num_threads(2)\n    {\n        #pragma omp parallel num_threads(2)\n        {\n            int tid = omp_get_thread_num();\n            int team = omp_get_team_num();\n            printf(\"Team %d, Thread %d\\n\", team, tid);\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#shared-memory-programming-33","title":"Shared Memory Programming (3/3)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#performance-considerations","title":"Performance Considerations","text":"<pre><code>// False sharing example\nstruct PaddedCounter {\n    int value;\n    char padding[60]; // Prevent false sharing\n};\n\nvoid increment_counters() {\n    PaddedCounter counters[NUM_THREADS];\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        for(int i = 0; i &lt; 1000000; i++) {\n            counters[tid].value++;\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#5-performance-analysis-optimization","title":"5. Performance Analysis &amp; Optimization","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#profiling-tools-13","title":"Profiling Tools (\u2153)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#using-intel-vtune","title":"Using Intel VTune","text":"<pre><code>// Code instrumentation example\n#include &lt;ittnotify.h&gt;\n\nvoid analyze_performance() {\n    __itt_domain* domain = __itt_domain_create(\"MyDomain\");\n    __itt_string_handle* task = __itt_string_handle_create(\"MyTask\");\n\n    __itt_task_begin(domain, __itt_null, __itt_null, task);\n    heavy_computation();\n    __itt_task_end(domain);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#profiling-tools-23","title":"Profiling Tools (\u2154)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#custom-performance-metrics","title":"Custom Performance Metrics","text":"<pre><code>class PerformanceMetrics {\n    std::chrono::high_resolution_clock::time_point start;\n    std::string name;\n\npublic:\n    PerformanceMetrics(const std::string&amp; n) \n        : name(n), start(std::chrono::high_resolution_clock::now()) {}\n\n    ~PerformanceMetrics() {\n        auto end = std::chrono::high_resolution_clock::now();\n        auto duration = std::chrono::duration_cast&lt;std::chrono::microseconds&gt;\n                       (end - start).count();\n        printf(\"%s took %ld microseconds\\n\", name.c_str(), duration);\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#profiling-tools-33","title":"Profiling Tools (3/3)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#memory-access-analysis","title":"Memory Access Analysis","text":"<pre><code>// Cache-friendly vs cache-unfriendly access\nvoid analyze_memory_access() {\n    const int SIZE = 1024 * 1024;\n    int* arr = new int[SIZE];\n\n    // Sequential access\n    PerformanceMetrics m1(\"Sequential\");\n    for(int i = 0; i &lt; SIZE; i++) {\n        arr[i] = i;\n    }\n\n    // Random access\n    PerformanceMetrics m2(\"Random\");\n    for(int i = 0; i &lt; SIZE; i++) {\n        arr[(i * 16) % SIZE] = i;\n    }\n\n    delete[] arr;\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#6-real-world-applications","title":"6. Real-World Applications","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#scientific-computing-13","title":"Scientific Computing (\u2153)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#n-body-simulation","title":"N-Body Simulation","text":"<pre><code>struct Particle {\n    double x, y, z;\n    double vx, vy, vz;\n    double mass;\n};\n\nvoid simulate_n_body(std::vector&lt;Particle&gt;&amp; particles) {\n    #pragma omp parallel for\n    for(size_t i = 0; i &lt; particles.size(); i++) {\n        for(size_t j = 0; j &lt; particles.size(); j++) {\n            if(i != j) {\n                update_velocity(particles[i], particles[j]);\n            }\n        }\n    }\n\n    #pragma omp parallel for\n    for(auto&amp; p : particles) {\n        update_position(p);\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#scientific-computing-23","title":"Scientific Computing (\u2154)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#matrix-operations","title":"Matrix Operations","text":"<pre><code>// Parallel matrix multiplication with blocking\nvoid block_matrix_multiply(const Matrix&amp; A,\n                         const Matrix&amp; B,\n                         Matrix&amp; C,\n                         int block_size) {\n    #pragma omp parallel for collapse(2)\n    for(int i = 0; i &lt; A.rows; i += block_size) {\n        for(int j = 0; j &lt; B.cols; j += block_size) {\n            for(int k = 0; k &lt; A.cols; k += block_size) {\n                multiply_block(A, B, C, i, j, k, block_size);\n            }\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#scientific-computing-33","title":"Scientific Computing (3/3)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#performance-analysis","title":"Performance Analysis","text":"<pre><code>void analyze_block_sizes() {\n    Matrix A(1024, 1024), B(1024, 1024), C(1024, 1024);\n\n    std::vector&lt;int&gt; block_sizes = {8, 16, 32, 64, 128};\n    for(int block_size : block_sizes) {\n        Timer t;\n        block_matrix_multiply(A, B, C, block_size);\n        printf(\"Block size %d: %.2f seconds\\n\", \n               block_size, t.elapsed());\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#7-parallel-programming-paradigms","title":"7. Parallel Programming Paradigms","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#spmd-pattern-13","title":"SPMD Pattern (\u2153)","text":"<pre><code>// Single Program Multiple Data Example\nvoid spmd_example() {\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n    // Same program, different data portions\n    std::vector&lt;double&gt; local_data = get_local_data(rank, size);\n    double local_sum = std::accumulate(local_data.begin(), \n                                     local_data.end(), 0.0);\n\n    // Combine results\n    double global_sum;\n    MPI_Reduce(&amp;local_sum, &amp;global_sum, 1, MPI_DOUBLE, \n               MPI_SUM, 0, MPI_COMM_WORLD);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#masterworker-pattern-23","title":"Master/Worker Pattern (\u2154)","text":"<pre><code>// Master-Worker Implementation\nclass TaskPool {\n    std::queue&lt;Task&gt; tasks;\n    std::mutex mtx;\n    std::condition_variable cv;\n    bool done = false;\n\npublic:\n    void master_function() {\n        while(has_more_tasks()) {\n            Task t = generate_task();\n            {\n                std::lock_guard&lt;std::mutex&gt; lock(mtx);\n                tasks.push(t);\n            }\n            cv.notify_one();\n        }\n        done = true;\n        cv.notify_all();\n    }\n\n    void worker_function(int id) {\n        while(true) {\n            Task t;\n            {\n                std::unique_lock&lt;std::mutex&gt; lock(mtx);\n                cv.wait(lock, [this]() { \n                    return !tasks.empty() || done; \n                });\n                if(tasks.empty() &amp;&amp; done) break;\n                t = tasks.front();\n                tasks.pop();\n            }\n            process_task(t, id);\n        }\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#pipeline-pattern-33","title":"Pipeline Pattern (3/3)","text":"<pre><code>// Pipeline Pattern with OpenMP\ntemplate&lt;typename T&gt;\nclass Pipeline {\n    std::queue&lt;T&gt; queue1, queue2;\n    std::mutex mtx1, mtx2;\n    std::condition_variable cv1, cv2;\n    bool done = false;\n\npublic:\n    void run_pipeline() {\n        #pragma omp parallel sections\n        {\n            #pragma omp section\n            stage1_producer();\n\n            #pragma omp section\n            stage2_processor();\n\n            #pragma omp section\n            stage3_consumer();\n        }\n    }\n\nprivate:\n    void stage1_producer() {\n        while(has_input()) {\n            T data = read_input();\n            {\n                std::lock_guard&lt;std::mutex&gt; lock(mtx1);\n                queue1.push(data);\n            }\n            cv1.notify_one();\n        }\n    }\n\n    // Similar implementations for other stages...\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#8-modern-cpu-architectures","title":"8. Modern CPU Architectures","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#numa-architecture-13","title":"NUMA Architecture (\u2153)","text":"<pre><code>   CPU0    CPU1       CPU2    CPU3\n    \u2193       \u2193          \u2193       \u2193\n  Memory0  Memory1   Memory2  Memory3\n    \u2193       \u2193          \u2193       \u2193\n    \u2190--- Interconnect Network ---&gt;\n</code></pre> <pre><code>// NUMA-aware allocation\nvoid numa_allocation() {\n    #pragma omp parallel\n    {\n        int numa_node = get_numa_node();\n        void* local_mem = numa_alloc_onnode(size, numa_node);\n\n        // Process data locally\n        process_local_data(local_mem);\n\n        numa_free(local_mem, size);\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#cache-hierarchy-23","title":"Cache Hierarchy (\u2154)","text":"<pre><code>Core 0          Core 1\n  \u2193               \u2193\nL1 Cache       L1 Cache\n  \u2193               \u2193\nL2 Cache       L2 Cache\n  \u2193               \u2193\n   Shared L3 Cache\n        \u2193\n   Main Memory\n</code></pre> <pre><code>// Cache-conscious programming\nvoid cache_optimization() {\n    const int CACHE_LINE = 64;\n    struct alignas(CACHE_LINE) CacheAligned {\n        double data[8]; // 64 bytes\n    };\n\n    std::vector&lt;CacheAligned&gt; array(1000);\n\n    #pragma omp parallel for\n    for(int i = 0; i &lt; array.size(); i++) {\n        process_aligned_data(array[i]);\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#instruction-level-parallelism-33","title":"Instruction Level Parallelism (3/3)","text":"<pre><code>// Loop unrolling example\nvoid optimize_loop() {\n    const int N = 1000000;\n    float a[N], b[N], c[N];\n\n    // Original loop\n    for(int i = 0; i &lt; N; i++) {\n        c[i] = a[i] + b[i];\n    }\n\n    // Unrolled loop\n    for(int i = 0; i &lt; N; i += 4) {\n        c[i] = a[i] + b[i];\n        c[i+1] = a[i+1] + b[i+1];\n        c[i+2] = a[i+2] + b[i+2];\n        c[i+3] = a[i+3] + b[i+3];\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#9-advanced-algorithm-analysis","title":"9. Advanced Algorithm Analysis","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#work-time-analysis-13","title":"Work-Time Analysis (\u2153)","text":"<pre><code>T_\u221e = \\text{Critical Path Length}\nT_1 = \\text{Total Work}\nP = \\text{Number of Processors}\nT_P \u2265 \\max(T_\u221e, T_1/P)\n</code></pre> <p>Example Analysis: </p><pre><code>// Parallel reduction analysis\nvoid analyze_reduction() {\n    const int N = 1024;\n    int depth = log2(N);  // T_\u221e\n    int total_ops = N-1;  // T_1\n\n    printf(\"Critical path length: %d\\n\", depth);\n    printf(\"Total work: %d\\n\", total_ops);\n    printf(\"Theoretical min time with P processors: %f\\n\",\n           max(depth, total_ops/P));\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#isoefficiency-analysis-23","title":"Isoefficiency Analysis (\u2154)","text":"<pre><code>// Isoefficiency measurement\nvoid measure_isoefficiency() {\n    std::vector&lt;int&gt; problem_sizes = {1000, 2000, 4000, 8000};\n    std::vector&lt;int&gt; processor_counts = {1, 2, 4, 8, 16};\n\n    for(int N : problem_sizes) {\n        for(int P : processor_counts) {\n            Timer t;\n            parallel_algorithm(N, P);\n            double Tp = t.elapsed();\n\n            double efficiency = T1/(P * Tp);\n            printf(\"N=%d, P=%d, Efficiency=%.2f\\n\", \n                   N, P, efficiency);\n        }\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#critical-path-analysis-33","title":"Critical Path Analysis (3/3)","text":"<pre><code>// Task graph analysis\nclass TaskGraph {\n    struct Task {\n        int id;\n        std::vector&lt;int&gt; dependencies;\n        int cost;\n    };\n\n    std::vector&lt;Task&gt; tasks;\n\npublic:\n    int calculate_critical_path() {\n        std::vector&lt;int&gt; earliest_start(tasks.size(), 0);\n\n        for(const Task&amp; t : tasks) {\n            int max_dep_time = 0;\n            for(int dep : t.dependencies) {\n                max_dep_time = std::max(max_dep_time,\n                    earliest_start[dep] + tasks[dep].cost);\n            }\n            earliest_start[t.id] = max_dep_time;\n        }\n\n        return *std::max_element(earliest_start.begin(),\n                               earliest_start.end());\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#10-performance-modeling","title":"10. Performance Modeling","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#roofline-model-13","title":"Roofline Model (\u2153)","text":"<pre><code>// Roofline model analysis\nstruct RooflineParams {\n    double peak_performance;    // FLOPS\n    double memory_bandwidth;    // bytes/s\n    double operational_intensity; // FLOPS/byte\n};\n\ndouble predict_performance(const RooflineParams&amp; params) {\n    return std::min(params.peak_performance,\n                   params.memory_bandwidth * \n                   params.operational_intensity);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#logp-model-23","title":"LogP Model (\u2154)","text":"<pre><code>// LogP model parameters\nstruct LogPParams {\n    double L;  // Latency\n    double o;  // Overhead\n    double g;  // Gap\n    int P;     // Processors\n};\n\ndouble estimate_communication_time(const LogPParams&amp; params,\n                                int message_size) {\n    return params.L + 2 * params.o + \n           (message_size - 1) * params.g;\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#bsp-model-33","title":"BSP Model (3/3)","text":"<pre><code>// Bulk Synchronous Parallel model\nclass BSPComputation {\n    struct SuperStep {\n        double computation_time;\n        double communication_volume;\n        int synchronization_cost;\n    };\n\n    std::vector&lt;SuperStep&gt; steps;\n\npublic:\n    double estimate_total_time(int P, double g, double L) {\n        double total = 0;\n        for(const auto&amp; step : steps) {\n            total += step.computation_time +\n                    g * step.communication_volume +\n                    L;  // barrier synchronization\n        }\n        return total;\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#11-advanced-optimization-techniques","title":"11. Advanced Optimization Techniques","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#vectorization-14","title":"Vectorization (\u00bc)","text":"<pre><code>// Auto-vectorization example\nvoid vectorized_operation(float* a, float* b, \n                         float* c, int n) {\n    // Hint for vectorization\n    #pragma omp simd\n    for(int i = 0; i &lt; n; i++) {\n        c[i] = std::sqrt(a[i] * a[i] + b[i] * b[i]);\n    }\n}\n\n// Explicit vectorization\nvoid explicit_vector() {\n    __m256 a = _mm256_set1_ps(1.0f);\n    __m256 b = _mm256_set1_ps(2.0f);\n    __m256 c = _mm256_add_ps(a, b);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#loop-transformations-24","title":"Loop Transformations (2/4)","text":"<pre><code>// Loop interchange\nvoid matrix_transform() {\n    int A[1000][1000];\n\n    // Original (cache-unfriendly)\n    for(int j = 0; j &lt; 1000; j++)\n        for(int i = 0; i &lt; 1000; i++)\n            A[i][j] = compute(i,j);\n\n    // Transformed (cache-friendly)\n    for(int i = 0; i &lt; 1000; i++)\n        for(int j = 0; j &lt; 1000; j++)\n            A[i][j] = compute(i,j);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#memory-coalescing-34","title":"Memory Coalescing (\u00be)","text":"<pre><code>// Memory coalescing for GPU\nstruct SOA {\n    float* x;\n    float* y;\n    float* z;\n};\n\nstruct AOS {\n    struct Point {\n        float x, y, z;\n    };\n    Point* points;\n};\n\nvoid coalesced_access() {\n    SOA soa;\n    // Coalesced access\n    #pragma omp target teams distribute parallel for\n    for(int i = 0; i &lt; N; i++) {\n        soa.x[i] = compute_x(i);\n        soa.y[i] = compute_y(i);\n        soa.z[i] = compute_z(i);\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#prefetching-44","title":"Prefetching (4/4)","text":"<pre><code>// Software prefetching\nvoid prefetch_example(int* data, int size) {\n    const int PREFETCH_DISTANCE = 16;\n\n    for(int i = 0; i &lt; size; i++) {\n        // Prefetch future data\n        __builtin_prefetch(&amp;data[i + PREFETCH_DISTANCE]);\n        process(data[i]);\n    }\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#extended-homework-assignment","title":"Extended Homework Assignment","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#project-1-advanced-matrix-operations","title":"Project 1: Advanced Matrix Operations","text":"<ol> <li>Implement parallel matrix operations with:</li> <li>Cache blocking</li> <li>SIMD optimization</li> <li> <p>NUMA awareness</p> </li> <li> <p>Performance Analysis:</p> </li> <li>Roofline model analysis</li> <li>Cache miss rates</li> <li> <p>Memory bandwidth utilization</p> </li> <li> <p>Documentation:</p> </li> <li>Implementation details</li> <li>Performance analysis</li> <li>Optimization strategies</li> <li>Scalability analysis</li> </ol>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#next-week-preview","title":"Next Week Preview","text":"<p>We will cover: - OpenMP in detail - Parallel regions and constructs - Data sharing and synchronization - Advanced OpenMP features</p>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#12-parallel-design-patterns","title":"12. Parallel Design Patterns","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#map-reduce-pattern-13","title":"Map-Reduce Pattern (\u2153)","text":"<pre><code>// Map-Reduce implementation\ntemplate&lt;typename T, typename MapFn, typename ReduceFn&gt;\nT parallel_map_reduce(const std::vector&lt;T&gt;&amp; data,\n                     MapFn map_fn,\n                     ReduceFn reduce_fn,\n                     T initial_value) {\n    std::vector&lt;T&gt; mapped_data(data.size());\n\n    // Map phase\n    #pragma omp parallel for\n    for(size_t i = 0; i &lt; data.size(); i++) {\n        mapped_data[i] = map_fn(data[i]);\n    }\n\n    // Reduce phase\n    T result = initial_value;\n    #pragma omp parallel for reduction(reduce_fn:result)\n    for(size_t i = 0; i &lt; mapped_data.size(); i++) {\n        result = reduce_fn(result, mapped_data[i]);\n    }\n\n    return result;\n}\n\n// Usage example\nvoid map_reduce_example() {\n    std::vector&lt;int&gt; data = {1, 2, 3, 4, 5, 6, 7, 8};\n\n    auto square = [](int x) { return x * x; };\n    auto sum = [](int a, int b) { return a + b; };\n\n    int result = parallel_map_reduce(data, square, sum, 0);\n    printf(\"Sum of squares: %d\\n\", result);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#fork-join-pattern-23","title":"Fork-Join Pattern (\u2154)","text":"<pre><code>// Fork-Join pattern with recursive task decomposition\ntemplate&lt;typename T&gt;\nT parallel_divide_conquer(T* data, int start, int end,\n                         int threshold) {\n    int length = end - start;\n    if(length &lt;= threshold) {\n        return sequential_solve(data, start, end);\n    }\n\n    T left_result, right_result;\n    int mid = start + length/2;\n\n    #pragma omp task shared(left_result)\n    left_result = parallel_divide_conquer(data, start, mid, \n                                        threshold);\n\n    #pragma omp task shared(right_result)\n    right_result = parallel_divide_conquer(data, mid, end, \n                                         threshold);\n\n    #pragma omp taskwait\n    return combine_results(left_result, right_result);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#wavefront-pattern-33","title":"Wavefront Pattern (3/3)","text":"<pre><code>// Wavefront pattern implementation\nvoid wavefront_computation(Matrix&amp; matrix, int N) {\n    for(int wave = 0; wave &lt; 2*N-1; wave++) {\n        #pragma omp parallel for\n        for(int i = max(0, wave-N+1); \n            i &lt;= min(wave, N-1); i++) {\n            int j = wave - i;\n            if(j &lt; N) {\n                compute_cell(matrix, i, j);\n            }\n        }\n    }\n}\n\n// Example usage for matrix computation\nvoid compute_cell(Matrix&amp; matrix, int i, int j) {\n    matrix(i,j) = matrix(i-1,j) + \n                  matrix(i,j-1) + \n                  matrix(i-1,j-1);\n}\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#13-parallel-data-structures","title":"13. Parallel Data Structures","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#lock-free-queue-14","title":"Lock-free Queue (\u00bc)","text":"<pre><code>template&lt;typename T&gt;\nclass LockFreeQueue {\n    struct Node {\n        T data;\n        std::atomic&lt;Node*&gt; next;\n    };\n\n    std::atomic&lt;Node*&gt; head;\n    std::atomic&lt;Node*&gt; tail;\n\npublic:\n    void enqueue(T value) {\n        Node* new_node = new Node{value, nullptr};\n\n        while(true) {\n            Node* last = tail.load();\n            Node* next = last-&gt;next.load();\n\n            if(last == tail.load()) {\n                if(next == nullptr) {\n                    if(last-&gt;next.compare_exchange_weak(next, new_node)) {\n                        tail.compare_exchange_weak(last, new_node);\n                        return;\n                    }\n                } else {\n                    tail.compare_exchange_weak(last, next);\n                }\n            }\n        }\n    }\n\n    bool dequeue(T&amp; result) {\n        while(true) {\n            Node* first = head.load();\n            Node* last = tail.load();\n            Node* next = first-&gt;next.load();\n\n            if(first == head.load()) {\n                if(first == last) {\n                    if(next == nullptr) {\n                        return false;\n                    }\n                    tail.compare_exchange_weak(last, next);\n                } else {\n                    result = next-&gt;data;\n                    if(head.compare_exchange_weak(first, next)) {\n                        delete first;\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#concurrent-hash-map-24","title":"Concurrent Hash Map (2/4)","text":"<pre><code>template&lt;typename K, typename V&gt;\nclass ConcurrentHashMap {\n    struct Bucket {\n        std::mutex mtx;\n        std::unordered_map&lt;K,V&gt; data;\n    };\n\n    std::vector&lt;Bucket&gt; buckets;\n    size_t num_buckets;\n\npublic:\n    ConcurrentHashMap(size_t n) : num_buckets(n) {\n        buckets.resize(n);\n    }\n\n    void insert(const K&amp; key, const V&amp; value) {\n        size_t bucket_idx = std::hash&lt;K&gt;{}(key) % num_buckets;\n        std::lock_guard&lt;std::mutex&gt; lock(buckets[bucket_idx].mtx);\n        buckets[bucket_idx].data[key] = value;\n    }\n\n    bool find(const K&amp; key, V&amp; value) {\n        size_t bucket_idx = std::hash&lt;K&gt;{}(key) % num_buckets;\n        std::lock_guard&lt;std::mutex&gt; lock(buckets[bucket_idx].mtx);\n        auto it = buckets[bucket_idx].data.find(key);\n        if(it != buckets[bucket_idx].data.end()) {\n            value = it-&gt;second;\n            return true;\n        }\n        return false;\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#thread-safe-vector-34","title":"Thread-safe Vector (\u00be)","text":"<pre><code>template&lt;typename T&gt;\nclass ThreadSafeVector {\n    std::vector&lt;T&gt; data;\n    mutable std::shared_mutex mutex;\n\npublic:\n    void push_back(const T&amp; value) {\n        std::unique_lock lock(mutex);\n        data.push_back(value);\n    }\n\n    T at(size_t index) const {\n        std::shared_lock lock(mutex);\n        return data.at(index);\n    }\n\n    void update(size_t index, const T&amp; value) {\n        std::unique_lock lock(mutex);\n        data[index] = value;\n    }\n\n    size_t size() const {\n        std::shared_lock lock(mutex);\n        return data.size();\n    }\n\n    // Atomic operation example\n    void atomic_update(size_t index, \n                      std::function&lt;void(T&amp;)&gt; update_fn) {\n        std::unique_lock lock(mutex);\n        update_fn(data[index]);\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#lock-free-stack-44","title":"Lock-free Stack (4/4)","text":"<pre><code>template&lt;typename T&gt;\nclass LockFreeStack {\n    struct Node {\n        T data;\n        std::atomic&lt;Node*&gt; next;\n    };\n\n    std::atomic&lt;Node*&gt; head;\n\npublic:\n    void push(T value) {\n        Node* new_node = new Node{value};\n\n        do {\n            new_node-&gt;next = head.load();\n        } while(!head.compare_exchange_weak(new_node-&gt;next, \n                                          new_node));\n    }\n\n    bool pop(T&amp; result) {\n        Node* old_head;\n\n        do {\n            old_head = head.load();\n            if(old_head == nullptr) {\n                return false;\n            }\n        } while(!head.compare_exchange_weak(old_head, \n                                          old_head-&gt;next));\n\n        result = old_head-&gt;data;\n        delete old_head;\n        return true;\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#14-parallel-debugging-techniques","title":"14. Parallel Debugging Techniques","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#race-condition-detection-13","title":"Race Condition Detection (\u2153)","text":"<pre><code>// Thread Sanitizer usage example\nvoid race_condition_example() {\n    int shared_var = 0;\n\n    #pragma omp parallel for\n    for(int i = 0; i &lt; 100; i++) {\n        // Race condition here\n        shared_var++;\n    }\n\n    // Fixed version\n    #pragma omp parallel for reduction(+:shared_var)\n    for(int i = 0; i &lt; 100; i++) {\n        shared_var++;\n    }\n}\n\n// Custom race detector\nclass RaceDetector {\n    std::atomic&lt;int&gt; access_count{0};\n    std::atomic&lt;std::thread::id&gt; last_writer;\n\npublic:\n    void on_read(void* addr) {\n        access_count++;\n        // Log read access\n    }\n\n    void on_write(void* addr) {\n        access_count++;\n        last_writer = std::this_thread::get_id();\n        // Log write access\n    }\n\n    void check_race() {\n        if(access_count &gt; 1) {\n            printf(\"Potential race detected!\\n\");\n        }\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#deadlock-detection-23","title":"Deadlock Detection (\u2154)","text":"<pre><code>// Deadlock detection implementation\nclass DeadlockDetector {\n    struct LockInfo {\n        std::thread::id thread_id;\n        void* lock_addr;\n        std::chrono::system_clock::time_point acquire_time;\n    };\n\n    std::map&lt;void*, std::vector&lt;LockInfo&gt;&gt; lock_graph;\n    std::mutex graph_mutex;\n\npublic:\n    void on_lock_attempt(void* lock_addr) {\n        std::lock_guard&lt;std::mutex&gt; guard(graph_mutex);\n\n        auto&amp; info = lock_graph[lock_addr];\n        info.push_back({\n            std::this_thread::get_id(),\n            lock_addr,\n            std::chrono::system_clock::now()\n        });\n\n        detect_cycle();\n    }\n\n    void on_lock_acquire(void* lock_addr) {\n        // Update lock status\n    }\n\n    void on_lock_release(void* lock_addr) {\n        std::lock_guard&lt;std::mutex&gt; guard(graph_mutex);\n        lock_graph[lock_addr].clear();\n    }\n\nprivate:\n    bool detect_cycle() {\n        // Implement cycle detection in lock graph\n        return false;\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#memory-leak-detection-33","title":"Memory Leak Detection (3/3)","text":"<pre><code>// Memory leak detector\nclass MemoryLeakDetector {\n    struct Allocation {\n        void* ptr;\n        size_t size;\n        std::string file;\n        int line;\n        std::thread::id thread_id;\n    };\n\n    std::map&lt;void*, Allocation&gt; allocations;\n    std::mutex mtx;\n\npublic:\n    void on_allocation(void* ptr, size_t size, \n                      const char* file, int line) {\n        std::lock_guard&lt;std::mutex&gt; guard(mtx);\n        allocations[ptr] = {\n            ptr,\n            size,\n            file,\n            line,\n            std::this_thread::get_id()\n        };\n    }\n\n    void on_deallocation(void* ptr) {\n        std::lock_guard&lt;std::mutex&gt; guard(mtx);\n        allocations.erase(ptr);\n    }\n\n    void report_leaks() {\n        std::lock_guard&lt;std::mutex&gt; guard(mtx);\n        for(const auto&amp; [ptr, alloc] : allocations) {\n            printf(\"Leak: %p, size: %zu, file: %s, line: %d\\n\",\n                   ptr, alloc.size, alloc.file.c_str(), \n                   alloc.line);\n        }\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#15-energy-efficiency-in-parallel-computing","title":"15. Energy Efficiency in Parallel Computing","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#power-aware-computing-12","title":"Power-Aware Computing (\u00bd)","text":"<pre><code>// Power monitoring and management\nclass PowerMonitor {\n    struct CoreStats {\n        int frequency;\n        double temperature;\n        double power_consumption;\n    };\n\n    std::vector&lt;CoreStats&gt; core_stats;\n\npublic:\n    void monitor_power_consumption() {\n        #pragma omp parallel\n        {\n            int tid = omp_get_thread_num();\n\n            while(true) {\n                update_core_stats(tid);\n\n                if(core_stats[tid].temperature &gt; THRESHOLD) {\n                    reduce_frequency(tid);\n                }\n\n                std::this_thread::sleep_for(\n                    std::chrono::milliseconds(100));\n            }\n        }\n    }\n\nprivate:\n    void update_core_stats(int core_id) {\n        // Read hardware counters\n        // Update statistics\n    }\n\n    void reduce_frequency(int core_id) {\n        // Implement frequency scaling\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#energy-efficiency-metrics-22","title":"Energy Efficiency Metrics (2/2)","text":"<pre><code>// Energy efficiency calculation\nstruct EnergyMetrics {\n    double energy_consumption;  // Joules\n    double execution_time;      // Seconds\n    double performance;         // FLOPS\n\n    double calculate_efficiency() {\n        return performance / energy_consumption;\n    }\n};\n\nclass EnergyProfiler {\n    std::vector&lt;EnergyMetrics&gt; measurements;\n\npublic:\n    EnergyMetrics profile_algorithm(\n        std::function&lt;void()&gt; algorithm) {\n\n        auto start_energy = measure_energy();\n        auto start_time = std::chrono::high_resolution_clock::now();\n\n        algorithm();\n\n        auto end_time = std::chrono::high_resolution_clock::now();\n        auto end_energy = measure_energy();\n\n        return {\n            end_energy - start_energy,\n            std::chrono::duration&lt;double&gt;(\n                end_time - start_time).count(),\n            measure_performance()\n        };\n    }\n\nprivate:\n    double measure_energy() {\n        // Read energy counters\n        return 0.0;\n    }\n\n    double measure_performance() {\n        // Calculate FLOPS\n        return 0.0;\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#extended-homework-assignment-part-2","title":"Extended Homework Assignment (Part 2)","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#project-2-energy-efficient-parallel-computing","title":"Project 2: Energy-Efficient Parallel Computing","text":"<ol> <li>Implement parallel algorithms with energy monitoring:</li> <li>Matrix multiplication</li> <li>Sorting algorithms</li> <li> <p>Graph algorithms</p> </li> <li> <p>Energy Analysis:</p> </li> <li>Power consumption measurement</li> <li>Performance per watt analysis</li> <li> <p>Temperature monitoring</p> </li> <li> <p>Optimization Strategies:</p> </li> <li>Frequency scaling</li> <li>Load balancing</li> <li> <p>Task scheduling</p> </li> <li> <p>Documentation:</p> </li> <li>Energy efficiency analysis</li> <li>Optimization techniques</li> <li>Trade-off discussion</li> <li>Recommendations</li> </ol>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#next-week-preview_1","title":"Next Week Preview","text":"<p>We will cover: - OpenMP Advanced Features - Task Parallelism - Nested Parallelism - SIMD Operations</p>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#16-advanced-data-structures-and-algorithms","title":"16. Advanced Data Structures and Algorithms","text":"","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#parallel-search-trees-14","title":"Parallel Search Trees (\u00bc)","text":"<pre><code>template&lt;typename T&gt;\nclass ParallelBST {\n    struct Node {\n        T data;\n        std::atomic&lt;Node*&gt; left, right;\n        std::atomic&lt;bool&gt; marked;  // For deletion\n    };\n\n    std::atomic&lt;Node*&gt; root;\n\npublic:\n    bool insert(const T&amp; value) {\n        Node* new_node = new Node{value, nullptr, nullptr, false};\n\n        while(true) {\n            Node* current = root.load();\n            if(!current) {\n                if(root.compare_exchange_strong(current, new_node)) {\n                    return true;\n                }\n                continue;\n            }\n        }\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#parallel-search-trees-24","title":"Parallel Search Trees (2/4)","text":"<pre><code>template&lt;typename T&gt;\nclass ParallelBST {\n    struct Node {\n        T data;\n        std::atomic&lt;Node*&gt; left, right;\n        std::atomic&lt;bool&gt; marked;  // For deletion\n    };\n\n    std::atomic&lt;Node*&gt; root;\n\npublic:\n    bool insert(const T&amp; value) {\n        Node* new_node = new Node{value, nullptr, nullptr, false};\n\n        while(true) {\n            Node* current = root.load();\n            if(!current) {\n                if(root.compare_exchange_strong(current, new_node)) {\n                    return true;\n                }\n                continue;\n            }\n        }\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#parallel-search-trees-34","title":"Parallel Search Trees (\u00be)","text":"<pre><code>template&lt;typename T&gt;\nclass ParallelBST {\n    struct Node {\n        T data;\n        std::atomic&lt;Node*&gt; left, right;\n        std::atomic&lt;bool&gt; marked;  // For deletion\n    };\n\n    std::atomic&lt;Node*&gt; root;\n\npublic:\n    bool insert(const T&amp; value) {\n        Node* new_node = new Node{value, nullptr, nullptr, false};\n\n        while(true) {\n            Node* current = root.load();\n            if(!current) {\n                if(root.compare_exchange_strong(current, new_node)) {\n                    return true;\n                }\n                continue;\n            }\n        }\n    }\n};\n</code></pre>","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-2/cen429-week-2/#parallel-search-trees-44","title":"Parallel Search Trees (4/4)","text":"<pre><code>template&lt;typename T&gt;\nclass ParallelBST {\n    struct Node {\n        T data;\n        std::atomic&lt;Node*&gt; left, right;\n        std::atomic&lt;bool&gt; marked;  // For deletion\n    };\n\n    std::atomic&lt;Node*&gt; root;\n\npublic:\n    bool insert(const T&amp; value) {\n        Node* new_node = new Node{value, nullptr, nullptr, false};\n\n        while(true) {\n            Node* current = root.load();\n            if(!current) {\n                if(root.compare_exchange_strong(current, new_node)) {\n                    return true;\n                }\n                continue;\n            }\n        }\n    }\n};\n</code></pre> \\[ End-Of-Week-2 \\]","tags":["cen310-week-2","parallel-programming","parallel-computing","parallel-architectures","performance-analysis"]},{"location":"week-3/cen310-week-3/","title":"CEN310 Parallel Programming Week-3","text":""},{"location":"week-3/cen310-week-3/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-3/cen310-week-3/#week-3","title":"Week-3","text":""},{"location":"week-3/cen310-week-3/#openmp-programming","title":"OpenMP Programming","text":""},{"location":"week-3/cen310-week-3/#outline","title":"Outline","text":"<ol> <li>Introduction to OpenMP</li> <li>What is OpenMP?</li> <li>Fork-Join Model</li> <li>Compiler Directives</li> <li>Runtime Library Functions</li> <li> <p>Environment Variables</p> </li> <li> <p>OpenMP Directives</p> </li> <li>Parallel Regions</li> <li>Work Sharing Constructs</li> <li>Data Sharing Attributes</li> <li> <p>Synchronization</p> </li> <li> <p>OpenMP Programming Examples</p> </li> <li>Basic Parallel Loops</li> <li>Reduction Operations</li> <li>Task Parallelism</li> <li> <p>Nested Parallelism</p> </li> <li> <p>Performance Considerations</p> </li> <li>Thread Management</li> <li>Load Balancing</li> <li>Data Locality</li> <li>Cache Effects</li> </ol>"},{"location":"week-3/cen310-week-3/#1-introduction-to-openmp","title":"1. Introduction to OpenMP","text":""},{"location":"week-3/cen310-week-3/#what-is-openmp","title":"What is OpenMP?","text":"<ul> <li>API for shared-memory parallel programming</li> <li>Supports C, C++, and Fortran</li> <li>Based on compiler directives</li> <li>Portable and scalable</li> </ul> <p>Example: </p><pre><code>#include &lt;omp.h&gt;\n\nint main() {\n    #pragma omp parallel\n    {\n        printf(\"Hello from thread %d\\n\", \n               omp_get_thread_num());\n    }\n    return 0;\n}\n</code></pre> <p>// ... continue with detailed content for Week-3 </p>"},{"location":"week-4/cen310-week-4/","title":"CEN310 Parallel Programming Week-4","text":""},{"location":"week-4/cen310-week-4/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-4/cen310-week-4/#week-4","title":"Week-4","text":""},{"location":"week-4/cen310-week-4/#mpi-programming","title":"MPI Programming","text":""},{"location":"week-4/cen310-week-4/#outline","title":"Outline","text":"<ol> <li>Introduction to MPI</li> <li>What is MPI?</li> <li>Distributed Memory Model</li> <li>MPI Implementation Types</li> <li>Basic Concepts</li> <li> <p>Environment Setup</p> </li> <li> <p>Point-to-Point Communication</p> </li> <li>Blocking Send/Receive</li> <li>Non-blocking Send/Receive</li> <li>Buffering and Synchronization</li> <li>Communication Modes</li> <li> <p>Error Handling</p> </li> <li> <p>Collective Communication</p> </li> <li>Broadcast</li> <li>Scatter/Gather</li> <li>Reduce Operations</li> <li>All-to-All Communication</li> <li> <p>Barriers</p> </li> <li> <p>Advanced MPI Features</p> </li> <li>Derived Datatypes</li> <li>Virtual Topologies</li> <li>One-sided Communication</li> <li>Hybrid Programming (MPI + OpenMP)</li> </ol>"},{"location":"week-4/cen310-week-4/#1-introduction-to-mpi","title":"1. Introduction to MPI","text":""},{"location":"week-4/cen310-week-4/#what-is-mpi","title":"What is MPI?","text":"<ul> <li>Message Passing Interface standard</li> <li>Platform-independent communication protocol</li> <li>Supports distributed memory systems</li> <li>Language bindings for C, C++, Fortran</li> </ul> <p>Example: </p><pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    int rank, size;\n\n    MPI_Init(&amp;argc, &amp;argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n    printf(\"Process %d of %d\\n\", rank, size);\n\n    MPI_Finalize();\n    return 0;\n}\n</code></pre> <p>// ... continue with detailed content for Week-4 </p>"},{"location":"week-5/cen310-week-5/","title":"CEN310 Parallel Programming Week-5","text":""},{"location":"week-5/cen310-week-5/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-5/cen310-week-5/#week-5","title":"Week-5","text":""},{"location":"week-5/cen310-week-5/#gpu-programming","title":"GPU Programming","text":""},{"location":"week-5/cen310-week-5/#outline","title":"Outline","text":"<ol> <li>Introduction to GPU Computing</li> <li>GPU Architecture Overview</li> <li>CUDA Programming Model</li> <li>GPU Memory Hierarchy</li> <li>Thread Hierarchy</li> <li> <p>Kernel Functions</p> </li> <li> <p>CUDA Programming Basics</p> </li> <li>Memory Management</li> <li>Thread Organization</li> <li>Synchronization</li> <li>Error Handling</li> <li> <p>CUDA Runtime API</p> </li> <li> <p>Performance Optimization</p> </li> <li>Memory Coalescing</li> <li>Shared Memory Usage</li> <li>Bank Conflicts</li> <li>Occupancy</li> <li> <p>Warp Divergence</p> </li> <li> <p>Advanced GPU Programming</p> </li> <li>Streams and Events</li> <li>Asynchronous Operations</li> <li>Multi-GPU Programming</li> <li>GPU-CPU Data Transfer</li> <li>Unified Memory</li> </ol>"},{"location":"week-5/cen310-week-5/#1-introduction-to-gpu-computing","title":"1. Introduction to GPU Computing","text":""},{"location":"week-5/cen310-week-5/#gpu-architecture","title":"GPU Architecture","text":"<pre><code>Host (CPU)           Device (GPU)\n   \u2193                      \u2193\nMemory               Global Memory\n   \u2193                      \u2193\n   \u2190--- PCI Express Bus -\u2192\n</code></pre> <p>Basic Concepts: - Massively parallel architecture - Thousands of cores - SIMT execution model - Memory hierarchy</p> <p>Example CUDA Program: </p><pre><code>#include &lt;cuda_runtime.h&gt;\n\n__global__ void vectorAdd(float* a, float* b, float* c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    // Allocate host memory\n    float *h_a = (float*)malloc(size);\n    float *h_b = (float*)malloc(size);\n    float *h_c = (float*)malloc(size);\n\n    // Initialize arrays\n    for(int i = 0; i &lt; n; i++) {\n        h_a[i] = rand() / (float)RAND_MAX;\n        h_b[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Allocate device memory\n    float *d_a, *d_b, *d_c;\n    cudaMalloc(&amp;d_a, size);\n    cudaMalloc(&amp;d_b, size);\n    cudaMalloc(&amp;d_c, size);\n\n    // Copy to device\n    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n\n    // Launch kernel\n    int blockSize = 256;\n    int numBlocks = (n + blockSize - 1) / blockSize;\n    vectorAdd&lt;&lt;&lt;numBlocks, blockSize&gt;&gt;&gt;(d_a, d_b, d_c, n);\n\n    // Copy result back\n    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n\n    // Cleanup\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n    free(h_a);\n    free(h_b);\n    free(h_c);\n\n    return 0;\n}\n</code></pre> <p>// ... continue with detailed content for Week-5 </p>"},{"location":"week-6/cen310-week-6/","title":"CEN310 Parallel Programming Week-6","text":""},{"location":"week-6/cen310-week-6/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-6/cen310-week-6/#week-6","title":"Week-6","text":""},{"location":"week-6/cen310-week-6/#performance-optimization","title":"Performance Optimization","text":""},{"location":"week-6/cen310-week-6/#outline","title":"Outline","text":"<ol> <li>Performance Analysis Tools</li> <li>Profilers</li> <li>Hardware Counters</li> <li>Performance Metrics</li> <li>Bottleneck Detection</li> <li> <p>Benchmarking</p> </li> <li> <p>Memory Optimization</p> </li> <li>Cache Optimization</li> <li>Memory Access Patterns</li> <li>Data Layout</li> <li>False Sharing</li> <li> <p>Memory Bandwidth</p> </li> <li> <p>Algorithm Optimization</p> </li> <li>Load Balancing</li> <li>Work Distribution</li> <li>Communication Patterns</li> <li>Synchronization Overhead</li> <li> <p>Scalability Analysis</p> </li> <li> <p>Advanced Optimization Techniques</p> </li> <li>Vectorization</li> <li>Loop Optimization</li> <li>Thread Affinity</li> <li>Compiler Optimizations</li> <li>Hardware-Specific Tuning</li> </ol>"},{"location":"week-6/cen310-week-6/#1-performance-analysis-tools","title":"1. Performance Analysis Tools","text":""},{"location":"week-6/cen310-week-6/#using-profilers","title":"Using Profilers","text":"<p>Example using Intel VTune: </p><pre><code>#include &lt;omp.h&gt;\n#include &lt;vector&gt;\n\nvoid optimize_matrix_multiply(const std::vector&lt;float&gt;&amp; A,\n                            const std::vector&lt;float&gt;&amp; B,\n                            std::vector&lt;float&gt;&amp; C,\n                            int N) {\n    #pragma omp parallel for collapse(2)\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            float sum = 0.0f;\n            // Cache-friendly access pattern\n            for(int k = 0; k &lt; N; k++) {\n                sum += A[i * N + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n\n// Performance measurement\nvoid measure_performance() {\n    const int N = 1024;\n    std::vector&lt;float&gt; A(N * N), B(N * N), C(N * N);\n\n    // Initialize matrices\n    for(int i = 0; i &lt; N * N; i++) {\n        A[i] = rand() / (float)RAND_MAX;\n        B[i] = rand() / (float)RAND_MAX;\n    }\n\n    double start = omp_get_wtime();\n    optimize_matrix_multiply(A, B, C, N);\n    double end = omp_get_wtime();\n\n    printf(\"Time: %f seconds\\n\", end - start);\n}\n</code></pre>"},{"location":"week-6/cen310-week-6/#2-memory-optimization","title":"2. Memory Optimization","text":""},{"location":"week-6/cen310-week-6/#cache-friendly-data-access","title":"Cache-Friendly Data Access","text":"<pre><code>// Bad: Cache-unfriendly access\nvoid bad_access(float* matrix, int N) {\n    for(int j = 0; j &lt; N; j++) {\n        for(int i = 0; i &lt; N; i++) {\n            matrix[i * N + j] = compute(i, j);\n        }\n    }\n}\n\n// Good: Cache-friendly access\nvoid good_access(float* matrix, int N) {\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            matrix[i * N + j] = compute(i, j);\n        }\n    }\n}\n</code></pre>"},{"location":"week-6/cen310-week-6/#false-sharing-prevention","title":"False Sharing Prevention","text":"<pre><code>// Bad: False sharing\nstruct BadCounter {\n    int count;  // Multiple threads updating adjacent memory\n};\n\n// Good: Padding to prevent false sharing\nstruct GoodCounter {\n    int count;\n    char padding[60];  // Align to cache line size\n};\n\nvoid parallel_count() {\n    const int NUM_THREADS = 4;\n    GoodCounter counters[NUM_THREADS];\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        for(int i = 0; i &lt; 1000000; i++) {\n            counters[tid].count++;\n        }\n    }\n}\n</code></pre> <p>// ... continue with detailed content for Week-6 </p>"},{"location":"week-7/cen310-week-7/","title":"CEN310 Parallel Programming Week-7","text":""},{"location":"week-7/cen310-week-7/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-7/cen310-week-7/#week-7-quiz-1","title":"Week-7 (Quiz-1)","text":""},{"location":"week-7/cen310-week-7/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-7/cen310-week-7/#quiz-1-information","title":"Quiz-1 Information","text":""},{"location":"week-7/cen310-week-7/#date-and-time","title":"Date and Time","text":"<ul> <li>Date: March 28, 2025</li> <li>Time: 09:00-12:00 (3 hours)</li> <li>Location: Regular classroom</li> </ul>"},{"location":"week-7/cen310-week-7/#format","title":"Format","text":"<ul> <li>Written examination</li> <li>Mix of theoretical questions and practical problems</li> <li>Both closed and open-ended questions</li> </ul>"},{"location":"week-7/cen310-week-7/#topics-covered","title":"Topics Covered","text":""},{"location":"week-7/cen310-week-7/#1-introduction-to-parallel-computing","title":"1. Introduction to Parallel Computing","text":"<ul> <li>Parallel computing concepts</li> <li>Types of parallelism</li> <li>Performance metrics</li> <li>Architecture overview</li> </ul>"},{"location":"week-7/cen310-week-7/#2-openmp-programming","title":"2. OpenMP Programming","text":"<ul> <li>Shared memory programming</li> <li>Thread management</li> <li>Data parallelism</li> <li>Synchronization</li> <li>Performance optimization</li> </ul>"},{"location":"week-7/cen310-week-7/#3-mpi-programming","title":"3. MPI Programming","text":"<ul> <li>Distributed memory concepts</li> <li>Point-to-point communication</li> <li>Collective operations</li> <li>Common parallel patterns</li> </ul>"},{"location":"week-7/cen310-week-7/#sample-questions","title":"Sample Questions","text":""},{"location":"week-7/cen310-week-7/#theoretical-questions","title":"Theoretical Questions","text":"<ol> <li>Explain the difference between task parallelism and data parallelism.</li> <li>What are the main considerations for load balancing in parallel programs?</li> <li>Compare and contrast OpenMP and MPI programming models.</li> </ol>"},{"location":"week-7/cen310-week-7/#practical-problems","title":"Practical Problems","text":"<pre><code>// Question 1: What is the output of this OpenMP program?\n#include &lt;omp.h&gt;\n#include &lt;stdio.h&gt;\n\nint main() {\n    int x = 0;\n    #pragma omp parallel num_threads(4) shared(x)\n    {\n        #pragma omp critical\n        x++;\n        #pragma omp barrier\n        if (omp_get_thread_num() == 0)\n            printf(\"x = %d\\n\", x);\n    }\n    return 0;\n}\n</code></pre>"},{"location":"week-7/cen310-week-7/#preparation-guidelines","title":"Preparation Guidelines","text":""},{"location":"week-7/cen310-week-7/#1-review-materials","title":"1. Review Materials","text":"<ul> <li>Course slides and notes</li> <li>Lab exercises</li> <li>Practice problems</li> <li>Sample codes</li> </ul>"},{"location":"week-7/cen310-week-7/#2-focus-areas","title":"2. Focus Areas","text":"<ul> <li>OpenMP directives and clauses</li> <li>MPI communication patterns</li> <li>Performance optimization techniques</li> <li>Parallel algorithm design</li> </ul>"},{"location":"week-7/cen310-week-7/#3-practice-exercises","title":"3. Practice Exercises","text":"<ul> <li>Solve previous examples</li> <li>Write and analyze parallel programs</li> <li>Debug common issues</li> <li>Measure performance improvements</li> </ul>"},{"location":"week-7/cen310-week-7/#quiz-rules","title":"Quiz Rules","text":"<ol> <li>Materials Allowed</li> <li>No books or notes allowed</li> <li>No electronic devices</li> <li> <p>Clean paper provided for scratch work</p> </li> <li> <p>Time Management</p> </li> <li>Read all questions carefully</li> <li>Plan your time for each section</li> <li> <p>Leave time for review</p> </li> <li> <p>Answering Questions</p> </li> <li>Show all your work</li> <li>Explain your reasoning</li> <li>Write clearly and organize your answers</li> </ol>"},{"location":"week-7/cen310-week-7/#grading-criteria","title":"Grading Criteria","text":""},{"location":"week-7/cen310-week-7/#distribution","title":"Distribution","text":"<ul> <li>Theoretical Questions: 40%</li> <li>Practical Problems: 60%</li> </ul>"},{"location":"week-7/cen310-week-7/#evaluation","title":"Evaluation","text":"<ul> <li>Understanding of concepts</li> <li>Problem-solving approach</li> <li>Code analysis and writing</li> <li>Performance considerations</li> <li>Clear explanations</li> </ul>"},{"location":"week-7/cen310-week-7/#additional-resources","title":"Additional Resources","text":""},{"location":"week-7/cen310-week-7/#review-materials","title":"Review Materials","text":"<ul> <li>Previous lecture slides</li> <li>Lab exercise solutions</li> <li>Practice problem sets</li> <li>Online documentation:</li> <li>OpenMP: https://www.openmp.org/</li> <li>MPI: https://www.open-mpi.org/</li> </ul>"},{"location":"week-7/cen310-week-7/#sample-code-repository","title":"Sample Code Repository","text":"<ul> <li>Course GitHub repository</li> <li>Example implementations</li> <li>Performance benchmarks</li> </ul>"},{"location":"week-7/cen310-week-7/#contact-information","title":"Contact Information","text":"<p>For any questions about the quiz:</p> <ul> <li>Email: ugur.coruh@erdogan.edu.tr</li> <li>Office Hours: By appointment</li> <li>Location: Engineering Faculty</li> </ul>"},{"location":"week-7/cen310-week-7/#good-luck","title":"Good Luck!","text":""},{"location":"week-8/cen310-week-8/","title":"CEN310 Parallel Programming Week-8","text":""},{"location":"week-8/cen310-week-8/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-8/cen310-week-8/#week-8-midterm-project-review","title":"Week-8 (Midterm Project Review)","text":""},{"location":"week-8/cen310-week-8/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-8/cen310-week-8/#project-review-day-schedule","title":"Project Review Day Schedule","text":""},{"location":"week-8/cen310-week-8/#morning-session-0900-1200","title":"Morning Session (09:00-12:00)","text":"<ul> <li>Project presentations (Group 1-4)</li> <li>Performance analysis discussions</li> <li>Q&amp;A sessions</li> </ul>"},{"location":"week-8/cen310-week-8/#lunch-break-1200-1300","title":"Lunch Break (12:00-13:00)","text":""},{"location":"week-8/cen310-week-8/#afternoon-session-1300-1700","title":"Afternoon Session (13:00-17:00)","text":"<ul> <li>Project presentations (Group 5-8)</li> <li>Technical demonstrations</li> <li>Final feedback</li> </ul>"},{"location":"week-8/cen310-week-8/#project-requirements","title":"Project Requirements","text":""},{"location":"week-8/cen310-week-8/#1-documentation","title":"1. Documentation","text":"<ul> <li>Project report</li> <li>Source code documentation</li> <li>Performance analysis results</li> <li>Implementation challenges</li> <li>Future improvements</li> </ul>"},{"location":"week-8/cen310-week-8/#2-implementation","title":"2. Implementation","text":"<ul> <li>Working parallel program</li> <li>Use of OpenMP and/or MPI</li> <li>Performance optimizations</li> <li>Error handling</li> <li>Code quality</li> </ul>"},{"location":"week-8/cen310-week-8/#presentation-guidelines","title":"Presentation Guidelines","text":""},{"location":"week-8/cen310-week-8/#format","title":"Format","text":"<ul> <li>20 minutes per group</li> <li>15 minutes presentation</li> <li>5 minutes Q&amp;A</li> </ul>"},{"location":"week-8/cen310-week-8/#content","title":"Content","text":"<ol> <li>Problem Description</li> <li>Solution Approach</li> <li>Implementation Details</li> <li>Performance Results</li> <li>Challenges &amp; Solutions</li> <li>Demo</li> </ol>"},{"location":"week-8/cen310-week-8/#performance-analysis-requirements","title":"Performance Analysis Requirements","text":""},{"location":"week-8/cen310-week-8/#metrics-to-cover","title":"Metrics to Cover","text":"<ul> <li>Execution time</li> <li>Speedup</li> <li>Efficiency</li> <li>Scalability</li> <li>Resource utilization</li> </ul>"},{"location":"week-8/cen310-week-8/#analysis-tools","title":"Analysis Tools","text":"<pre><code># Example performance measurement\n$ perf stat ./parallel_program\n$ nvprof ./cuda_program\n$ vtune ./openmp_program\n</code></pre>"},{"location":"week-8/cen310-week-8/#example-project-structure","title":"Example Project Structure","text":"<pre><code>// Project architecture example\nproject/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.cpp\n\u2502   \u251c\u2500\u2500 parallel_impl.cpp\n\u2502   \u2514\u2500\u2500 utils.cpp\n\u251c\u2500\u2500 include/\n\u2502   \u251c\u2500\u2500 parallel_impl.h\n\u2502   \u2514\u2500\u2500 utils.h\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_parallel.cpp\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 report.pdf\n\u2502   \u2514\u2500\u2500 presentation.pptx\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"week-8/cen310-week-8/#performance-comparison-template","title":"Performance Comparison Template","text":""},{"location":"week-8/cen310-week-8/#sequential-vs-parallel-implementation","title":"Sequential vs Parallel Implementation","text":"<pre><code>// Sequential implementation\ndouble sequential_time = 0.0;\n{\n    auto start = std::chrono::high_resolution_clock::now();\n    sequential_result = sequential_compute();\n    auto end = std::chrono::high_resolution_clock::now();\n    sequential_time = std::chrono::duration&lt;double&gt;(end-start).count();\n}\n\n// Parallel implementation\ndouble parallel_time = 0.0;\n{\n    auto start = std::chrono::high_resolution_clock::now();\n    parallel_result = parallel_compute();\n    auto end = std::chrono::high_resolution_clock::now();\n    parallel_time = std::chrono::duration&lt;double&gt;(end-start).count();\n}\n\n// Calculate speedup\ndouble speedup = sequential_time / parallel_time;\n</code></pre>"},{"location":"week-8/cen310-week-8/#common-project-topics","title":"Common Project Topics","text":"<ol> <li>Matrix Operations</li> <li>Matrix multiplication</li> <li>Matrix decomposition</li> <li> <p>Linear equation solving</p> </li> <li> <p>Scientific Computing</p> </li> <li>N-body simulation</li> <li>Wave equation solver</li> <li> <p>Monte Carlo methods</p> </li> <li> <p>Data Processing</p> </li> <li>Image processing</li> <li>Signal processing</li> <li> <p>Data mining</p> </li> <li> <p>Graph Algorithms</p> </li> <li>Shortest path</li> <li>Graph coloring</li> <li>Maximum flow</li> </ol>"},{"location":"week-8/cen310-week-8/#evaluation-criteria","title":"Evaluation Criteria","text":""},{"location":"week-8/cen310-week-8/#technical-aspects-60","title":"Technical Aspects (60%)","text":"<ul> <li>Correct implementation (20%)</li> <li>Performance optimization (20%)</li> <li>Code quality (10%)</li> <li>Documentation (10%)</li> </ul>"},{"location":"week-8/cen310-week-8/#presentation-40","title":"Presentation (40%)","text":"<ul> <li>Clear explanation (15%)</li> <li>Demo quality (15%)</li> <li>Q&amp;A handling (10%)</li> </ul>"},{"location":"week-8/cen310-week-8/#project-report-template","title":"Project Report Template","text":""},{"location":"week-8/cen310-week-8/#1-introduction","title":"1. Introduction","text":"<ul> <li>Problem statement</li> <li>Objectives</li> <li>Background</li> </ul>"},{"location":"week-8/cen310-week-8/#2-design","title":"2. Design","text":"<ul> <li>Architecture</li> <li>Algorithms</li> <li>Parallelization strategy</li> </ul>"},{"location":"week-8/cen310-week-8/#3-implementation","title":"3. Implementation","text":"<ul> <li>Technologies used</li> <li>Code structure</li> <li>Key components</li> </ul>"},{"location":"week-8/cen310-week-8/#4-results","title":"4. Results","text":"<ul> <li>Performance measurements</li> <li>Analysis</li> <li>Comparisons</li> </ul>"},{"location":"week-8/cen310-week-8/#5-conclusion","title":"5. Conclusion","text":"<ul> <li>Achievements</li> <li>Challenges</li> <li>Future work</li> </ul>"},{"location":"week-8/cen310-week-8/#resources-references","title":"Resources &amp; References","text":""},{"location":"week-8/cen310-week-8/#documentation","title":"Documentation","text":"<ul> <li>OpenMP: https://www.openmp.org/</li> <li>MPI: https://www.open-mpi.org/</li> <li>CUDA: https://docs.nvidia.com/cuda/</li> </ul>"},{"location":"week-8/cen310-week-8/#tools","title":"Tools","text":"<ul> <li>Performance analysis tools</li> <li>Debugging tools</li> <li>Profiling tools</li> </ul>"},{"location":"week-8/cen310-week-8/#questions-discussion","title":"Questions &amp; Discussion","text":""},{"location":"week-9-midterm/cen310-week-9/","title":"CEN310 Parallel Programming Week-9","text":""},{"location":"week-9-midterm/cen310-week-9/#cen310-parallel-programming","title":"CEN310 Parallel Programming","text":""},{"location":"week-9-midterm/cen310-week-9/#week-9-midterm-examination-period","title":"Week-9 (Midterm Examination Period)","text":""},{"location":"week-9-midterm/cen310-week-9/#spring-semester-2024-2025","title":"Spring Semester, 2024-2025","text":""},{"location":"week-9-midterm/cen310-week-9/#midterm-examination-period-information","title":"Midterm Examination Period Information","text":""},{"location":"week-9-midterm/cen310-week-9/#dates","title":"Dates","text":"<ul> <li>Period: April 5-13, 2025</li> <li>Project Report Due: As scheduled by the university</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#location","title":"Location","text":"<ul> <li>As assigned by the university</li> <li>Check the official examination schedule</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#midterm-project-report-requirements","title":"Midterm Project Report Requirements","text":""},{"location":"week-9-midterm/cen310-week-9/#1-project-documentation","title":"1. Project Documentation","text":"<ul> <li>Complete project report</li> <li>Source code with documentation</li> <li>Performance analysis results</li> <li>Implementation details</li> <li>Future work proposals</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#2-technical-requirements","title":"2. Technical Requirements","text":"<ul> <li>Code quality and organization</li> <li>Performance optimization results</li> <li>Comparison with sequential version</li> <li>Scalability analysis</li> <li>Error handling implementation</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#report-structure","title":"Report Structure","text":""},{"location":"week-9-midterm/cen310-week-9/#1-executive-summary","title":"1. Executive Summary","text":"<ul> <li>Project overview</li> <li>Key achievements</li> <li>Performance highlights</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#2-technical-implementation","title":"2. Technical Implementation","text":"<ul> <li>Architecture details</li> <li>Algorithm descriptions</li> <li>Parallelization strategy</li> <li>Code structure</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#3-performance-analysis","title":"3. Performance Analysis","text":"<ul> <li>Benchmark results</li> <li>Scalability tests</li> <li>Resource utilization</li> <li>Optimization efforts</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#4-conclusions","title":"4. Conclusions","text":"<ul> <li>Lessons learned</li> <li>Challenges overcome</li> <li>Future improvements</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#submission-guidelines","title":"Submission Guidelines","text":""},{"location":"week-9-midterm/cen310-week-9/#format-requirements","title":"Format Requirements","text":"<ul> <li>PDF format</li> <li>Professional formatting</li> <li>Clear code listings</li> <li>Proper citations</li> <li>Performance graphs</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#submission-process","title":"Submission Process","text":"<ul> <li>Digital submission</li> <li>Source code repository</li> <li>Documentation package</li> <li>Presentation slides</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#evaluation-criteria","title":"Evaluation Criteria","text":""},{"location":"week-9-midterm/cen310-week-9/#technical-depth-40","title":"Technical Depth (40%)","text":"<ul> <li>Implementation quality</li> <li>Performance optimization</li> <li>Code organization</li> <li>Documentation quality</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#analysis-results-40","title":"Analysis &amp; Results (40%)","text":"<ul> <li>Performance measurements</li> <li>Scalability analysis</li> <li>Comparative evaluation</li> <li>Problem-solving approach</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#documentation-20","title":"Documentation (20%)","text":"<ul> <li>Report quality</li> <li>Code documentation</li> <li>Presentation materials</li> <li>Future recommendations</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#important-notes","title":"Important Notes","text":""},{"location":"week-9-midterm/cen310-week-9/#deadlines","title":"Deadlines","text":"<ul> <li>Report submission deadline is strict</li> <li>Late submissions may not be accepted</li> <li>Extensions require prior approval</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#academic-integrity","title":"Academic Integrity","text":"<ul> <li>Original work required</li> <li>Proper citations needed</li> <li>Code plagiarism checked</li> <li>Collaboration must be acknowledged</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#contact-information","title":"Contact Information","text":"<p>For examination related queries:</p> <ul> <li>Email: ugur.coruh@erdogan.edu.tr</li> <li>Office Hours: By appointment</li> <li>Location: Engineering Faculty</li> </ul>"},{"location":"week-9-midterm/cen310-week-9/#good-luck-with-your-midterm-examination","title":"Good Luck with Your Midterm Examination!","text":""},{"location":"tr/license/","title":"Lisans","text":""},{"location":"tr/license/#code-license","title":"Code License","text":"<p>MIT Lisans\u0131</p> <p>Telif hakk\u0131 \u00a9 2019-2024 U\u011fur CORUH</p> <p>Bu yaz\u0131l\u0131m\u0131n ve ili\u015fkili dok\u00fcmantasyon dosyalar\u0131n\u0131n (\"Yaz\u0131l\u0131m\") bir kopyas\u0131n\u0131 edinen her ki\u015fiye, a\u015fa\u011f\u0131daki ko\u015fullara tabi olmak kayd\u0131yla, Yaz\u0131l\u0131m\u0131 herhangi bir k\u0131s\u0131tlama olmaks\u0131z\u0131n kullanma, kopyalama, de\u011fi\u015ftirme, birle\u015ftirme, yay\u0131nlama, da\u011f\u0131tma, alt lisanslama ve/veya Yaz\u0131l\u0131m\u0131n kopyalar\u0131n\u0131 satma hakk\u0131 \u00fccretsiz olarak verilmektedir, ayr\u0131ca Yaz\u0131l\u0131m\u0131n sa\u011fland\u0131\u011f\u0131 ki\u015filere de bu haklar\u0131n verilmesine izin verilmektedir:</p> <p>Yukar\u0131daki telif hakk\u0131 bildirimi ve bu izin bildirimi, Yaz\u0131l\u0131m\u0131n t\u00fcm kopyalar\u0131na veya \u00f6nemli k\u0131s\u0131mlar\u0131na dahil edilecektir.</p> <p>YAZILIM, \"OLDU\u011eU G\u0130B\u0130\" SA\u011eLANMAKTADIR, T\u0130CAR\u0130 ELVER\u0130\u015eL\u0130L\u0130K, BEL\u0130RL\u0130 B\u0130R AMACA UYGUNLUK VE \u0130HLAL ETMEME DAH\u0130L ANCAK BUNLARLA SINIRLI OLMAMAK \u00dcZERE A\u00c7IK VEYA ZIMN\u0130 HERHANG\u0130 B\u0130R GARANT\u0130 OLMAKSIZIN SA\u011eLANMAKTADIR. H\u0130\u00c7B\u0130R DURUMDA YAZARLAR VEYA TEL\u0130F HAKKI SAH\u0130PLER\u0130, YAZILIMIN VEYA YAZILIMIN KULLANIMI VEYA BA\u015eKA B\u0130R \u015eEK\u0130LDE \u0130LG\u0130L\u0130 OLAN HERHANG\u0130 B\u0130R TALEP, ZARAR VEYA D\u0130\u011eER Y\u00dcK\u00dcML\u00dcL\u00dcKLERDEN SORUMLU TUTULAMAZ.</p>"},{"location":"tr/license/#content-license","title":"Content License","text":"<p>Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 Uluslararas\u0131 Lisans\u0131 (CC BY-NC-ND 4.0)</p> <p>Serbestsiniz:</p> <ul> <li>Payla\u015fmak: Materyali her ortamda veya formatta kopyalayabilir ve yeniden da\u011f\u0131tabilirsiniz.</li> </ul> <p>A\u015fa\u011f\u0131daki ko\u015fullar alt\u0131nda:</p> <ul> <li>At\u0131f: Uygun at\u0131f yapmal\u0131s\u0131n\u0131z, lisansa bir ba\u011flant\u0131 sa\u011flamal\u0131s\u0131n\u0131z ve de\u011fi\u015fiklik yap\u0131l\u0131p yap\u0131lmad\u0131\u011f\u0131n\u0131 belirtmelisiniz. Bunu makul bir \u015fekilde yapabilirsiniz, ancak lisans verenin sizi veya kullan\u0131m\u0131n\u0131z\u0131 onaylad\u0131\u011f\u0131n\u0131 ima edecek bir \u015fekilde de\u011fil.</li> <li>Ticari Olmayan: Materyali ticari ama\u00e7larla kullanamazs\u0131n\u0131z.</li> <li>T\u00fcrev Yaratamazs\u0131n\u0131z: Materyali yeniden kar\u0131\u015ft\u0131ramaz, d\u00f6n\u00fc\u015ft\u00fcremez veya \u00fczerine in\u015fa edemezsiniz.</li> </ul> <p>Ek k\u0131s\u0131tlama yoktur \u2014 Lisans\u0131n izin verdi\u011fi hi\u00e7bir \u015feyi ba\u015fkalar\u0131n\u0131n yapmas\u0131n\u0131 yasal olarak k\u0131s\u0131tlayan yasal terimler veya teknolojik \u00f6nlemler uygulayamazs\u0131n\u0131z.</p> <p>Daha fazla detay i\u00e7in: Creative Commons CC BY-NC-ND 4.0</p>"},{"location":"tr/tags/#tag:cen310","title":"cen310","text":"<ul> <li>            Ders Program\u0131          </li> </ul>"},{"location":"tr/tags/#tag:cen310-syllabus","title":"cen310-syllabus","text":"<ul> <li>            Ders Program\u0131          </li> </ul>"},{"location":"tr/tags/#tag:parallel-programming","title":"parallel-programming","text":"<ul> <li>            CEN310 Parallel Programming Week-1          </li> <li>            CEN310 Parallel Programming Week-2          </li> <li>            Ders Program\u0131          </li> </ul>"},{"location":"tr/tags/#tag:spring-2025","title":"spring-2025","text":"<ul> <li>            CEN310 Parallel Programming Week-1          </li> <li>            Ders Program\u0131          </li> </ul>"},{"location":"tr/changelog/","title":"Revizyonlar","text":""},{"location":"tr/resume/","title":"\u00d6zge\u00e7mi\u015f","text":""},{"location":"tr/resume/#ozgecmis","title":"\u00d6zge\u00e7mi\u015f","text":"<ul> <li>\u00d6zge\u00e7mi\u015fi \u0130ndir</li> </ul>"},{"location":"tr/week-10/cen310-week-10/","title":"CEN310 Paralel Programlama Hafta-10","text":""},{"location":"tr/week-10/cen310-week-10/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-10/cen310-week-10/#hafta-10-paralel-algoritma-tasarm-ve-gpu-temelleri","title":"Hafta-10 (Paralel Algoritma Tasar\u0131m\u0131 ve GPU Temelleri)","text":""},{"location":"tr/week-10/cen310-week-10/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-10/cen310-week-10/#genel-baks","title":"Genel Bak\u0131\u015f","text":""},{"location":"tr/week-10/cen310-week-10/#konular","title":"Konular","text":"<ol> <li>Paralel Algoritma Tasar\u0131m Stratejileri</li> <li>Ayr\u0131\u015ft\u0131rma Teknikleri</li> <li>GPU Mimarisi Temelleri</li> <li>CUDA Programlamaya Giri\u015f</li> </ol>"},{"location":"tr/week-10/cen310-week-10/#hedefler","title":"Hedefler","text":"<ul> <li>Paralel algoritma tasar\u0131m prensiplerini anlamak</li> <li>Veri ayr\u0131\u015ft\u0131rma y\u00f6ntemlerini \u00f6\u011frenmek</li> <li>GPU mimarisini ke\u015ffetmek</li> <li>CUDA programlamaya ba\u015flamak</li> </ul>"},{"location":"tr/week-10/cen310-week-10/#1-paralel-algoritma-tasarm-stratejileri","title":"1. Paralel Algoritma Tasar\u0131m Stratejileri","text":""},{"location":"tr/week-10/cen310-week-10/#tasarm-kalplar","title":"Tasar\u0131m Kal\u0131plar\u0131","text":"<ul> <li>G\u00f6rev paralelli\u011fi</li> <li>Veri paralelli\u011fi</li> <li>Boru hatt\u0131 paralelli\u011fi</li> <li>B\u00f6l ve y\u00f6net</li> </ul>"},{"location":"tr/week-10/cen310-week-10/#ornek-matris-carpm","title":"\u00d6rnek: Matris \u00c7arp\u0131m\u0131","text":"<pre><code>// S\u0131ral\u0131 versiyon\nvoid matris_carpimi(float* A, float* B, float* C, int N) {\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            float toplam = 0.0f;\n            for(int k = 0; k &lt; N; k++) {\n                toplam += A[i*N + k] * B[k*N + j];\n            }\n            C[i*N + j] = toplam;\n        }\n    }\n}\n\n// Paralel versiyon\n#pragma omp parallel for collapse(2)\nvoid paralel_matris_carpimi(float* A, float* B, float* C, int N) {\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            float toplam = 0.0f;\n            for(int k = 0; k &lt; N; k++) {\n                toplam += A[i*N + k] * B[k*N + j];\n            }\n            C[i*N + j] = toplam;\n        }\n    }\n}\n</code></pre>"},{"location":"tr/week-10/cen310-week-10/#2-ayrstrma-teknikleri","title":"2. Ayr\u0131\u015ft\u0131rma Teknikleri","text":""},{"location":"tr/week-10/cen310-week-10/#veri-ayrstrma","title":"Veri Ayr\u0131\u015ft\u0131rma","text":"<ul> <li>Blok ayr\u0131\u015ft\u0131rma</li> <li>D\u00f6ng\u00fcsel ayr\u0131\u015ft\u0131rma</li> <li>Blok-d\u00f6ng\u00fcsel ayr\u0131\u015ft\u0131rma</li> </ul>"},{"location":"tr/week-10/cen310-week-10/#ornek-dizi-isleme","title":"\u00d6rnek: Dizi \u0130\u015fleme","text":"<pre><code>// Blok ayr\u0131\u015ft\u0131rma\nvoid blok_ayristirma(float* veri, int boyut, int blok_sayisi) {\n    int blok_boyutu = boyut / blok_sayisi;\n    #pragma omp parallel for\n    for(int b = 0; b &lt; blok_sayisi; b++) {\n        int baslangic = b * blok_boyutu;\n        int bitis = (b == blok_sayisi-1) ? boyut : (b+1) * blok_boyutu;\n        for(int i = baslangic; i &lt; bitis; i++) {\n            // veri[i] i\u015fle\n        }\n    }\n}\n</code></pre>"},{"location":"tr/week-10/cen310-week-10/#3-gpu-mimarisi-temelleri","title":"3. GPU Mimarisi Temelleri","text":""},{"location":"tr/week-10/cen310-week-10/#donanm-bilesenleri","title":"Donan\u0131m Bile\u015fenleri","text":"<ul> <li>Ak\u0131\u015f \u00c7oki\u015flemcileri (SM'ler)</li> <li>CUDA \u00c7ekirdekleri</li> <li>Bellek Hiyerar\u015fisi</li> <li>Warp Zamanlama</li> </ul>"},{"location":"tr/week-10/cen310-week-10/#bellek-turleri","title":"Bellek T\u00fcrleri","text":"<pre><code>CPU (Ana Bilgisayar)    GPU (Cihaz)\n        \u2193                    \u2193\n   Ana Bellek          Global Bellek\n                           \u2193\n                     Payla\u015f\u0131ml\u0131 Bellek\n                           \u2193\n                       L1 \u00d6nbellek\n                           \u2193\n                      Yazma\u00e7lar\n</code></pre>"},{"location":"tr/week-10/cen310-week-10/#4-cuda-programlamaya-giris","title":"4. CUDA Programlamaya Giri\u015f","text":""},{"location":"tr/week-10/cen310-week-10/#temel-kavramlar","title":"Temel Kavramlar","text":"<ul> <li>\u00c7ekirdekler</li> <li>\u0130\u015f Par\u00e7ac\u0131klar\u0131</li> <li>Bloklar</li> <li>Izgaralar</li> </ul>"},{"location":"tr/week-10/cen310-week-10/#merhaba-dunya-ornegi","title":"Merhaba D\u00fcnya \u00d6rne\u011fi","text":"<pre><code>#include &lt;cuda_runtime.h&gt;\n#include &lt;stdio.h&gt;\n\n__global__ void merhaba_cekirdek() {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    printf(\"%d numaral\u0131 i\u015f par\u00e7ac\u0131\u011f\u0131ndan merhaba\\n\", idx);\n}\n\nint main() {\n    // 256 i\u015f par\u00e7ac\u0131kl\u0131 1 blok ba\u015flat\n    merhaba_cekirdek&lt;&lt;&lt;1, 256&gt;&gt;&gt;();\n    cudaDeviceSynchronize();\n    return 0;\n}\n</code></pre>"},{"location":"tr/week-10/cen310-week-10/#cuda-bellek-yonetimi","title":"CUDA Bellek Y\u00f6netimi","text":""},{"location":"tr/week-10/cen310-week-10/#bellek-islemleri","title":"Bellek \u0130\u015flemleri","text":"<pre><code>// Cihaz belle\u011fi ay\u0131r\nfloat *d_veri;\ncudaMalloc(&amp;d_veri, boyut * sizeof(float));\n\n// Veriyi cihaza kopyala\ncudaMemcpy(d_veri, h_veri, boyut * sizeof(float), \n           cudaMemcpyHostToDevice);\n\n// Sonu\u00e7lar\u0131 geri kopyala\ncudaMemcpy(h_sonuc, d_sonuc, boyut * sizeof(float), \n           cudaMemcpyDeviceToHost);\n\n// Cihaz belle\u011fini serbest b\u0131rak\ncudaFree(d_veri);\n</code></pre>"},{"location":"tr/week-10/cen310-week-10/#vektor-toplama-ornegi","title":"Vekt\u00f6r Toplama \u00d6rne\u011fi","text":"<pre><code>__global__ void vektor_topla(float* a, float* b, float* c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int N = 1000000;\n    size_t boyut = N * sizeof(float);\n\n    // Ana bilgisayar belle\u011fi ay\u0131r\n    float *h_a = (float*)malloc(boyut);\n    float *h_b = (float*)malloc(boyut);\n    float *h_c = (float*)malloc(boyut);\n\n    // Dizileri ba\u015flat\n    for(int i = 0; i &lt; N; i++) {\n        h_a[i] = rand() / (float)RAND_MAX;\n        h_b[i] = rand() / (float)RAND_MAX;\n    }\n\n    // Cihaz belle\u011fi ay\u0131r\n    float *d_a, *d_b, *d_c;\n    cudaMalloc(&amp;d_a, boyut);\n    cudaMalloc(&amp;d_b, boyut);\n    cudaMalloc(&amp;d_c, boyut);\n\n    // Cihaza kopyala\n    cudaMemcpy(d_a, h_a, boyut, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, boyut, cudaMemcpyHostToDevice);\n\n    // \u00c7ekirde\u011fi ba\u015flat\n    int blokBasinaIs = 256;\n    int izgaraBasinaBlok = (N + blokBasinaIs - 1) / blokBasinaIs;\n    vektor_topla&lt;&lt;&lt;izgaraBasinaBlok, blokBasinaIs&gt;&gt;&gt;(d_a, d_b, d_c, N);\n\n    // Sonucu geri kopyala\n    cudaMemcpy(h_c, d_c, boyut, cudaMemcpyDeviceToHost);\n\n    // Temizlik\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n    free(h_a);\n    free(h_b);\n    free(h_c);\n\n    return 0;\n}\n</code></pre>"},{"location":"tr/week-10/cen310-week-10/#laboratuvar-alstrmas","title":"Laboratuvar Al\u0131\u015ft\u0131rmas\u0131","text":""},{"location":"tr/week-10/cen310-week-10/#gorevler","title":"G\u00f6revler","text":"<ol> <li>CUDA kullanarak matris \u00e7arp\u0131m\u0131 uygulama</li> <li>CPU versiyonu ile performans kar\u015f\u0131la\u015ft\u0131rmas\u0131</li> <li>Farkl\u0131 blok boyutlar\u0131 ile denemeler</li> <li>Bellek eri\u015fim desenlerini analiz etme</li> </ol>"},{"location":"tr/week-10/cen310-week-10/#performans-analizi","title":"Performans Analizi","text":"<ul> <li>Profil \u00e7\u0131karmak i\u00e7in nvprof kullan\u0131m\u0131</li> <li>\u00c7al\u0131\u015fma s\u00fcresini \u00f6l\u00e7me</li> <li>H\u0131zlanmay\u0131 hesaplama</li> <li>Bellek transferlerini izleme</li> </ul>"},{"location":"tr/week-10/cen310-week-10/#kaynaklar","title":"Kaynaklar","text":""},{"location":"tr/week-10/cen310-week-10/#dokumantasyon","title":"Dok\u00fcmantasyon","text":"<ul> <li>CUDA Programlama K\u0131lavuzu</li> <li>CUDA En \u0130yi Uygulamalar K\u0131lavuzu</li> <li>NVIDIA Geli\u015ftirici Blogu</li> </ul>"},{"location":"tr/week-10/cen310-week-10/#araclar","title":"Ara\u00e7lar","text":"<ul> <li>NVIDIA NSight</li> <li>CUDA Ara\u00e7 Seti</li> <li>G\u00f6rsel Profilleyici</li> </ul>"},{"location":"tr/week-10/cen310-week-10/#sorular-ve-tartsma","title":"Sorular ve Tart\u0131\u015fma","text":""},{"location":"tr/week-11/cen310-week-11/","title":"CEN310 Paralel Programlama Hafta-11","text":""},{"location":"tr/week-11/cen310-week-11/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-11/cen310-week-11/#hafta-11-ileri-gpu-programlama","title":"Hafta-11 (\u0130leri GPU Programlama)","text":""},{"location":"tr/week-11/cen310-week-11/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-11/cen310-week-11/#genel-baks","title":"Genel Bak\u0131\u015f","text":""},{"location":"tr/week-11/cen310-week-11/#konular","title":"Konular","text":"<ol> <li>CUDA Bellek Modeli</li> <li>Payla\u015f\u0131ml\u0131 Bellek Optimizasyonu</li> <li>\u0130\u015f Par\u00e7ac\u0131\u011f\u0131 Senkronizasyonu</li> <li>Performans Optimizasyon Teknikleri</li> </ol>"},{"location":"tr/week-11/cen310-week-11/#hedefler","title":"Hedefler","text":"<ul> <li>CUDA bellek hiyerar\u015fisini anlamak</li> <li>Payla\u015f\u0131ml\u0131 bellek kullan\u0131m\u0131n\u0131 \u00f6\u011frenmek</li> <li>\u0130\u015f par\u00e7ac\u0131\u011f\u0131 senkronizasyonunda ustala\u015fmak</li> <li>Optimizasyon stratejilerini uygulamak</li> </ul>"},{"location":"tr/week-11/cen310-week-11/#1-cuda-bellek-modeli","title":"1. CUDA Bellek Modeli","text":""},{"location":"tr/week-11/cen310-week-11/#bellek-turleri","title":"Bellek T\u00fcrleri","text":"<ul> <li>Global Bellek</li> <li>Payla\u015f\u0131ml\u0131 Bellek</li> <li>Sabit Bellek</li> <li>Doku Belle\u011fi</li> <li>Yazma\u00e7lar</li> </ul>"},{"location":"tr/week-11/cen310-week-11/#bellek-erisim-desenleri","title":"Bellek Eri\u015fim Desenleri","text":"<pre><code>// Birle\u015ftirilmi\u015f bellek eri\u015fimi \u00f6rne\u011fi\n__global__ void birlesik_erisim(float* veri, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        // Birle\u015ftirilmi\u015f eri\u015fim deseni\n        float deger = veri[idx];\n        // De\u011feri i\u015fle\n        veri[idx] = deger * 2.0f;\n    }\n}\n</code></pre>"},{"location":"tr/week-11/cen310-week-11/#2-paylasml-bellek-optimizasyonu","title":"2. Payla\u015f\u0131ml\u0131 Bellek Optimizasyonu","text":""},{"location":"tr/week-11/cen310-week-11/#paylasml-bellek-kullanm","title":"Payla\u015f\u0131ml\u0131 Bellek Kullan\u0131m\u0131","text":"<pre><code>__global__ void matris_carpimi(float* A, float* B, float* C, int N) {\n    __shared__ float paylasimliA[BLOK_BOYUTU][BLOK_BOYUTU];\n    __shared__ float paylasimliB[BLOK_BOYUTU][BLOK_BOYUTU];\n\n    int satir = blockIdx.y * blockDim.y + threadIdx.y;\n    int sutun = blockIdx.x * blockDim.x + threadIdx.x;\n    float toplam = 0.0f;\n\n    for(int karo = 0; karo &lt; N/BLOK_BOYUTU; karo++) {\n        // Veriyi payla\u015f\u0131ml\u0131 belle\u011fe y\u00fckle\n        paylasimliA[threadIdx.y][threadIdx.x] = \n            A[satir * N + karo * BLOK_BOYUTU + threadIdx.x];\n        paylasimliB[threadIdx.y][threadIdx.x] = \n            B[(karo * BLOK_BOYUTU + threadIdx.y) * N + sutun];\n\n        __syncthreads();\n\n        // Payla\u015f\u0131ml\u0131 bellek kullanarak hesapla\n        for(int k = 0; k &lt; BLOK_BOYUTU; k++) {\n            toplam += paylasimliA[threadIdx.y][k] * paylasimliB[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    C[satir * N + sutun] = toplam;\n}\n</code></pre>"},{"location":"tr/week-11/cen310-week-11/#3-is-parcacg-senkronizasyonu","title":"3. \u0130\u015f Par\u00e7ac\u0131\u011f\u0131 Senkronizasyonu","text":""},{"location":"tr/week-11/cen310-week-11/#senkronizasyon-yontemleri","title":"Senkronizasyon Y\u00f6ntemleri","text":"<ul> <li>Blok seviyesi senkronizasyon</li> <li>Izgara seviyesi senkronizasyon</li> <li>Atomik i\u015flemler</li> </ul>"},{"location":"tr/week-11/cen310-week-11/#ornek-atomik-islemler","title":"\u00d6rnek: Atomik \u0130\u015flemler","text":"<pre><code>__global__ void histogram(int* veri, int* hist, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        atomicAdd(&amp;hist[veri[idx]], 1);\n    }\n}\n</code></pre>"},{"location":"tr/week-11/cen310-week-11/#4-performans-optimizasyonu","title":"4. Performans Optimizasyonu","text":""},{"location":"tr/week-11/cen310-week-11/#optimizasyon-teknikleri","title":"Optimizasyon Teknikleri","text":"<ol> <li>Bellek Birle\u015ftirme</li> <li>Bank \u00c7ak\u0131\u015fmas\u0131 \u00d6nleme</li> <li>Doluluk Optimizasyonu</li> <li>D\u00f6ng\u00fc A\u00e7ma</li> </ol>"},{"location":"tr/week-11/cen310-week-11/#ornek-bank-caksmas-cozumu","title":"\u00d6rnek: Bank \u00c7ak\u0131\u015fmas\u0131 \u00c7\u00f6z\u00fcm\u00fc","text":"<pre><code>// K\u00f6t\u00fc: Bank \u00e7ak\u0131\u015fmalar\u0131\n__shared__ float paylasimli_veri[BLOK_BOYUTU][BLOK_BOYUTU];\n\n// \u0130yi: Bank \u00e7ak\u0131\u015fmalar\u0131n\u0131 \u00f6nlemek i\u00e7in dolgulu\n__shared__ float paylasimli_veri[BLOK_BOYUTU][BLOK_BOYUTU + 1];\n</code></pre>"},{"location":"tr/week-11/cen310-week-11/#ileri-bellek-yonetimi","title":"\u0130leri Bellek Y\u00f6netimi","text":""},{"location":"tr/week-11/cen310-week-11/#birlesik-bellek","title":"Birle\u015fik Bellek","text":"<pre><code>// Birle\u015fik bellek ay\u0131r\nfloat* birlesik_veri;\ncudaMallocManaged(&amp;birlesik_veri, boyut);\n\n// Ana bilgisayar veya cihazdan eri\u015fim\n// A\u00e7\u0131k transfer gerekmiyor\ncekirdek&lt;&lt;&lt;izgara, blok&gt;&gt;&gt;(birlesik_veri);\n\n// Birle\u015fik belle\u011fi serbest b\u0131rak\ncudaFree(birlesik_veri);\n</code></pre>"},{"location":"tr/week-11/cen310-week-11/#aks-isleme","title":"Ak\u0131\u015f \u0130\u015fleme","text":""},{"location":"tr/week-11/cen310-week-11/#eszamanl-yurutme","title":"E\u015fzamanl\u0131 Y\u00fcr\u00fctme","text":"<pre><code>cudaStream_t akis1, akis2;\ncudaStreamCreate(&amp;akis1);\ncudaStreamCreate(&amp;akis2);\n\n// Farkl\u0131 ak\u0131\u015flarda asenkron i\u015flemler\ncekirdek1&lt;&lt;&lt;izgara, blok, 0, akis1&gt;&gt;&gt;(veri1);\ncekirdek2&lt;&lt;&lt;izgara, blok, 0, akis2&gt;&gt;&gt;(veri2);\n\ncudaStreamSynchronize(akis1);\ncudaStreamSynchronize(akis2);\n\ncudaStreamDestroy(akis1);\ncudaStreamDestroy(akis2);\n</code></pre>"},{"location":"tr/week-11/cen310-week-11/#dinamik-paralellik","title":"Dinamik Paralellik","text":""},{"location":"tr/week-11/cen310-week-11/#ic-ice-cekirdek-baslatma","title":"\u0130\u00e7 \u0130\u00e7e \u00c7ekirdek Ba\u015flatma","text":"<pre><code>__global__ void cocuk_cekirdek(float* veri) {\n    // \u00c7ocuk \u00e7ekirdek kodu\n}\n\n__global__ void ebeveyn_cekirdek(float* veri) {\n    if(threadIdx.x == 0) {\n        cocuk_cekirdek&lt;&lt;&lt;izgara, blok&gt;&gt;&gt;(veri);\n        cudaDeviceSynchronize();\n    }\n}\n</code></pre>"},{"location":"tr/week-11/cen310-week-11/#laboratuvar-alstrmas","title":"Laboratuvar Al\u0131\u015ft\u0131rmas\u0131","text":""},{"location":"tr/week-11/cen310-week-11/#gorevler","title":"G\u00f6revler","text":"<ol> <li>Payla\u015f\u0131ml\u0131 bellek ile matris \u00e7arp\u0131m\u0131 uygulama</li> <li>Global bellek versiyonu ile performans kar\u015f\u0131la\u015ft\u0131rmas\u0131</li> <li>Bellek eri\u015fim desenlerini analiz etme</li> <li>Farkl\u0131 GPU mimarileri i\u00e7in optimize etme</li> </ol>"},{"location":"tr/week-11/cen310-week-11/#performans-metrikleri","title":"Performans Metrikleri","text":"<ul> <li>\u00c7al\u0131\u015fma s\u00fcresi</li> <li>Bellek verimi</li> <li>Doluluk oran\u0131</li> <li>\u00d6nbellek isabet oran\u0131</li> </ul>"},{"location":"tr/week-11/cen310-week-11/#kaynaklar","title":"Kaynaklar","text":""},{"location":"tr/week-11/cen310-week-11/#dokumantasyon","title":"Dok\u00fcmantasyon","text":"<ul> <li>CUDA C++ Programlama K\u0131lavuzu</li> <li>CUDA En \u0130yi Uygulamalar K\u0131lavuzu</li> <li>GPU Hesaplama Webinarlar\u0131</li> </ul>"},{"location":"tr/week-11/cen310-week-11/#araclar","title":"Ara\u00e7lar","text":"<ul> <li>Nsight Compute</li> <li>CUDA Profilleyici</li> <li>Visual Studio GPU Hata Ay\u0131klay\u0131c\u0131</li> </ul>"},{"location":"tr/week-11/cen310-week-11/#sorular-ve-tartsma","title":"Sorular ve Tart\u0131\u015fma","text":""},{"location":"tr/week-12/cen310-week-12/","title":"CEN310 Paralel Programlama Hafta-12","text":""},{"location":"tr/week-12/cen310-week-12/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-12/cen310-week-12/#hafta-12-gercek-dunya-uygulamalar-i","title":"Hafta-12 (Ger\u00e7ek D\u00fcnya Uygulamalar\u0131 I)","text":""},{"location":"tr/week-12/cen310-week-12/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-12/cen310-week-12/#genel-baks","title":"Genel Bak\u0131\u015f","text":""},{"location":"tr/week-12/cen310-week-12/#konular","title":"Konular","text":"<ol> <li>Bilimsel Hesaplama Uygulamalar\u0131</li> <li>Veri \u0130\u015fleme Uygulamalar\u0131</li> <li>Performans Optimizasyonu</li> <li>\u00d6rnek \u00c7al\u0131\u015fmalar</li> </ol>"},{"location":"tr/week-12/cen310-week-12/#hedefler","title":"Hedefler","text":"<ul> <li>Paralel programlamay\u0131 ger\u00e7ek problemlere uygulama</li> <li>Bilimsel hesaplamalar\u0131 optimize etme</li> <li>B\u00fcy\u00fck veri k\u00fcmelerini verimli i\u015fleme</li> <li>Ger\u00e7ek d\u00fcnya performans\u0131n\u0131 analiz etme</li> </ul>"},{"location":"tr/week-12/cen310-week-12/#1-bilimsel-hesaplama-uygulamalar","title":"1. Bilimsel Hesaplama Uygulamalar\u0131","text":""},{"location":"tr/week-12/cen310-week-12/#n-cisim-simulasyonu","title":"N-Cisim Sim\u00fclasyonu","text":"<pre><code>__global__ void kuvvet_hesapla(float4* konum, float4* hiz, float4* kuvvetler, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        float4 benim_konum = konum[idx];\n        float4 kuvvet = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n\n        for(int j = 0; j &lt; n; j++) {\n            if(j != idx) {\n                float4 diger_konum = konum[j];\n                float3 r = make_float3(\n                    diger_konum.x - benim_konum.x,\n                    diger_konum.y - benim_konum.y,\n                    diger_konum.z - benim_konum.z\n                );\n                float uzaklik = sqrtf(r.x*r.x + r.y*r.y + r.z*r.z);\n                float f = (G * benim_konum.w * diger_konum.w) / (uzaklik * uzaklik);\n                kuvvet.x += f * r.x/uzaklik;\n                kuvvet.y += f * r.y/uzaklik;\n                kuvvet.z += f * r.z/uzaklik;\n            }\n        }\n        kuvvetler[idx] = kuvvet;\n    }\n}\n</code></pre>"},{"location":"tr/week-12/cen310-week-12/#2-veri-isleme-uygulamalar","title":"2. Veri \u0130\u015fleme Uygulamalar\u0131","text":""},{"location":"tr/week-12/cen310-week-12/#goruntu-isleme","title":"G\u00f6r\u00fcnt\u00fc \u0130\u015fleme","text":"<pre><code>__global__ void gaussian_bulanik(\n    unsigned char* girdi,\n    unsigned char* cikti,\n    int genislik,\n    int yukseklik,\n    float* cekirdek,\n    int cekirdek_boyutu\n) {\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if(x &lt; genislik &amp;&amp; y &lt; yukseklik) {\n        float toplam = 0.0f;\n        int c_yaricap = cekirdek_boyutu / 2;\n\n        for(int ky = -c_yaricap; ky &lt;= c_yaricap; ky++) {\n            for(int kx = -c_yaricap; kx &lt;= c_yaricap; kx++) {\n                int px = min(max(x + kx, 0), genislik - 1);\n                int py = min(max(y + ky, 0), yukseklik - 1);\n                float cekirdek_deger = cekirdek[(ky+c_yaricap)*cekirdek_boyutu + (kx+c_yaricap)];\n                toplam += girdi[py*genislik + px] * cekirdek_deger;\n            }\n        }\n\n        cikti[y*genislik + x] = (unsigned char)toplam;\n    }\n}\n</code></pre>"},{"location":"tr/week-12/cen310-week-12/#3-performans-optimizasyonu","title":"3. Performans Optimizasyonu","text":""},{"location":"tr/week-12/cen310-week-12/#bellek-erisim-optimizasyonu","title":"Bellek Eri\u015fim Optimizasyonu","text":"<pre><code>// Matris transpozunu optimize et\n__global__ void matris_transpoz(float* girdi, float* cikti, int genislik, int yukseklik) {\n    __shared__ float karo[BLOK_BOYUTU][BLOK_BOYUTU+1]; // Bank \u00e7ak\u0131\u015fmalar\u0131n\u0131 \u00f6nle\n\n    int x = blockIdx.x * blockDim.x + threadIdx.x;\n    int y = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if(x &lt; genislik &amp;&amp; y &lt; yukseklik) {\n        // Payla\u015f\u0131ml\u0131 belle\u011fe y\u00fckle\n        karo[threadIdx.y][threadIdx.x] = girdi[y*genislik + x];\n        __syncthreads();\n\n        // Transpoz indislerini hesapla\n        int yeni_x = blockIdx.y * blockDim.y + threadIdx.x;\n        int yeni_y = blockIdx.x * blockDim.x + threadIdx.y;\n\n        if(yeni_x &lt; yukseklik &amp;&amp; yeni_y &lt; genislik) {\n            cikti[yeni_y*yukseklik + yeni_x] = karo[threadIdx.x][threadIdx.y];\n        }\n    }\n}\n</code></pre>"},{"location":"tr/week-12/cen310-week-12/#4-ornek-calsmalar","title":"4. \u00d6rnek \u00c7al\u0131\u015fmalar","text":""},{"location":"tr/week-12/cen310-week-12/#monte-carlo-simulasyonu","title":"Monte Carlo Sim\u00fclasyonu","text":"<pre><code>__global__ void monte_carlo_pi(float* noktalar_x, float* noktalar_y, int* daire_icinde, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx &lt; n) {\n        float x = noktalar_x[idx];\n        float y = noktalar_y[idx];\n        float uzaklik = x*x + y*y;\n\n        if(uzaklik &lt;= 1.0f) {\n            atomicAdd(daire_icinde, 1);\n        }\n    }\n}\n\nint main() {\n    int n = 1000000;\n    float *h_x, *h_y, *d_x, *d_y;\n    int *h_icinde, *d_icinde;\n\n    // Bellek ay\u0131r ve ba\u015flat\n    // ... (bellek ay\u0131rma kodu)\n\n    // Rastgele noktalar \u00fcret\n    for(int i = 0; i &lt; n; i++) {\n        h_x[i] = (float)rand()/RAND_MAX;\n        h_y[i] = (float)rand()/RAND_MAX;\n    }\n\n    // Veriyi cihaza kopyala ve \u00e7ekirde\u011fi \u00e7al\u0131\u015ft\u0131r\n    // ... (CUDA bellek i\u015flemleri ve \u00e7ekirdek ba\u015flatma)\n\n    // Pi'yi hesapla\n    float pi = 4.0f * (*h_icinde) / (float)n;\n    printf(\"Hesaplanan Pi: %f\\n\", pi);\n\n    // Temizlik\n    // ... (bellek temizleme kodu)\n\n    return 0;\n}\n</code></pre>"},{"location":"tr/week-12/cen310-week-12/#laboratuvar-alstrmas","title":"Laboratuvar Al\u0131\u015ft\u0131rmas\u0131","text":""},{"location":"tr/week-12/cen310-week-12/#gorevler","title":"G\u00f6revler","text":"<ol> <li>N-cisim sim\u00fclasyonu uygulama</li> <li>G\u00f6r\u00fcnt\u00fc i\u015fleme \u00e7ekirde\u011fini optimize etme</li> <li>Monte Carlo sim\u00fclasyonu geli\u015ftirme</li> <li>CPU versiyonlar\u0131 ile performans kar\u015f\u0131la\u015ft\u0131rmas\u0131</li> </ol>"},{"location":"tr/week-12/cen310-week-12/#performans-analizi","title":"Performans Analizi","text":"<ul> <li>\u00c7al\u0131\u015fma s\u00fcresi</li> <li>Bellek bant geni\u015fli\u011fi</li> <li>GPU kullan\u0131m\u0131</li> <li>\u00d6l\u00e7ekleme davran\u0131\u015f\u0131</li> </ul>"},{"location":"tr/week-12/cen310-week-12/#kaynaklar","title":"Kaynaklar","text":""},{"location":"tr/week-12/cen310-week-12/#dokumantasyon","title":"Dok\u00fcmantasyon","text":"<ul> <li>CUDA \u00d6rnek Uygulamalar\u0131</li> <li>Bilimsel Hesaplama K\u00fct\u00fcphaneleri</li> <li>Performans Analiz Ara\u00e7lar\u0131</li> </ul>"},{"location":"tr/week-12/cen310-week-12/#araclar","title":"Ara\u00e7lar","text":"<ul> <li>NVIDIA G\u00f6rsel Profilleyici</li> <li>Paralel Hesaplama Ara\u00e7 Seti</li> <li>Performans K\u00fct\u00fcphaneleri</li> </ul>"},{"location":"tr/week-12/cen310-week-12/#sorular-ve-tartsma","title":"Sorular ve Tart\u0131\u015fma","text":""},{"location":"tr/week-13/cen310-week-13/","title":"CEN310 Paralel Programlama Hafta-13","text":""},{"location":"tr/week-13/cen310-week-13/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-13/cen310-week-13/#hafta-13-gercek-dunya-uygulamalar-ii","title":"Hafta-13 (Ger\u00e7ek D\u00fcnya Uygulamalar\u0131 II)","text":""},{"location":"tr/week-13/cen310-week-13/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-13/cen310-week-13/#genel-baks","title":"Genel Bak\u0131\u015f","text":""},{"location":"tr/week-13/cen310-week-13/#konular","title":"Konular","text":"<ol> <li>\u0130leri Paralel Desenler</li> <li>N-Cisim Sim\u00fclasyonlar\u0131</li> <li>Matris Hesaplamalar\u0131</li> <li>B\u00fcy\u00fck Veri \u0130\u015fleme</li> </ol>"},{"location":"tr/week-13/cen310-week-13/#hedefler","title":"Hedefler","text":"<ul> <li>Karma\u015f\u0131k paralel desenleri uygulamak</li> <li>Bilimsel sim\u00fclasyonlar\u0131 optimize etmek</li> <li>B\u00fcy\u00fck \u00f6l\u00e7ekli matris i\u015flemleri ger\u00e7ekle\u015ftirmek</li> <li>B\u00fcy\u00fck veriyi verimli i\u015flemek</li> </ul>"},{"location":"tr/week-13/cen310-week-13/#1-ileri-paralel-desenler","title":"1. \u0130leri Paralel Desenler","text":""},{"location":"tr/week-13/cen310-week-13/#boru-hatt-deseni","title":"Boru Hatt\u0131 Deseni","text":"<pre><code>template&lt;typename T&gt;\nclass ParalelBoruHatti {\nprivate:\n    std::vector&lt;std::thread&gt; asamalar;\n    std::vector&lt;std::queue&lt;T&gt;&gt; kuyruklar;\n    std::vector&lt;std::mutex&gt; muteksler;\n    std::vector&lt;std::condition_variable&gt; kosul_degiskenleri;\n    bool calisiyor;\n\npublic:\n    ParalelBoruHatti(int asama_sayisi) {\n        kuyruklar.resize(asama_sayisi - 1);\n        muteksler.resize(asama_sayisi - 1);\n        kosul_degiskenleri.resize(asama_sayisi - 1);\n        calisiyor = true;\n    }\n\n    void asama_ekle(std::function&lt;void(T&amp;)&gt; asama_fonk, int asama_id) {\n        asamalar.emplace_back([this, asama_fonk, asama_id]() {\n            while(calisiyor) {\n                T veri;\n                if(asama_id == 0) {\n                    // \u0130lk a\u015fama: veri \u00fcret\n                    veri = veri_uret();\n                } else {\n                    // \u00d6nceki a\u015famadan veri al\n                    std::unique_lock&lt;std::mutex&gt; kilit(muteksler[asama_id-1]);\n                    kosul_degiskenleri[asama_id-1].wait(kilit, \n                        [this, asama_id]() { \n                            return !kuyruklar[asama_id-1].empty() || !calisiyor; \n                        });\n                    if(!calisiyor) break;\n                    veri = kuyruklar[asama_id-1].front();\n                    kuyruklar[asama_id-1].pop();\n                    kilit.unlock();\n                    kosul_degiskenleri[asama_id-1].notify_one();\n                }\n\n                // Veriyi i\u015fle\n                asama_fonk(veri);\n\n                if(asama_id &lt; asamalar.size() - 1) {\n                    // Sonraki a\u015famaya ge\u00e7\n                    std::unique_lock&lt;std::mutex&gt; kilit(muteksler[asama_id]);\n                    kuyruklar[asama_id].push(veri);\n                    kilit.unlock();\n                    kosul_degiskenleri[asama_id].notify_one();\n                }\n            }\n        });\n    }\n\n    void baslat() {\n        for(auto&amp; asama : asamalar) {\n            asama.join();\n        }\n    }\n\n    void durdur() {\n        calisiyor = false;\n        for(auto&amp; kd : kosul_degiskenleri) {\n            kd.notify_all();\n        }\n    }\n};\n</code></pre>"},{"location":"tr/week-13/cen310-week-13/#2-n-cisim-simulasyonlar","title":"2. N-Cisim Sim\u00fclasyonlar\u0131","text":""},{"location":"tr/week-13/cen310-week-13/#barnes-hut-algoritmas","title":"Barnes-Hut Algoritmas\u0131","text":"<pre><code>struct Sekizli_Agac {\n    struct Dugum {\n        vec3 merkez;\n        float boyut;\n        float kutle;\n        vec3 kutle_merkezi;\n        std::vector&lt;Dugum*&gt; cocuklar;\n    };\n\n    Dugum* kok;\n    float teta;\n\n    __device__ void kuvvet_hesapla(vec3&amp; konum, vec3&amp; kuvvet, Dugum* dugum) {\n        vec3 fark = dugum-&gt;kutle_merkezi - konum;\n        float uzaklik = length(fark);\n\n        if(dugum-&gt;boyut / uzaklik &lt; teta || dugum-&gt;cocuklar.empty()) {\n            // Yakla\u015f\u0131k hesaplama kullan\n            float f = G * dugum-&gt;kutle / (uzaklik * uzaklik * uzaklik);\n            kuvvet += fark * f;\n        } else {\n            // \u00c7ocuklara \u00f6zyinele\n            for(auto cocuk : dugum-&gt;cocuklar) {\n                if(cocuk != nullptr) {\n                    kuvvet_hesapla(konum, kuvvet, cocuk);\n                }\n            }\n        }\n    }\n\n    __global__ void cisimleri_guncelle(vec3* konum, vec3* hiz, vec3* ivme, \n                                     float dt, int n) {\n        int idx = blockIdx.x * blockDim.x + threadIdx.x;\n        if(idx &lt; n) {\n            vec3 kuvvet(0.0f);\n            kuvvet_hesapla(konum[idx], kuvvet, kok);\n            ivme[idx] = kuvvet;\n            hiz[idx] += ivme[idx] * dt;\n            konum[idx] += hiz[idx] * dt;\n        }\n    }\n};\n</code></pre>"},{"location":"tr/week-13/cen310-week-13/#3-matris-hesaplamalar","title":"3. Matris Hesaplamalar\u0131","text":""},{"location":"tr/week-13/cen310-week-13/#paralel-matris-ayrstrma","title":"Paralel Matris Ayr\u0131\u015ft\u0131rma","text":"<pre><code>__global__ void lu_ayristirma(float* A, int n, int k) {\n    int satir = blockIdx.y * blockDim.y + threadIdx.y;\n    int sutun = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if(satir &gt; k &amp;&amp; satir &lt; n &amp;&amp; sutun &gt; k &amp;&amp; sutun &lt; n) {\n        A[satir * n + sutun] -= A[satir * n + k] * A[k * n + sutun] / A[k * n + k];\n    }\n}\n\nvoid paralel_lu(float* A, int n) {\n    dim3 blok(16, 16);\n    dim3 izgara((n + blok.x - 1) / blok.x, \n                (n + blok.y - 1) / blok.y);\n\n    for(int k = 0; k &lt; n-1; k++) {\n        lu_ayristirma&lt;&lt;&lt;izgara, blok&gt;&gt;&gt;(A, n, k);\n        cudaDeviceSynchronize();\n    }\n}\n</code></pre>"},{"location":"tr/week-13/cen310-week-13/#4-buyuk-veri-isleme","title":"4. B\u00fcy\u00fck Veri \u0130\u015fleme","text":""},{"location":"tr/week-13/cen310-week-13/#paralel-veri-analizi","title":"Paralel Veri Analizi","text":"<pre><code>template&lt;typename T&gt;\nclass ParalelVeriIsleyici {\nprivate:\n    std::vector&lt;T&gt; veri;\n    int is_parcacigi_sayisi;\n\npublic:\n    ParalelVeriIsleyici(const std::vector&lt;T&gt;&amp; girdi, int is_parcaciklari) \n        : veri(girdi), is_parcacigi_sayisi(is_parcaciklari) {}\n\n    template&lt;typename Fonk&gt;\n    std::vector&lt;T&gt; haritalama(Fonk f) {\n        std::vector&lt;T&gt; sonuc(veri.size());\n        #pragma omp parallel for num_threads(is_parcacigi_sayisi)\n        for(size_t i = 0; i &lt; veri.size(); i++) {\n            sonuc[i] = f(veri[i]);\n        }\n        return sonuc;\n    }\n\n    template&lt;typename Fonk&gt;\n    T indirgeme(Fonk f, T baslangic) {\n        T sonuc = baslangic;\n        #pragma omp parallel num_threads(is_parcacigi_sayisi)\n        {\n            T yerel_toplam = baslangic;\n            #pragma omp for nowait\n            for(size_t i = 0; i &lt; veri.size(); i++) {\n                yerel_toplam = f(yerel_toplam, veri[i]);\n            }\n            #pragma omp critical\n            {\n                sonuc = f(sonuc, yerel_toplam);\n            }\n        }\n        return sonuc;\n    }\n};\n</code></pre>"},{"location":"tr/week-13/cen310-week-13/#laboratuvar-alstrmas","title":"Laboratuvar Al\u0131\u015ft\u0131rmas\u0131","text":""},{"location":"tr/week-13/cen310-week-13/#gorevler","title":"G\u00f6revler","text":"<ol> <li>Barnes-Hut sim\u00fclasyonu uygulama</li> <li>Paralel LU ayr\u0131\u015ft\u0131rma geli\u015ftirme</li> <li>B\u00fcy\u00fck veri i\u015fleme boru hatt\u0131 olu\u015fturma</li> <li>Performans \u00f6zelliklerini analiz etme</li> </ol>"},{"location":"tr/week-13/cen310-week-13/#performans-analizi","title":"Performans Analizi","text":"<ul> <li>Algoritma karma\u015f\u0131kl\u0131\u011f\u0131</li> <li>Bellek eri\u015fim desenleri</li> <li>Y\u00fck dengeleme</li> <li>\u00d6l\u00e7eklenebilirlik testi</li> </ul>"},{"location":"tr/week-13/cen310-week-13/#kaynaklar","title":"Kaynaklar","text":""},{"location":"tr/week-13/cen310-week-13/#dokumantasyon","title":"Dok\u00fcmantasyon","text":"<ul> <li>\u0130leri CUDA Programlama K\u0131lavuzu</li> <li>Paralel Algoritmalar Referans\u0131</li> <li>Bilimsel Hesaplama K\u00fct\u00fcphaneleri</li> </ul>"},{"location":"tr/week-13/cen310-week-13/#araclar","title":"Ara\u00e7lar","text":"<ul> <li>Performans Profilleyiciler</li> <li>Hata Ay\u0131klama Ara\u00e7lar\u0131</li> <li>Analiz \u00c7er\u00e7eveleri</li> </ul>"},{"location":"tr/week-13/cen310-week-13/#sorular-ve-tartsma","title":"Sorular ve Tart\u0131\u015fma","text":""},{"location":"tr/week-14/cen310-week-14/","title":"CEN310 Paralel Programlama Hafta-14","text":""},{"location":"tr/week-14/cen310-week-14/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-14/cen310-week-14/#hafta-14-quiz-2","title":"Hafta-14 (Quiz-2)","text":""},{"location":"tr/week-14/cen310-week-14/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-14/cen310-week-14/#quiz-2-bilgileri","title":"Quiz-2 Bilgileri","text":""},{"location":"tr/week-14/cen310-week-14/#tarih-ve-saat","title":"Tarih ve Saat","text":"<ul> <li>Tarih: 16 May\u0131s 2025</li> <li>Saat: 09:00-12:00 (3 saat)</li> <li>Konum: Normal s\u0131n\u0131f</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#format","title":"Format","text":"<ul> <li>Yaz\u0131l\u0131 s\u0131nav</li> <li>Teorik ve pratik sorular\u0131n kar\u0131\u015f\u0131m\u0131</li> <li>Hem kapal\u0131 hem a\u00e7\u0131k u\u00e7lu sorular</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#kapsanan-konular","title":"Kapsanan Konular","text":""},{"location":"tr/week-14/cen310-week-14/#1-gpu-programlama","title":"1. GPU Programlama","text":"<ul> <li>CUDA Mimarisi</li> <li>Bellek Hiyerar\u015fisi</li> <li>\u0130\u015f Par\u00e7ac\u0131\u011f\u0131 Organizasyonu</li> <li>Performans Optimizasyonu</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#2-ileri-paralel-desenler","title":"2. \u0130leri Paralel Desenler","text":"<ul> <li>Boru Hatt\u0131 \u0130\u015fleme</li> <li>G\u00f6rev Paralelli\u011fi</li> <li>Veri Paralelli\u011fi</li> <li>Hibrit Yakla\u015f\u0131mlar</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#3-gercek-dunya-uygulamalar","title":"3. Ger\u00e7ek D\u00fcnya Uygulamalar\u0131","text":"<ul> <li>Bilimsel Hesaplama</li> <li>Veri \u0130\u015fleme</li> <li>Matris \u0130\u015flemleri</li> <li>N-cisim Sim\u00fclasyonlar\u0131</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#ornek-sorular","title":"\u00d6rnek Sorular","text":""},{"location":"tr/week-14/cen310-week-14/#teorik-sorular","title":"Teorik Sorular","text":"<ol> <li>CUDA bellek hiyerar\u015fisini ve performansa etkisini a\u00e7\u0131klay\u0131n.</li> <li>Farkl\u0131 paralel desenleri ve kullan\u0131m durumlar\u0131n\u0131 kar\u015f\u0131la\u015ft\u0131r\u0131n.</li> <li>GPU programlar\u0131 i\u00e7in optimizasyon stratejilerini tan\u0131mlay\u0131n.</li> </ol>"},{"location":"tr/week-14/cen310-week-14/#pratik-problemler","title":"Pratik Problemler","text":"<pre><code>// Soru 1: Bu CUDA program\u0131n\u0131n \u00e7\u0131kt\u0131s\u0131 nedir?\n__global__ void cekirdek(int* veri) {\n    int idx = threadIdx.x;\n    __shared__ int paylasimli_veri[256];\n\n    paylasimli_veri[idx] = veri[idx];\n    __syncthreads();\n\n    if(idx &lt; 128) {\n        paylasimli_veri[idx] += paylasimli_veri[idx + 128];\n    }\n    __syncthreads();\n\n    if(idx == 0) {\n        veri[0] = paylasimli_veri[0];\n    }\n}\n\nint main() {\n    int* veri;\n    // ... ba\u015flatma kodu ...\n    cekirdek&lt;&lt;&lt;1, 256&gt;&gt;&gt;(veri);\n    // ... temizleme kodu ...\n}\n</code></pre>"},{"location":"tr/week-14/cen310-week-14/#hazrlk-yonergeleri","title":"Haz\u0131rl\u0131k Y\u00f6nergeleri","text":""},{"location":"tr/week-14/cen310-week-14/#1-incelenecek-materyaller","title":"1. \u0130ncelenecek Materyaller","text":"<ul> <li>Ders slaytlar\u0131 ve notlar\u0131</li> <li>Laboratuvar al\u0131\u015ft\u0131rmalar\u0131</li> <li>\u00d6rnek kodlar</li> <li>Pratik problemler</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#2-odak-alanlar","title":"2. Odak Alanlar\u0131","text":"<ul> <li>CUDA Programlama</li> <li>Bellek Y\u00f6netimi</li> <li>Performans Optimizasyonu</li> <li>Ger\u00e7ek D\u00fcnya Uygulamalar\u0131</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#3-pratik-alstrmalar","title":"3. Pratik Al\u0131\u015ft\u0131rmalar","text":"<ul> <li>CUDA programlar\u0131 yazma ve analiz etme</li> <li>Paralel desenleri uygulama</li> <li>Mevcut kodu optimize etme</li> <li>Performans \u00f6l\u00e7\u00fcm\u00fc</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#snav-kurallar","title":"S\u0131nav Kurallar\u0131","text":"<ol> <li>\u0130zin Verilen Materyaller</li> <li>Kitap veya not kullan\u0131m\u0131 yasak</li> <li>Elektronik cihaz kullan\u0131m\u0131 yasak</li> <li> <p>M\u00fcsvedde i\u00e7in temiz ka\u011f\u0131t</p> </li> <li> <p>Zaman Y\u00f6netimi</p> </li> <li>T\u00fcm sorular\u0131 dikkatlice okuyun</li> <li>Her b\u00f6l\u00fcm i\u00e7in zaman\u0131n\u0131z\u0131 planlay\u0131n</li> <li> <p>\u0130nceleme i\u00e7in zaman b\u0131rak\u0131n</p> </li> <li> <p>Sorular\u0131 Yan\u0131tlama</p> </li> <li>T\u00fcm \u00e7al\u0131\u015fman\u0131z\u0131 g\u00f6sterin</li> <li>Mant\u0131\u011f\u0131n\u0131z\u0131 a\u00e7\u0131klay\u0131n</li> <li>A\u00e7\u0131k ve d\u00fczenli yaz\u0131n</li> </ol>"},{"location":"tr/week-14/cen310-week-14/#degerlendirme-kriterleri","title":"De\u011ferlendirme Kriterleri","text":""},{"location":"tr/week-14/cen310-week-14/#daglm","title":"Da\u011f\u0131l\u0131m","text":"<ul> <li>Teorik Sorular: 40%</li> <li>Pratik Problemler: 60%</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#degerlendirme","title":"De\u011ferlendirme","text":"<ul> <li>Kavramlar\u0131 anlama</li> <li>Problem \u00e7\u00f6zme yakla\u015f\u0131m\u0131</li> <li>Kod analizi ve yaz\u0131m\u0131</li> <li>Performans de\u011ferlendirmeleri</li> <li>A\u00e7\u0131k a\u00e7\u0131klamalar</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#ek-kaynaklar","title":"Ek Kaynaklar","text":""},{"location":"tr/week-14/cen310-week-14/#inceleme-materyalleri","title":"\u0130nceleme Materyalleri","text":"<ul> <li>CUDA Programlama K\u0131lavuzu</li> <li>Performans Optimizasyon K\u0131lavuzu</li> <li>\u00d6rnek Uygulamalar</li> <li>\u00c7evrimi\u00e7i Dok\u00fcmantasyon:</li> <li>CUDA Dok\u00fcmantasyonu</li> <li>OpenMP Referans\u0131</li> <li>MPI Dok\u00fcmantasyonu</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#ornek-kod-deposu","title":"\u00d6rnek Kod Deposu","text":"<ul> <li>Ders GitHub deposu</li> <li>\u00d6rnek uygulamalar</li> <li>Performans k\u0131yaslamalar\u0131</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#iletisim-bilgileri","title":"\u0130leti\u015fim Bilgileri","text":"<p>S\u0131nav ile ilgili sorular\u0131n\u0131z i\u00e7in:</p> <ul> <li>E-posta: ugur.coruh@erdogan.edu.tr</li> <li>Ofis Saatleri: Randevu ile</li> <li>Konum: M\u00fchendislik Fak\u00fcltesi</li> </ul>"},{"location":"tr/week-14/cen310-week-14/#basarlar","title":"Ba\u015far\u0131lar!","text":""},{"location":"tr/week-15/cen310-week-15/","title":"CEN310 Paralel Programlama Hafta-15","text":""},{"location":"tr/week-15/cen310-week-15/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-15/cen310-week-15/#hafta-15-final-proje-degerlendirmesi","title":"Hafta-15 (Final Proje De\u011ferlendirmesi)","text":""},{"location":"tr/week-15/cen310-week-15/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-15/cen310-week-15/#proje-degerlendirme-gunu-program","title":"Proje De\u011ferlendirme G\u00fcn\u00fc Program\u0131","text":""},{"location":"tr/week-15/cen310-week-15/#sabah-oturumu-0900-1200","title":"Sabah Oturumu (09:00-12:00)","text":"<ul> <li>Proje sunumlar\u0131 (Grup 1-4)</li> <li>Performans analizi tart\u0131\u015fmalar\u0131</li> <li>Soru-cevap oturumlar\u0131</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#ogle-aras-1200-1300","title":"\u00d6\u011fle Aras\u0131 (12:00-13:00)","text":""},{"location":"tr/week-15/cen310-week-15/#ogleden-sonra-oturumu-1300-1700","title":"\u00d6\u011fleden Sonra Oturumu (13:00-17:00)","text":"<ul> <li>Proje sunumlar\u0131 (Grup 5-8)</li> <li>Teknik g\u00f6sterimler</li> <li>Son de\u011ferlendirmeler</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#final-proje-gereksinimleri","title":"Final Proje Gereksinimleri","text":""},{"location":"tr/week-15/cen310-week-15/#1-proje-dokumantasyonu","title":"1. Proje Dok\u00fcmantasyonu","text":"<ul> <li>Kapsaml\u0131 proje raporu</li> <li>Kaynak kod dok\u00fcmantasyonu</li> <li>Performans analizi sonu\u00e7lar\u0131</li> <li>Uygulama detaylar\u0131</li> <li>Gelecek \u00e7al\u0131\u015fma \u00f6nerileri</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#2-teknik-uygulama","title":"2. Teknik Uygulama","text":"<ul> <li>\u00c7al\u0131\u015fan paralel uygulama</li> <li>\u00c7oklu paralel programlama modelleri</li> <li>\u0130leri optimizasyon teknikleri</li> <li>Hata y\u00f6netimi ve sa\u011flaml\u0131k</li> <li>Kod kalitesi ve organizasyonu</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#sunum-yonergeleri","title":"Sunum Y\u00f6nergeleri","text":""},{"location":"tr/week-15/cen310-week-15/#format","title":"Format","text":"<ul> <li>Grup ba\u015f\u0131na 30 dakika</li> <li>20 dakika sunum</li> <li>10 dakika soru-cevap</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#icerik","title":"\u0130\u00e7erik","text":"<ol> <li>Proje Genel Bak\u0131\u015f\u0131</li> <li>Problem tan\u0131m\u0131</li> <li>\u00c7\u00f6z\u00fcm yakla\u015f\u0131m\u0131</li> <li> <p>Teknik zorluklar</p> </li> <li> <p>Uygulama Detaylar\u0131</p> </li> <li>Mimari tasar\u0131m</li> <li>Paralel stratejiler</li> <li> <p>Optimizasyon teknikleri</p> </li> <li> <p>Sonu\u00e7lar ve Analiz</p> </li> <li>Performans \u00f6l\u00e7\u00fcmleri</li> <li>\u00d6l\u00e7eklenebilirlik testleri</li> <li> <p>Kar\u015f\u0131la\u015ft\u0131rmal\u0131 analiz</p> </li> <li> <p>Canl\u0131 Demo</p> </li> <li>Sistem kurulumu</li> <li>\u00d6zellik g\u00f6sterimi</li> <li>Performans sunumu</li> </ol>"},{"location":"tr/week-15/cen310-week-15/#performans-analizi-gereksinimleri","title":"Performans Analizi Gereksinimleri","text":""},{"location":"tr/week-15/cen310-week-15/#olculecek-metrikler","title":"\u00d6l\u00e7\u00fclecek Metrikler","text":"<ul> <li>\u00c7al\u0131\u015fma s\u00fcresi</li> <li>H\u0131zlanma</li> <li>Verimlilik</li> <li>Kaynak kullan\u0131m\u0131</li> <li>\u00d6l\u00e7eklenebilirlik</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#analiz-araclar","title":"Analiz Ara\u00e7lar\u0131","text":"<pre><code># Performans \u00f6l\u00e7\u00fcm \u00f6rnekleri\n$ nvprof ./cuda_programi\n$ mpirun -np 4 ./mpi_programi\n$ perf stat ./openmp_programi\n</code></pre>"},{"location":"tr/week-15/cen310-week-15/#proje-yaps-ornegi","title":"Proje Yap\u0131s\u0131 \u00d6rne\u011fi","text":"<pre><code>proje/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.cpp\n\u2502   \u251c\u2500\u2500 cuda/\n\u2502   \u2502   \u251c\u2500\u2500 cekirdek.cu\n\u2502   \u2502   \u2514\u2500\u2500 gpu_yardimcilar.cuh\n\u2502   \u251c\u2500\u2500 mpi/\n\u2502   \u2502   \u251c\u2500\u2500 iletisimci.cpp\n\u2502   \u2502   \u2514\u2500\u2500 veri_transfer.h\n\u2502   \u2514\u2500\u2500 openmp/\n\u2502       \u251c\u2500\u2500 paralel_donguler.cpp\n\u2502       \u2514\u2500\u2500 is_parcacigi_yardimcilar.h\n\u251c\u2500\u2500 include/\n\u2502   \u251c\u2500\u2500 ortak.h\n\u2502   \u2514\u2500\u2500 yapilandirma.h\n\u251c\u2500\u2500 test/\n\u2502   \u251c\u2500\u2500 birim_testler.cpp\n\u2502   \u2514\u2500\u2500 performans_testleri.cpp\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 rapor.pdf\n\u2502   \u2514\u2500\u2500 sunum.pptx\n\u251c\u2500\u2500 veri/\n\u2502   \u251c\u2500\u2500 girdi/\n\u2502   \u2514\u2500\u2500 cikti/\n\u251c\u2500\u2500 betikler/\n\u2502   \u251c\u2500\u2500 derle.sh\n\u2502   \u2514\u2500\u2500 testleri_calistir.sh\n\u251c\u2500\u2500 CMakeLists.txt\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"tr/week-15/cen310-week-15/#degerlendirme-kriterleri","title":"De\u011ferlendirme Kriterleri","text":""},{"location":"tr/week-15/cen310-week-15/#teknik-yonler-50","title":"Teknik Y\u00f6nler (50%)","text":"<ul> <li>Uygulama kalitesi (15%)</li> <li>Performans optimizasyonu (15%)</li> <li>Kod organizasyonu (10%)</li> <li>Hata y\u00f6netimi (10%)</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#dokumantasyon-25","title":"Dok\u00fcmantasyon (25%)","text":"<ul> <li>Proje raporu (10%)</li> <li>Kod dok\u00fcmantasyonu (10%)</li> <li>Sunum kalitesi (5%)</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#sonuclar-ve-analiz-25","title":"Sonu\u00e7lar ve Analiz (25%)","text":"<ul> <li>Performans sonu\u00e7lar\u0131 (10%)</li> <li>Kar\u015f\u0131la\u015ft\u0131rmal\u0131 analiz (10%)</li> <li>Gelecek iyile\u015ftirmeler (5%)</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#yaygn-proje-konular","title":"Yayg\u0131n Proje Konular\u0131","text":"<ol> <li>Bilimsel Hesaplama</li> <li>N-cisim sim\u00fclasyonlar\u0131</li> <li>Ak\u0131\u015fkanlar dinami\u011fi</li> <li>Monte Carlo y\u00f6ntemleri</li> <li> <p>Matris hesaplamalar\u0131</p> </li> <li> <p>Veri \u0130\u015fleme</p> </li> <li>G\u00f6r\u00fcnt\u00fc/video i\u015fleme</li> <li>Sinyal i\u015fleme</li> <li>Veri madencili\u011fi</li> <li> <p>\u00d6r\u00fcnt\u00fc tan\u0131ma</p> </li> <li> <p>Makine \u00d6\u011frenmesi</p> </li> <li>Sinir a\u011f\u0131 e\u011fitimi</li> <li>Paralel model \u00e7\u0131kar\u0131m\u0131</li> <li>Veri \u00f6n i\u015fleme</li> <li> <p>\u00d6zellik \u00e7\u0131kar\u0131m\u0131</p> </li> <li> <p>Graf \u0130\u015fleme</p> </li> <li>Yol bulma</li> <li>Graf analiti\u011fi</li> <li>A\u011f analizi</li> <li>A\u011fa\u00e7 algoritmalar\u0131</li> </ol>"},{"location":"tr/week-15/cen310-week-15/#kaynaklar-ve-referanslar","title":"Kaynaklar ve Referanslar","text":""},{"location":"tr/week-15/cen310-week-15/#dokumantasyon","title":"Dok\u00fcmantasyon","text":"<ul> <li>CUDA Programlama K\u0131lavuzu</li> <li>OpenMP API Spesifikasyonu</li> <li>MPI Standart Dok\u00fcmantasyonu</li> <li>Performans Optimizasyon K\u0131lavuzlar\u0131</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#araclar","title":"Ara\u00e7lar","text":"<ul> <li>Visual Studio</li> <li>NVIDIA NSight</li> <li>Intel VTune</li> <li>Performans Profilleyiciler</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#proje-raporu-sablonu","title":"Proje Raporu \u015eablonu","text":""},{"location":"tr/week-15/cen310-week-15/#1-giris","title":"1. Giri\u015f","text":"<ul> <li>Arka plan</li> <li>Hedefler</li> <li>Kapsam</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#2-tasarm","title":"2. Tasar\u0131m","text":"<ul> <li>Sistem mimarisi</li> <li>Bile\u015fen tasar\u0131m\u0131</li> <li>Paralel stratejiler</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#3-uygulama","title":"3. Uygulama","text":"<ul> <li>Geli\u015ftirme ortam\u0131</li> <li>Teknik detaylar</li> <li>Optimizasyon teknikleri</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#4-sonuclar","title":"4. Sonu\u00e7lar","text":"<ul> <li>Performans \u00f6l\u00e7\u00fcmleri</li> <li>Analiz</li> <li>Kar\u015f\u0131la\u015ft\u0131rmalar</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#5-sonuc","title":"5. Sonu\u00e7","text":"<ul> <li>Ba\u015far\u0131lar</li> <li>Zorluklar</li> <li>Gelecek \u00e7al\u0131\u015fmalar</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#iletisim-bilgileri","title":"\u0130leti\u015fim Bilgileri","text":"<p>Proje ile ilgili sorular\u0131n\u0131z i\u00e7in:</p> <ul> <li>E-posta: ugur.coruh@erdogan.edu.tr</li> <li>Ofis Saatleri: Randevu ile</li> <li>Konum: M\u00fchendislik Fak\u00fcltesi</li> </ul>"},{"location":"tr/week-15/cen310-week-15/#sorular-ve-tartsma","title":"Sorular ve Tart\u0131\u015fma","text":""},{"location":"tr/week-16-final/cen310-week-16/","title":"CEN310 Paralel Programlama Hafta-16","text":""},{"location":"tr/week-16-final/cen310-week-16/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-16-final/cen310-week-16/#hafta-16-final-snav-donemi","title":"Hafta-16 (Final S\u0131nav D\u00f6nemi)","text":""},{"location":"tr/week-16-final/cen310-week-16/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-16-final/cen310-week-16/#final-snav-donemi-bilgileri","title":"Final S\u0131nav D\u00f6nemi Bilgileri","text":""},{"location":"tr/week-16-final/cen310-week-16/#tarihler","title":"Tarihler","text":"<ul> <li>D\u00f6nem: 24 May\u0131s - 4 Haziran 2025</li> <li>Proje Raporu Teslimi: \u00dcniversite taraf\u0131ndan belirlenen tarihte</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#konum","title":"Konum","text":"<ul> <li>\u00dcniversite taraf\u0131ndan atanan yerde</li> <li>Resmi s\u0131nav program\u0131n\u0131 kontrol edin</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#final-proje-raporu-gereksinimleri","title":"Final Proje Raporu Gereksinimleri","text":""},{"location":"tr/week-16-final/cen310-week-16/#1-proje-dokumantasyonu","title":"1. Proje Dok\u00fcmantasyonu","text":"<ul> <li>Eksiksiz proje raporu</li> <li>Dok\u00fcmantasyonlu kaynak kod</li> <li>Performans analizi sonu\u00e7lar\u0131</li> <li>Uygulama detaylar\u0131</li> <li>Gelecek \u00e7al\u0131\u015fma \u00f6nerileri</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#2-teknik-gereksinimler","title":"2. Teknik Gereksinimler","text":"<ul> <li>Kod kalitesi ve organizasyonu</li> <li>Performans optimizasyon sonu\u00e7lar\u0131</li> <li>S\u0131ral\u0131 versiyon ile kar\u015f\u0131la\u015ft\u0131rma</li> <li>\u00d6l\u00e7eklenebilirlik analizi</li> <li>Hata y\u00f6netimi uygulamas\u0131</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#rapor-yaps","title":"Rapor Yap\u0131s\u0131","text":""},{"location":"tr/week-16-final/cen310-week-16/#1-yonetici-ozeti","title":"1. Y\u00f6netici \u00d6zeti","text":"<ul> <li>Proje genel bak\u0131\u015f\u0131</li> <li>Temel ba\u015far\u0131lar</li> <li>Performans \u00f6ne \u00e7\u0131kanlar\u0131</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#2-teknik-uygulama","title":"2. Teknik Uygulama","text":"<ul> <li>Mimari detaylar</li> <li>Algoritma a\u00e7\u0131klamalar\u0131</li> <li>Paralelle\u015ftirme stratejisi</li> <li>Kod yap\u0131s\u0131</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#3-performans-analizi","title":"3. Performans Analizi","text":"<ul> <li>K\u0131yaslama sonu\u00e7lar\u0131</li> <li>\u00d6l\u00e7eklenebilirlik testleri</li> <li>Kaynak kullan\u0131m\u0131</li> <li>Optimizasyon \u00e7al\u0131\u015fmalar\u0131</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#4-sonuclar","title":"4. Sonu\u00e7lar","text":"<ul> <li>\u00d6\u011frenilen dersler</li> <li>A\u015f\u0131lan zorluklar</li> <li>Gelecek iyile\u015ftirmeler</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#teslim-yonergeleri","title":"Teslim Y\u00f6nergeleri","text":""},{"location":"tr/week-16-final/cen310-week-16/#format-gereksinimleri","title":"Format Gereksinimleri","text":"<ul> <li>PDF format\u0131</li> <li>Profesyonel formatlama</li> <li>A\u00e7\u0131k kod listeleri</li> <li>Uygun al\u0131nt\u0131lar</li> <li>Performans grafikleri</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#teslim-sureci","title":"Teslim S\u00fcreci","text":"<ul> <li>Dijital teslim</li> <li>Kaynak kod deposu</li> <li>Dok\u00fcmantasyon paketi</li> <li>Sunum slaytlar\u0131</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#degerlendirme-kriterleri","title":"De\u011ferlendirme Kriterleri","text":""},{"location":"tr/week-16-final/cen310-week-16/#teknik-derinlik-40","title":"Teknik Derinlik (40%)","text":"<ul> <li>Uygulama kalitesi</li> <li>Performans optimizasyonu</li> <li>Kod organizasyonu</li> <li>Dok\u00fcmantasyon kalitesi</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#analiz-ve-sonuclar-40","title":"Analiz ve Sonu\u00e7lar (40%)","text":"<ul> <li>Performans \u00f6l\u00e7\u00fcmleri</li> <li>\u00d6l\u00e7eklenebilirlik analizi</li> <li>Kar\u015f\u0131la\u015ft\u0131rmal\u0131 de\u011ferlendirme</li> <li>Problem \u00e7\u00f6zme yakla\u015f\u0131m\u0131</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#dokumantasyon-20","title":"Dok\u00fcmantasyon (20%)","text":"<ul> <li>Rapor kalitesi</li> <li>Kod dok\u00fcmantasyonu</li> <li>Sunum materyalleri</li> <li>Gelecek \u00f6nerileri</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#onemli-notlar","title":"\u00d6nemli Notlar","text":""},{"location":"tr/week-16-final/cen310-week-16/#teslim-tarihleri","title":"Teslim Tarihleri","text":"<ul> <li>Rapor teslim tarihi kesindir</li> <li>Ge\u00e7 teslimler kabul edilmeyebilir</li> <li>Uzatmalar \u00f6nceden onay gerektirir</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#akademik-durustluk","title":"Akademik D\u00fcr\u00fcstl\u00fck","text":"<ul> <li>\u00d6zg\u00fcn \u00e7al\u0131\u015fma gereklidir</li> <li>Uygun al\u0131nt\u0131lar gereklidir</li> <li>Kod intihal kontrol\u00fc yap\u0131l\u0131r</li> <li>\u0130\u015fbirli\u011fi belirtilmelidir</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#ders-tamamlama-gereksinimleri","title":"Ders Tamamlama Gereksinimleri","text":""},{"location":"tr/week-16-final/cen310-week-16/#minimum-gereksinimler","title":"Minimum Gereksinimler","text":"<ul> <li>Proje raporu teslimi</li> <li>Dok\u00fcmantasyonlu kod deposu</li> <li>Performans analizi sonu\u00e7lar\u0131</li> <li>Uygulama g\u00f6sterimi</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#not-bilesenleri","title":"Not Bile\u015fenleri","text":"<ul> <li>Quiz-1: 15%</li> <li>Vize: 35%</li> <li>Quiz-2: 15%</li> <li>Final Projesi: 35%</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#iletisim-bilgileri","title":"\u0130leti\u015fim Bilgileri","text":"<p>S\u0131nav ile ilgili sorular\u0131n\u0131z i\u00e7in:</p> <ul> <li>E-posta: ugur.coruh@erdogan.edu.tr</li> <li>Ofis Saatleri: Randevu ile</li> <li>Konum: M\u00fchendislik Fak\u00fcltesi</li> </ul>"},{"location":"tr/week-16-final/cen310-week-16/#final-snavnzda-basarlar","title":"Final S\u0131nav\u0131n\u0131zda Ba\u015far\u0131lar!","text":""},{"location":"tr/week-3/cen310-week-3/","title":"CEN310 Paralel Programlama Hafta-3","text":""},{"location":"tr/week-3/cen310-week-3/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-3/cen310-week-3/#hafta-3","title":"Hafta-3","text":""},{"location":"tr/week-3/cen310-week-3/#openmp-programlama","title":"OpenMP Programlama","text":""},{"location":"tr/week-3/cen310-week-3/#ders-icerigi","title":"Ders \u0130\u00e7eri\u011fi","text":"<ol> <li>OpenMP'ye Giri\u015f</li> <li>OpenMP Nedir?</li> <li>Fork-Join Modeli</li> <li>Derleyici Direktifleri</li> <li>\u00c7al\u0131\u015fma Zaman\u0131 K\u00fct\u00fcphane Fonksiyonlar\u0131</li> <li> <p>Ortam De\u011fi\u015fkenleri</p> </li> <li> <p>OpenMP Direktifleri</p> </li> <li>Paralel B\u00f6lgeler</li> <li>\u0130\u015f Payla\u015f\u0131m\u0131 Yap\u0131lar\u0131</li> <li>Veri Payla\u015f\u0131m \u00d6zellikleri</li> <li> <p>Senkronizasyon</p> </li> <li> <p>OpenMP Programlama \u00d6rnekleri</p> </li> <li>Temel Paralel D\u00f6ng\u00fcler</li> <li>\u0130ndirgeme \u0130\u015flemleri</li> <li>G\u00f6rev Paralelli\u011fi</li> <li> <p>\u0130\u00e7 \u0130\u00e7e Paralellik</p> </li> <li> <p>Performans De\u011ferlendirmeleri</p> </li> <li>\u0130\u015f Par\u00e7ac\u0131\u011f\u0131 Y\u00f6netimi</li> <li>Y\u00fck Dengeleme</li> <li>Veri Yerle\u015fimi</li> <li>\u00d6nbellek Etkileri</li> </ol>"},{"location":"tr/week-3/cen310-week-3/#1-openmpye-giris","title":"1. OpenMP'ye Giri\u015f","text":""},{"location":"tr/week-3/cen310-week-3/#openmp-nedir","title":"OpenMP Nedir?","text":"<ul> <li>Payla\u015f\u0131ml\u0131 bellek paralel programlama i\u00e7in API</li> <li>C, C++ ve Fortran deste\u011fi</li> <li>Derleyici direktiflerine dayal\u0131</li> <li>Ta\u015f\u0131nabilir ve \u00f6l\u00e7eklenebilir</li> </ul> <p>\u00d6rnek: </p><pre><code>#include &lt;omp.h&gt;\n\nint main() {\n    #pragma omp parallel\n    {\n        printf(\"%d numaral\u0131 i\u015f par\u00e7ac\u0131\u011f\u0131ndan merhaba\\n\", \n               omp_get_thread_num());\n    }\n    return 0;\n}\n</code></pre> <p>// ... Week-3 i\u00e7in detayl\u0131 i\u00e7erik devam edecek </p>"},{"location":"tr/week-4/cen310-week-4/","title":"CEN310 Paralel Programlama Hafta-4","text":""},{"location":"tr/week-4/cen310-week-4/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-4/cen310-week-4/#hafta-4","title":"Hafta-4","text":""},{"location":"tr/week-4/cen310-week-4/#mpi-programlama","title":"MPI Programlama","text":""},{"location":"tr/week-4/cen310-week-4/#ders-icerigi","title":"Ders \u0130\u00e7eri\u011fi","text":"<ol> <li>MPI'ya Giri\u015f</li> <li>MPI Nedir?</li> <li>Da\u011f\u0131t\u0131k Bellek Modeli</li> <li>MPI Uygulama T\u00fcrleri</li> <li>Temel Kavramlar</li> <li> <p>Ortam Kurulumu</p> </li> <li> <p>Noktadan Noktaya \u0130leti\u015fim</p> </li> <li>Engelleyici G\u00f6nderme/Alma</li> <li>Engelleyici Olmayan G\u00f6nderme/Alma</li> <li>Tamponlama ve Senkronizasyon</li> <li>\u0130leti\u015fim Modlar\u0131</li> <li> <p>Hata Y\u00f6netimi</p> </li> <li> <p>Toplu \u0130leti\u015fim</p> </li> <li>Yay\u0131n (Broadcast)</li> <li>Da\u011f\u0131tma/Toplama (Scatter/Gather)</li> <li>\u0130ndirgeme \u0130\u015flemleri</li> <li>T\u00fcm\u00fcnden T\u00fcm\u00fcne \u0130leti\u015fim</li> <li> <p>Bariyerler</p> </li> <li> <p>\u0130leri Seviye MPI \u00d6zellikleri</p> </li> <li>T\u00fcretilmi\u015f Veri Tipleri</li> <li>Sanal Topolojiler</li> <li>Tek Tarafl\u0131 \u0130leti\u015fim</li> <li>Hibrit Programlama (MPI + OpenMP)</li> </ol>"},{"location":"tr/week-4/cen310-week-4/#1-mpiya-giris","title":"1. MPI'ya Giri\u015f","text":""},{"location":"tr/week-4/cen310-week-4/#mpi-nedir","title":"MPI Nedir?","text":"<ul> <li>Mesaj Ge\u00e7irme Aray\u00fcz\u00fc standard\u0131</li> <li>Platform-ba\u011f\u0131ms\u0131z ileti\u015fim protokol\u00fc</li> <li>Da\u011f\u0131t\u0131k bellek sistemlerini destekler</li> <li>C, C++, Fortran dil ba\u011flant\u0131lar\u0131</li> </ul> <p>\u00d6rnek: </p><pre><code>#include &lt;mpi.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv) {\n    int rank, size;\n\n    MPI_Init(&amp;argc, &amp;argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n    printf(\"S\u00fcre\u00e7 %d / %d\\n\", rank, size);\n\n    MPI_Finalize();\n    return 0;\n}\n</code></pre> <p>// ... Hafta-4 i\u00e7in detayl\u0131 i\u00e7erik devam edecek </p>"},{"location":"tr/week-5/cen310-week-5/","title":"CEN310 Paralel Programlama Hafta-5","text":""},{"location":"tr/week-5/cen310-week-5/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-5/cen310-week-5/#hafta-5","title":"Hafta-5","text":""},{"location":"tr/week-5/cen310-week-5/#gpu-programlama","title":"GPU Programlama","text":""},{"location":"tr/week-5/cen310-week-5/#ders-icerigi","title":"Ders \u0130\u00e7eri\u011fi","text":"<ol> <li>GPU Hesaplamaya Giri\u015f</li> <li>GPU Mimarisi Genel Bak\u0131\u015f</li> <li>CUDA Programlama Modeli</li> <li>GPU Bellek Hiyerar\u015fisi</li> <li>\u0130\u015f Par\u00e7ac\u0131\u011f\u0131 Hiyerar\u015fisi</li> <li> <p>\u00c7ekirdek Fonksiyonlar</p> </li> <li> <p>CUDA Programlama Temelleri</p> </li> <li>Bellek Y\u00f6netimi</li> <li>\u0130\u015f Par\u00e7ac\u0131\u011f\u0131 Organizasyonu</li> <li>Senkronizasyon</li> <li>Hata Y\u00f6netimi</li> <li> <p>CUDA \u00c7al\u0131\u015fma Zaman\u0131 API'si</p> </li> <li> <p>Performans Optimizasyonu</p> </li> <li>Bellek Birle\u015ftirme</li> <li>Payla\u015f\u0131ml\u0131 Bellek Kullan\u0131m\u0131</li> <li>Bank \u00c7ak\u0131\u015fmalar\u0131</li> <li>Doluluk Oran\u0131</li> <li> <p>Warp Sapmas\u0131</p> </li> <li> <p>\u0130leri GPU Programlama</p> </li> <li>Ak\u0131\u015flar ve Olaylar</li> <li>Asenkron \u0130\u015flemler</li> <li>\u00c7oklu-GPU Programlama</li> <li>GPU-CPU Veri Transferi</li> <li>Birle\u015fik Bellek</li> </ol>"},{"location":"tr/week-5/cen310-week-5/#1-gpu-hesaplamaya-giris","title":"1. GPU Hesaplamaya Giri\u015f","text":""},{"location":"tr/week-5/cen310-week-5/#gpu-mimarisi","title":"GPU Mimarisi","text":"<pre><code>Ana Bilgisayar (CPU)    Cihaz (GPU)\n        \u2193                    \u2193\n     Bellek            Global Bellek\n        \u2193                    \u2193\n      \u2190--- PCI Express Yolu -\u2192\n</code></pre> <p>Temel Kavramlar: - Yo\u011fun paralel mimari - Binlerce \u00e7ekirdek - SIMT y\u00fcr\u00fctme modeli - Bellek hiyerar\u015fisi</p> <p>\u00d6rnek CUDA Program\u0131: </p><pre><code>#include &lt;cuda_runtime.h&gt;\n\n__global__ void vektorTopla(float* a, float* b, float* c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t boyut = n * sizeof(float);\n\n    // Ana bilgisayar belle\u011fi ay\u0131rma\n    float *h_a = (float*)malloc(boyut);\n    float *h_b = (float*)malloc(boyut);\n    float *h_c = (float*)malloc(boyut);\n\n    // Dizileri ba\u015flatma\n    for(int i = 0; i &lt; n; i++) {\n        h_a[i] = rand() / (float)RAND_MAX;\n        h_b[i] = rand() / (float)RAND_MAX;\n    }\n\n    // GPU belle\u011fi ay\u0131rma\n    float *d_a, *d_b, *d_c;\n    cudaMalloc(&amp;d_a, boyut);\n    cudaMalloc(&amp;d_b, boyut);\n    cudaMalloc(&amp;d_c, boyut);\n\n    // GPU'ya kopyalama\n    cudaMemcpy(d_a, h_a, boyut, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_b, h_b, boyut, cudaMemcpyHostToDevice);\n\n    // \u00c7ekirde\u011fi ba\u015flatma\n    int blokBoyutu = 256;\n    int blokSayisi = (n + blokBoyutu - 1) / blokBoyutu;\n    vektorTopla&lt;&lt;&lt;blokSayisi, blokBoyutu&gt;&gt;&gt;(d_a, d_b, d_c, n);\n\n    // Sonucu geri kopyalama\n    cudaMemcpy(h_c, d_c, boyut, cudaMemcpyDeviceToHost);\n\n    // Temizlik\n    cudaFree(d_a);\n    cudaFree(d_b);\n    cudaFree(d_c);\n    free(h_a);\n    free(h_b);\n    free(h_c);\n\n    return 0;\n}\n</code></pre> <p>// ... Hafta-5 i\u00e7in detayl\u0131 i\u00e7erik devam edecek </p>"},{"location":"tr/week-6/cen310-week-6/","title":"CEN310 Paralel Programlama Hafta-6","text":""},{"location":"tr/week-6/cen310-week-6/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-6/cen310-week-6/#hafta-6","title":"Hafta-6","text":""},{"location":"tr/week-6/cen310-week-6/#performans-optimizasyonu","title":"Performans Optimizasyonu","text":""},{"location":"tr/week-6/cen310-week-6/#ders-icerigi","title":"Ders \u0130\u00e7eri\u011fi","text":"<ol> <li>Performans Analiz Ara\u00e7lar\u0131</li> <li>Profil \u00c7\u0131kar\u0131c\u0131lar</li> <li>Donan\u0131m Saya\u00e7lar\u0131</li> <li>Performans Metrikleri</li> <li>Darbo\u011faz Tespiti</li> <li> <p>K\u0131yaslama</p> </li> <li> <p>Bellek Optimizasyonu</p> </li> <li>\u00d6nbellek Optimizasyonu</li> <li>Bellek Eri\u015fim Desenleri</li> <li>Veri Yerle\u015fimi</li> <li>Yanl\u0131\u015f Payla\u015f\u0131m</li> <li> <p>Bellek Bant Geni\u015fli\u011fi</p> </li> <li> <p>Algoritma Optimizasyonu</p> </li> <li>Y\u00fck Dengeleme</li> <li>\u0130\u015f Da\u011f\u0131t\u0131m\u0131</li> <li>\u0130leti\u015fim Desenleri</li> <li>Senkronizasyon Y\u00fck\u00fc</li> <li> <p>\u00d6l\u00e7eklenebilirlik Analizi</p> </li> <li> <p>\u0130leri Optimizasyon Teknikleri</p> </li> <li>Vekt\u00f6rle\u015ftirme</li> <li>D\u00f6ng\u00fc Optimizasyonu</li> <li>\u0130\u015f Par\u00e7ac\u0131\u011f\u0131 \u0130li\u015fkilendirme</li> <li>Derleyici Optimizasyonlar\u0131</li> <li>Donan\u0131ma \u00d6zel Ayarlamalar</li> </ol>"},{"location":"tr/week-6/cen310-week-6/#1-performans-analiz-araclar","title":"1. Performans Analiz Ara\u00e7lar\u0131","text":""},{"location":"tr/week-6/cen310-week-6/#profil-ckarc-kullanm","title":"Profil \u00c7\u0131kar\u0131c\u0131 Kullan\u0131m\u0131","text":"<p>Intel VTune \u00f6rne\u011fi: </p><pre><code>#include &lt;omp.h&gt;\n#include &lt;vector&gt;\n\nvoid matris_carpimi_optimize(const std::vector&lt;float&gt;&amp; A,\n                           const std::vector&lt;float&gt;&amp; B,\n                           std::vector&lt;float&gt;&amp; C,\n                           int N) {\n    #pragma omp parallel for collapse(2)\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            float toplam = 0.0f;\n            // \u00d6nbellek dostu eri\u015fim deseni\n            for(int k = 0; k &lt; N; k++) {\n                toplam += A[i * N + k] * B[k * N + j];\n            }\n            C[i * N + j] = toplam;\n        }\n    }\n}\n\n// Performans \u00f6l\u00e7\u00fcm\u00fc\nvoid performans_olc() {\n    const int N = 1024;\n    std::vector&lt;float&gt; A(N * N), B(N * N), C(N * N);\n\n    // Matrisleri ba\u015flat\n    for(int i = 0; i &lt; N * N; i++) {\n        A[i] = rand() / (float)RAND_MAX;\n        B[i] = rand() / (float)RAND_MAX;\n    }\n\n    double baslangic = omp_get_wtime();\n    matris_carpimi_optimize(A, B, C, N);\n    double bitis = omp_get_wtime();\n\n    printf(\"S\u00fcre: %f saniye\\n\", bitis - baslangic);\n}\n</code></pre>"},{"location":"tr/week-6/cen310-week-6/#2-bellek-optimizasyonu","title":"2. Bellek Optimizasyonu","text":""},{"location":"tr/week-6/cen310-week-6/#onbellek-dostu-veri-erisimi","title":"\u00d6nbellek Dostu Veri Eri\u015fimi","text":"<pre><code>// K\u00f6t\u00fc: \u00d6nbellek dostu olmayan eri\u015fim\nvoid kotu_erisim(float* matris, int N) {\n    for(int j = 0; j &lt; N; j++) {\n        for(int i = 0; i &lt; N; i++) {\n            matris[i * N + j] = hesapla(i, j);\n        }\n    }\n}\n\n// \u0130yi: \u00d6nbellek dostu eri\u015fim\nvoid iyi_erisim(float* matris, int N) {\n    for(int i = 0; i &lt; N; i++) {\n        for(int j = 0; j &lt; N; j++) {\n            matris[i * N + j] = hesapla(i, j);\n        }\n    }\n}\n</code></pre>"},{"location":"tr/week-6/cen310-week-6/#yanls-paylasm-onleme","title":"Yanl\u0131\u015f Payla\u015f\u0131m \u00d6nleme","text":"<pre><code>// K\u00f6t\u00fc: Yanl\u0131\u015f payla\u015f\u0131m\nstruct KotuSayac {\n    int sayac;  // Birden \u00e7ok i\u015f par\u00e7ac\u0131\u011f\u0131 biti\u015fik belle\u011fi g\u00fcncelliyor\n};\n\n// \u0130yi: Yanl\u0131\u015f payla\u015f\u0131m\u0131 \u00f6nlemek i\u00e7in dolgu\nstruct IyiSayac {\n    int sayac;\n    char dolgu[60];  // \u00d6nbellek sat\u0131r\u0131 boyutuna hizala\n};\n\nvoid paralel_sayim() {\n    const int IS_PARCACIGI_SAYISI = 4;\n    IyiSayac sayaclar[IS_PARCACIGI_SAYISI];\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        for(int i = 0; i &lt; 1000000; i++) {\n            sayaclar[tid].sayac++;\n        }\n    }\n}\n</code></pre> <p>// ... Hafta-6 i\u00e7in detayl\u0131 i\u00e7erik devam edecek </p>"},{"location":"tr/week-8/cen310-week-8/","title":"CEN310 Paralel Programlama Hafta-8","text":""},{"location":"tr/week-8/cen310-week-8/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-8/cen310-week-8/#hafta-8-vize-proje-degerlendirmesi","title":"Hafta-8 (Vize Proje De\u011ferlendirmesi)","text":""},{"location":"tr/week-8/cen310-week-8/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-8/cen310-week-8/#proje-degerlendirme-gunu-program","title":"Proje De\u011ferlendirme G\u00fcn\u00fc Program\u0131","text":""},{"location":"tr/week-8/cen310-week-8/#sabah-oturumu-0900-1200","title":"Sabah Oturumu (09:00-12:00)","text":"<ul> <li>Proje sunumlar\u0131 (Grup 1-4)</li> <li>Performans analizi tart\u0131\u015fmalar\u0131</li> <li>Soru-cevap oturumlar\u0131</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#ogle-aras-1200-1300","title":"\u00d6\u011fle Aras\u0131 (12:00-13:00)","text":""},{"location":"tr/week-8/cen310-week-8/#ogleden-sonra-oturumu-1300-1700","title":"\u00d6\u011fleden Sonra Oturumu (13:00-17:00)","text":"<ul> <li>Proje sunumlar\u0131 (Grup 5-8)</li> <li>Teknik g\u00f6sterimler</li> <li>Son de\u011ferlendirmeler</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#proje-gereksinimleri","title":"Proje Gereksinimleri","text":""},{"location":"tr/week-8/cen310-week-8/#1-dokumantasyon","title":"1. Dok\u00fcmantasyon","text":"<ul> <li>Proje raporu</li> <li>Kaynak kod dok\u00fcmantasyonu</li> <li>Performans analizi sonu\u00e7lar\u0131</li> <li>Uygulama zorluklar\u0131</li> <li>Gelecek iyile\u015ftirmeler</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#2-uygulama","title":"2. Uygulama","text":"<ul> <li>\u00c7al\u0131\u015fan paralel program</li> <li>OpenMP ve/veya MPI kullan\u0131m\u0131</li> <li>Performans optimizasyonlar\u0131</li> <li>Hata y\u00f6netimi</li> <li>Kod kalitesi</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#sunum-yonergeleri","title":"Sunum Y\u00f6nergeleri","text":""},{"location":"tr/week-8/cen310-week-8/#format","title":"Format","text":"<ul> <li>Grup ba\u015f\u0131na 20 dakika</li> <li>15 dakika sunum</li> <li>5 dakika soru-cevap</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#icerik","title":"\u0130\u00e7erik","text":"<ol> <li>Problem Tan\u0131m\u0131</li> <li>\u00c7\u00f6z\u00fcm Yakla\u015f\u0131m\u0131</li> <li>Uygulama Detaylar\u0131</li> <li>Performans Sonu\u00e7lar\u0131</li> <li>Zorluklar ve \u00c7\u00f6z\u00fcmler</li> <li>Demo</li> </ol>"},{"location":"tr/week-8/cen310-week-8/#performans-analizi-gereksinimleri","title":"Performans Analizi Gereksinimleri","text":""},{"location":"tr/week-8/cen310-week-8/#olculecek-metrikler","title":"\u00d6l\u00e7\u00fclecek Metrikler","text":"<ul> <li>\u00c7al\u0131\u015fma s\u00fcresi</li> <li>H\u0131zlanma</li> <li>Verimlilik</li> <li>\u00d6l\u00e7eklenebilirlik</li> <li>Kaynak kullan\u0131m\u0131</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#analiz-araclar","title":"Analiz Ara\u00e7lar\u0131","text":"<pre><code># \u00d6rnek performans \u00f6l\u00e7\u00fcm\u00fc\n$ perf stat ./paralel_program\n$ nvprof ./cuda_program\n$ vtune ./openmp_program\n</code></pre>"},{"location":"tr/week-8/cen310-week-8/#ornek-proje-yaps","title":"\u00d6rnek Proje Yap\u0131s\u0131","text":"<pre><code>// Proje mimarisi \u00f6rne\u011fi\nproje/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.cpp\n\u2502   \u251c\u2500\u2500 paralel_uygulama.cpp\n\u2502   \u2514\u2500\u2500 yardimcilar.cpp\n\u251c\u2500\u2500 include/\n\u2502   \u251c\u2500\u2500 paralel_uygulama.h\n\u2502   \u2514\u2500\u2500 yardimcilar.h\n\u251c\u2500\u2500 testler/\n\u2502   \u2514\u2500\u2500 test_paralel.cpp\n\u251c\u2500\u2500 dokumanlar/\n\u2502   \u251c\u2500\u2500 rapor.pdf\n\u2502   \u2514\u2500\u2500 sunum.pptx\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"tr/week-8/cen310-week-8/#performans-karslastrma-sablonu","title":"Performans Kar\u015f\u0131la\u015ft\u0131rma \u015eablonu","text":""},{"location":"tr/week-8/cen310-week-8/#sral-vs-paralel-uygulama","title":"S\u0131ral\u0131 vs Paralel Uygulama","text":"<pre><code>// S\u0131ral\u0131 uygulama\ndouble sirali_sure = 0.0;\n{\n    auto baslangic = std::chrono::high_resolution_clock::now();\n    sirali_sonuc = sirali_hesapla();\n    auto bitis = std::chrono::high_resolution_clock::now();\n    sirali_sure = std::chrono::duration&lt;double&gt;(bitis-baslangic).count();\n}\n\n// Paralel uygulama\ndouble paralel_sure = 0.0;\n{\n    auto baslangic = std::chrono::high_resolution_clock::now();\n    paralel_sonuc = paralel_hesapla();\n    auto bitis = std::chrono::high_resolution_clock::now();\n    paralel_sure = std::chrono::duration&lt;double&gt;(bitis-baslangic).count();\n}\n\n// H\u0131zlanma hesapla\ndouble hizlanma = sirali_sure / paralel_sure;\n</code></pre>"},{"location":"tr/week-8/cen310-week-8/#yaygn-proje-konular","title":"Yayg\u0131n Proje Konular\u0131","text":"<ol> <li>Matris \u0130\u015flemleri</li> <li>Matris \u00e7arp\u0131m\u0131</li> <li>Matris ayr\u0131\u015ft\u0131rma</li> <li> <p>Lineer denklem \u00e7\u00f6z\u00fcm\u00fc</p> </li> <li> <p>Bilimsel Hesaplama</p> </li> <li>N-cisim sim\u00fclasyonu</li> <li>Dalga denklemi \u00e7\u00f6z\u00fcc\u00fc</li> <li> <p>Monte Carlo y\u00f6ntemleri</p> </li> <li> <p>Veri \u0130\u015fleme</p> </li> <li>G\u00f6r\u00fcnt\u00fc i\u015fleme</li> <li>Sinyal i\u015fleme</li> <li> <p>Veri madencili\u011fi</p> </li> <li> <p>Graf Algoritmalar\u0131</p> </li> <li>En k\u0131sa yol</li> <li>Graf boyama</li> <li>Maksimum ak\u0131\u015f</li> </ol>"},{"location":"tr/week-8/cen310-week-8/#degerlendirme-kriterleri","title":"De\u011ferlendirme Kriterleri","text":""},{"location":"tr/week-8/cen310-week-8/#teknik-yonler-60","title":"Teknik Y\u00f6nler (60%)","text":"<ul> <li>Do\u011fru uygulama (20%)</li> <li>Performans optimizasyonu (20%)</li> <li>Kod kalitesi (10%)</li> <li>Dok\u00fcmantasyon (10%)</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#sunum-40","title":"Sunum (40%)","text":"<ul> <li>A\u00e7\u0131k anlat\u0131m (15%)</li> <li>Demo kalitesi (15%)</li> <li>Soru-cevap y\u00f6netimi (10%)</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#proje-raporu-sablonu","title":"Proje Raporu \u015eablonu","text":""},{"location":"tr/week-8/cen310-week-8/#1-giris","title":"1. Giri\u015f","text":"<ul> <li>Problem tan\u0131m\u0131</li> <li>Hedefler</li> <li>Arka plan</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#2-tasarm","title":"2. Tasar\u0131m","text":"<ul> <li>Mimari</li> <li>Algoritmalar</li> <li>Paralelle\u015ftirme stratejisi</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#3-uygulama","title":"3. Uygulama","text":"<ul> <li>Kullan\u0131lan teknolojiler</li> <li>Kod yap\u0131s\u0131</li> <li>Temel bile\u015fenler</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#4-sonuclar","title":"4. Sonu\u00e7lar","text":"<ul> <li>Performans \u00f6l\u00e7\u00fcmleri</li> <li>Analiz</li> <li>Kar\u015f\u0131la\u015ft\u0131rmalar</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#5-sonuc","title":"5. Sonu\u00e7","text":"<ul> <li>Ba\u015far\u0131lar</li> <li>Zorluklar</li> <li>Gelecek \u00e7al\u0131\u015fmalar</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#kaynaklar-ve-referanslar","title":"Kaynaklar ve Referanslar","text":""},{"location":"tr/week-8/cen310-week-8/#dokumantasyon","title":"Dok\u00fcmantasyon","text":"<ul> <li>OpenMP: https://www.openmp.org/</li> <li>MPI: https://www.open-mpi.org/</li> <li>CUDA: https://docs.nvidia.com/cuda/</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#araclar","title":"Ara\u00e7lar","text":"<ul> <li>Performans analiz ara\u00e7lar\u0131</li> <li>Hata ay\u0131klama ara\u00e7lar\u0131</li> <li>Profilleme ara\u00e7lar\u0131</li> </ul>"},{"location":"tr/week-8/cen310-week-8/#sorular-ve-tartsma","title":"Sorular ve Tart\u0131\u015fma","text":""},{"location":"tr/week-9-midterm/cen310-week-9/","title":"CEN310 Paralel Programlama Hafta-9","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#cen310-paralel-programlama","title":"CEN310 Paralel Programlama","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#hafta-9-vize-snav-donemi","title":"Hafta-9 (Vize S\u0131nav D\u00f6nemi)","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#bahar-donemi-2024-2025","title":"Bahar D\u00f6nemi, 2024-2025","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#vize-snav-donemi-bilgileri","title":"Vize S\u0131nav D\u00f6nemi Bilgileri","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#tarihler","title":"Tarihler","text":"<ul> <li>D\u00f6nem: 5-13 Nisan 2025</li> <li>Proje Raporu Teslimi: \u00dcniversite taraf\u0131ndan belirlenen tarihte</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#konum","title":"Konum","text":"<ul> <li>\u00dcniversite taraf\u0131ndan atanan yerde</li> <li>Resmi s\u0131nav program\u0131n\u0131 kontrol edin</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#vize-proje-raporu-gereksinimleri","title":"Vize Proje Raporu Gereksinimleri","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#1-proje-dokumantasyonu","title":"1. Proje Dok\u00fcmantasyonu","text":"<ul> <li>Eksiksiz proje raporu</li> <li>Dok\u00fcmantasyonlu kaynak kod</li> <li>Performans analizi sonu\u00e7lar\u0131</li> <li>Uygulama detaylar\u0131</li> <li>Gelecek \u00e7al\u0131\u015fma \u00f6nerileri</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#2-teknik-gereksinimler","title":"2. Teknik Gereksinimler","text":"<ul> <li>Kod kalitesi ve organizasyonu</li> <li>Performans optimizasyon sonu\u00e7lar\u0131</li> <li>S\u0131ral\u0131 versiyon ile kar\u015f\u0131la\u015ft\u0131rma</li> <li>\u00d6l\u00e7eklenebilirlik analizi</li> <li>Hata y\u00f6netimi uygulamas\u0131</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#rapor-yaps","title":"Rapor Yap\u0131s\u0131","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#1-yonetici-ozeti","title":"1. Y\u00f6netici \u00d6zeti","text":"<ul> <li>Proje genel bak\u0131\u015f\u0131</li> <li>Temel ba\u015far\u0131lar</li> <li>Performans \u00f6ne \u00e7\u0131kanlar\u0131</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#2-teknik-uygulama","title":"2. Teknik Uygulama","text":"<ul> <li>Mimari detaylar</li> <li>Algoritma a\u00e7\u0131klamalar\u0131</li> <li>Paralelle\u015ftirme stratejisi</li> <li>Kod yap\u0131s\u0131</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#3-performans-analizi","title":"3. Performans Analizi","text":"<ul> <li>K\u0131yaslama sonu\u00e7lar\u0131</li> <li>\u00d6l\u00e7eklenebilirlik testleri</li> <li>Kaynak kullan\u0131m\u0131</li> <li>Optimizasyon \u00e7al\u0131\u015fmalar\u0131</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#4-sonuclar","title":"4. Sonu\u00e7lar","text":"<ul> <li>\u00d6\u011frenilen dersler</li> <li>A\u015f\u0131lan zorluklar</li> <li>Gelecek iyile\u015ftirmeler</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#teslim-yonergeleri","title":"Teslim Y\u00f6nergeleri","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#format-gereksinimleri","title":"Format Gereksinimleri","text":"<ul> <li>PDF format\u0131</li> <li>Profesyonel formatlama</li> <li>A\u00e7\u0131k kod listeleri</li> <li>Uygun al\u0131nt\u0131lar</li> <li>Performans grafikleri</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#teslim-sureci","title":"Teslim S\u00fcreci","text":"<ul> <li>Dijital teslim</li> <li>Kaynak kod deposu</li> <li>Dok\u00fcmantasyon paketi</li> <li>Sunum slaytlar\u0131</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#degerlendirme-kriterleri","title":"De\u011ferlendirme Kriterleri","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#teknik-derinlik-40","title":"Teknik Derinlik (40%)","text":"<ul> <li>Uygulama kalitesi</li> <li>Performans optimizasyonu</li> <li>Kod organizasyonu</li> <li>Dok\u00fcmantasyon kalitesi</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#analiz-ve-sonuclar-40","title":"Analiz ve Sonu\u00e7lar (40%)","text":"<ul> <li>Performans \u00f6l\u00e7\u00fcmleri</li> <li>\u00d6l\u00e7eklenebilirlik analizi</li> <li>Kar\u015f\u0131la\u015ft\u0131rmal\u0131 de\u011ferlendirme</li> <li>Problem \u00e7\u00f6zme yakla\u015f\u0131m\u0131</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#dokumantasyon-20","title":"Dok\u00fcmantasyon (20%)","text":"<ul> <li>Rapor kalitesi</li> <li>Kod dok\u00fcmantasyonu</li> <li>Sunum materyalleri</li> <li>Gelecek \u00f6nerileri</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#onemli-notlar","title":"\u00d6nemli Notlar","text":""},{"location":"tr/week-9-midterm/cen310-week-9/#teslim-tarihleri","title":"Teslim Tarihleri","text":"<ul> <li>Rapor teslim tarihi kesindir</li> <li>Ge\u00e7 teslimler kabul edilmeyebilir</li> <li>Uzatmalar \u00f6nceden onay gerektirir</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#akademik-durustluk","title":"Akademik D\u00fcr\u00fcstl\u00fck","text":"<ul> <li>\u00d6zg\u00fcn \u00e7al\u0131\u015fma gereklidir</li> <li>Uygun al\u0131nt\u0131lar gereklidir</li> <li>Kod intihal kontrol\u00fc yap\u0131l\u0131r</li> <li>\u0130\u015fbirli\u011fi belirtilmelidir</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#iletisim-bilgileri","title":"\u0130leti\u015fim Bilgileri","text":"<p>S\u0131nav ile ilgili sorular\u0131n\u0131z i\u00e7in:</p> <ul> <li>E-posta: ugur.coruh@erdogan.edu.tr</li> <li>Ofis Saatleri: Randevu ile</li> <li>Konum: M\u00fchendislik Fak\u00fcltesi</li> </ul>"},{"location":"tr/week-9-midterm/cen310-week-9/#vize-snavnzda-basarlar","title":"Vize S\u0131nav\u0131n\u0131zda Ba\u015far\u0131lar!","text":""}]}